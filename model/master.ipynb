{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries for video classification\n",
    "import torch, os, cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from lipreading.model import Lipreading\n",
    "from lipreading.optim_utils import CosineScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize the seed and the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed for reproducibility\n",
    "seed = 0\n",
    "def reset_seed():\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Setting the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. List of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ٱ': 1, 'يْ': 2, 'يّْ': 3, 'يِّ': 4, 'يُّ': 5, 'يَّ': 6, 'يٌّ': 7, 'يِ': 8, 'يُ': 9, 'يَ': 10, 'يٌ': 11, 'ي': 12, 'ى': 13, 'وْ': 14, 'وِّ': 15, 'وُّ': 16, 'وَّ': 17, 'وِ': 18, 'وُ': 19, 'وَ': 20, 'وً': 21, 'و': 22, 'هْ': 23, 'هُّ': 24, 'هِ': 25, 'هُ': 26, 'هَ': 27, 'نۢ': 28, 'نْ': 29, 'نِّ': 30, 'نُّ': 31, 'نَّ': 32, 'نِ': 33, 'نُ': 34, 'نَ': 35, 'ن': 36, 'مْ': 37, 'مّْ': 38, 'مِّ': 39, 'مُّ': 40, 'مَّ': 41, 'مِ': 42, 'مُ': 43, 'مَ': 44, 'مٍ': 45, 'مٌ': 46, 'مً': 47, 'لْ': 48, 'لّْ': 49, 'لِّ': 50, 'لُّ': 51, 'لَّ': 52, 'لِ': 53, 'لُ': 54, 'لَ': 55, 'لٍ': 56, 'لٌ': 57, 'لً': 58, 'ل': 59, 'كْ': 60, 'كِّ': 61, 'كَّ': 62, 'كِ': 63, 'كُ': 64, 'كَ': 65, 'ك': 66, 'قْ': 67, 'قَّ': 68, 'قِ': 69, 'قُ': 70, 'قَ': 71, 'قٍ': 72, 'قً': 73, 'ق': 74, 'فْ': 75, 'فِّ': 76, 'فَّ': 77, 'فِ': 78, 'فُ': 79, 'فَ': 80, 'غْ': 81, 'غِ': 82, 'غَ': 83, 'عْ': 84, 'عَّ': 85, 'عِ': 86, 'عُ': 87, 'عَ': 88, 'عٍ': 89, 'ظْ': 90, 'ظِّ': 91, 'ظَّ': 92, 'ظِ': 93, 'ظُ': 94, 'ظَ': 95, 'طْ': 96, 'طِّ': 97, 'طَّ': 98, 'طِ': 99, 'طُ': 100, 'طَ': 101, 'ط': 102, 'ضْ': 103, 'ضِّ': 104, 'ضُّ': 105, 'ضَّ': 106, 'ضِ': 107, 'ضُ': 108, 'ضَ': 109, 'ضً': 110, 'صْ': 111, 'صّْ': 112, 'صِّ': 113, 'صُّ': 114, 'صَّ': 115, 'صِ': 116, 'صُ': 117, 'صَ': 118, 'صٍ': 119, 'صً': 120, 'شْ': 121, 'شِّ': 122, 'شُّ': 123, 'شَّ': 124, 'شِ': 125, 'شُ': 126, 'شَ': 127, 'ش': 128, 'سْ': 129, 'سّْ': 130, 'سِّ': 131, 'سُّ': 132, 'سَّ': 133, 'سِ': 134, 'سُ': 135, 'سَ': 136, 'سٍ': 137, 'زْ': 138, 'زَّ': 139, 'زِ': 140, 'زُ': 141, 'زَ': 142, 'رْ': 143, 'رِّ': 144, 'رُّ': 145, 'رَّ': 146, 'رِ': 147, 'رُ': 148, 'رَ': 149, 'رٍ': 150, 'رٌ': 151, 'رً': 152, 'ذْ': 153, 'ذَّ': 154, 'ذِ': 155, 'ذُ': 156, 'ذَ': 157, 'دْ': 158, 'دِّ': 159, 'دُّ': 160, 'دَّ': 161, 'دًّ': 162, 'دِ': 163, 'دُ': 164, 'دَ': 165, 'دٍ': 166, 'دٌ': 167, 'دً': 168, 'خْ': 169, 'خِ': 170, 'خُ': 171, 'خَ': 172, 'حْ': 173, 'حَّ': 174, 'حِ': 175, 'حُ': 176, 'حَ': 177, 'ح': 178, 'جْ': 179, 'جِّ': 180, 'جُّ': 181, 'جَّ': 182, 'جِ': 183, 'جُ': 184, 'جَ': 185, 'ثْ': 186, 'ثِّ': 187, 'ثُّ': 188, 'ثَّ': 189, 'ثِ': 190, 'ثُ': 191, 'ثَ': 192, 'تْ': 193, 'تِّ': 194, 'تُّ': 195, 'تَّ': 196, 'تِ': 197, 'تُ': 198, 'تَ': 199, 'تٍ': 200, 'تٌ': 201, 'ةْ': 202, 'ةِ': 203, 'ةُ': 204, 'ةَ': 205, 'ةٍ': 206, 'ةٌ': 207, 'ةً': 208, 'ة': 209, 'بْ': 210, 'بِّ': 211, 'بَّ': 212, 'بِ': 213, 'بُ': 214, 'بَ': 215, 'بٍ': 216, 'بً': 217, 'ب': 218, 'ا': 219, 'ئْ': 220, 'ئِ': 221, 'ئَ': 222, 'ئً': 223, 'إِ': 224, 'إ': 225, 'ؤْ': 226, 'ؤُ': 227, 'ؤَ': 228, 'ؤ': 229, 'أْ': 230, 'أُ': 231, 'أَ': 232, 'آ': 233, 'ءْ': 234, 'ءِ': 235, 'ءَ': 236, 'ءً': 237}\n"
     ]
    }
   ],
   "source": [
    "def extract_label(file):\n",
    "    label = []\n",
    "    diacritics = {\n",
    "        '\\u064B',  # Fathatan\n",
    "        '\\u064C',  # Dammatan\n",
    "        '\\u064D',  # Kasratan\n",
    "        '\\u064E',  # Fatha\n",
    "        '\\u064F',  # Damma\n",
    "        '\\u0650',  # Kasra\n",
    "        '\\u0651',  # Shadda\n",
    "        '\\u0652',  # Sukun\n",
    "        '\\u06E2',  # Small High meem\n",
    "    }\n",
    "\n",
    "    sentence = pd.read_csv(file)\n",
    "    for word in sentence.word:\n",
    "        for char in word:\n",
    "            if char not in diacritics:\n",
    "                label.append(char)\n",
    "            else:\n",
    "                label[-1] += char\n",
    "\n",
    "    return label\n",
    "\n",
    "classes = set()\n",
    "for i in os.listdir('Dataset/Csv (with Diacritics)'):\n",
    "    file = 'Dataset/Csv (with Diacritics)/' + i\n",
    "    label = extract_label(file)\n",
    "    classes.update(label)\n",
    "\n",
    "mapped_classes = {}\n",
    "for i, c in enumerate(sorted(classes, reverse=True), 1):\n",
    "    mapped_classes[c] = i\n",
    "\n",
    "print(mapped_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Video Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the video dataset class\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_paths, label_paths, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        frames = self.load_frames(video_path=video_path)\n",
    "        label = torch.tensor(list(map(lambda x: mapped_classes[x], extract_label(label_path))))\n",
    "        input_length = torch.tensor(len(frames), dtype=torch.long)\n",
    "        label_length = torch.tensor(len(label), dtype=torch.long)\n",
    "        return frames, input_length, label, label_length\n",
    "    \n",
    "    def load_frames(self, video_path):\n",
    "        frames = []\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        for i in range(total_frames):\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = video.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame_pil = Image.fromarray(frame, 'L')\n",
    "                frames.append(frame_pil)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            frames = [self.transform(frame) for frame in frames] \n",
    "        frames = torch.stack(frames).permute(1, 0, 2, 3)\n",
    "        return frames\n",
    "\n",
    "# Defining the video transform\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.421, std=0.165),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_dir = \"Dataset/Video\"\n",
    "labels_dir = \"Dataset/Csv (with Diacritics)\"\n",
    "videos, labels = [], []\n",
    "file_names = [file_name[:-4] for file_name in os.listdir(videos_dir)]\n",
    "for file_name in file_names:\n",
    "    videos.append(os.path.join(videos_dir, file_name + \".mp4\"))\n",
    "    labels.append(os.path.join(labels_dir, file_name + \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(videos, labels, test_size=0.1000, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_packed_collate(batch):\n",
    "    \"\"\"Pads data and labels with different lengths in the same batch\n",
    "    \"\"\"\n",
    "    data_list, input_lengths, labels_list, label_lengths = zip(*batch)\n",
    "    c, max_len, h, w = max(data_list, key=lambda x: x.shape[1]).shape\n",
    "\n",
    "    data = torch.zeros((len(data_list), c, max_len, h, w))\n",
    "    \n",
    "    # Only copy up to the actual sequence length\n",
    "    for idx in range(len(data)):\n",
    "        data[idx, :, :input_lengths[idx], :, :] = data_list[idx][:, :input_lengths[idx], :, :]\n",
    "    \n",
    "    # Flatten labels for CTC loss\n",
    "    labels_flat = []\n",
    "    for label_seq in labels_list:\n",
    "        labels_flat.extend(label_seq)\n",
    "    labels_flat = torch.LongTensor(labels_flat)\n",
    "    \n",
    "    # Convert lengths to tensor\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    label_lengths = torch.LongTensor(label_lengths)\n",
    "    return data, input_lengths, labels_flat, label_lengths\n",
    "\n",
    "\n",
    "# Defining the video dataloaders (train, validation, test)\n",
    "train_dataset = VideoDataset(X_train, y_train, transform=transforms)\n",
    "val_dataset = VideoDataset(X_val, y_val, transform=transforms)\n",
    "test_dataset = VideoDataset(X_test, y_test, transform=transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the greedy CTC decoder functions\n",
    "def greedy_ctc_decoder(logits, blank_index=0):\n",
    "    \"\"\"\n",
    "    Greedy decoding for CTC.\n",
    "    Assumes logits shape is (T, C) (log probabilities).\n",
    "    Returns a list of predicted indices (for one sample).\n",
    "    \"\"\"\n",
    "    # Convert to numpy if it's a tensor\n",
    "    if isinstance(logits, torch.Tensor):\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    # Get the highest probability index at each timestep\n",
    "    indices = np.argmax(logits, axis=1)  # (T,)\n",
    "    \n",
    "    # Remove duplicates and blanks\n",
    "    filtered_indices = []\n",
    "    prev_idx = -1\n",
    "    for idx in indices:\n",
    "        if idx != blank_index and idx != prev_idx:  # Skip blanks and duplicates\n",
    "            filtered_indices.append(idx)\n",
    "        prev_idx = idx\n",
    "    \n",
    "    return filtered_indices\n",
    "\n",
    "def indices_to_text(indices, idx2char):\n",
    "    \"\"\"\n",
    "    Converts a list of indices to text using the reverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    return ''.join([idx2char.get(i, '') for i in indices])\n",
    "\n",
    "def compute_cer(reference_indices, hypothesis_indices):\n",
    "    \"\"\"\n",
    "    Computes Character Error Rate (CER) directly using token indices.\n",
    "    Takes raw token indices from our vocabulary (class_mapping.txt) rather than Unicode text.\n",
    "    \n",
    "    Returns a tuple of (CER, reference_len, hypothesis_len, edit_distance)\n",
    "    \"\"\"\n",
    "    # Use the indices directly - each index is one token in our vocabulary\n",
    "    ref_tokens = reference_indices\n",
    "    hyp_tokens = hypothesis_indices\n",
    "    \n",
    "    print(f\"Debug - Reference tokens ({len(ref_tokens)} tokens): {ref_tokens}\")\n",
    "    print(f\"Debug - Hypothesis tokens ({len(hyp_tokens)} tokens): {hyp_tokens}\")\n",
    "    \n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    \n",
    "    # Initialize the distance matrix\n",
    "    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    \n",
    "    # Base cases: empty hypothesis or reference\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Fill the distance matrix\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            # If tokens match, no operation needed\n",
    "            if ref_tokens[i - 1] == hyp_tokens[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                # Minimum of:\n",
    "                # 1. Substitution: dp[i-1][j-1] + 1\n",
    "                # 2. Insertion: dp[i][j-1] + 1\n",
    "                # 3. Deletion: dp[i-1][j] + 1\n",
    "                dp[i][j] = min(dp[i - 1][j - 1] + 1,  # substitution\n",
    "                              dp[i][j - 1] + 1,      # insertion\n",
    "                              dp[i - 1][j] + 1)      # deletion\n",
    "    \n",
    "    edit_distance = dp[m][n]\n",
    "    cer = edit_distance / max(m, 1)  # Avoid division by zero\n",
    "    \n",
    "    return cer, edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lipreading(\n",
      "  (frontend): Sequential(\n",
      "    (0): Conv3d(1, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (trunk): ResNet(\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): PReLU(num_parameters=64)\n",
      "        (relu2): PReLU(num_parameters=64)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): PReLU(num_parameters=64)\n",
      "        (relu2): PReLU(num_parameters=64)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): PReLU(num_parameters=128)\n",
      "        (relu2): PReLU(num_parameters=128)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): PReLU(num_parameters=128)\n",
      "        (relu2): PReLU(num_parameters=128)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): PReLU(num_parameters=256)\n",
      "        (relu2): PReLU(num_parameters=256)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): PReLU(num_parameters=256)\n",
      "        (relu2): PReLU(num_parameters=256)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): PReLU(num_parameters=512)\n",
      "        (relu2): PReLU(num_parameters=512)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): PReLU(num_parameters=512)\n",
      "        (relu2): PReLU(num_parameters=512)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  )\n",
      "  (tcn): DenseTCN(\n",
      "    (tcn_trunk): DenseTemporalConvNet(\n",
      "      (features): Sequential(\n",
      "        (transition0): _Transition(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (prelu): PReLU(num_parameters=512)\n",
      "        )\n",
      "        (denseblock1): _DenseBlock(\n",
      "          (denselayer1): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(5,), stride=(1,), padding=(4,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(4,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(512, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "          (denselayer2): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(896, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "          (denselayer3): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(3,), stride=(1,), padding=(10,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(5,), stride=(1,), padding=(20,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(7,), stride=(1,), padding=(30,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(10,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(20,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(30,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(1280, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "        )\n",
      "        (transition1): _Transition(\n",
      "          (conv): Conv1d(1664, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (prelu): PReLU(num_parameters=512)\n",
      "        )\n",
      "        (denseblock2): _DenseBlock(\n",
      "          (denselayer1): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(5,), stride=(1,), padding=(4,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(4,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(512, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "          (denselayer2): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(896, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "          (denselayer3): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(3,), stride=(1,), padding=(10,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(5,), stride=(1,), padding=(20,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(7,), stride=(1,), padding=(30,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(10,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(20,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(30,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(1280, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "        )\n",
      "        (transition2): _Transition(\n",
      "          (conv): Conv1d(1664, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (prelu): PReLU(num_parameters=512)\n",
      "        )\n",
      "        (denseblock3): _DenseBlock(\n",
      "          (denselayer1): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(5,), stride=(1,), padding=(4,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(4,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(512, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "          (denselayer2): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(896, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "          (denselayer3): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(3,), stride=(1,), padding=(10,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(5,), stride=(1,), padding=(20,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(7,), stride=(1,), padding=(30,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(10,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(20,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(30,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(1280, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "        )\n",
      "        (transition3): _Transition(\n",
      "          (conv): Conv1d(1664, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (prelu): PReLU(num_parameters=512)\n",
      "        )\n",
      "        (denseblock4): _DenseBlock(\n",
      "          (denselayer1): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(5,), stride=(1,), padding=(4,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(4,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(512, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "          (denselayer2): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=896, out_features=56, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=56, out_features=896, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(896, 128, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(896, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "          (denselayer3): _ConvBatchChompRelu(\n",
      "            (cbcr0_se_0): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(3,), stride=(1,), padding=(10,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_1): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(5,), stride=(1,), padding=(20,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_se_2): SELayer(\n",
      "              (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "              (fc): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=80, bias=False)\n",
      "                (1): Swish()\n",
      "                (2): Linear(in_features=80, out_features=1280, bias=False)\n",
      "                (3): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (cbcr0_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(1280, 128, kernel_size=(7,), stride=(1,), padding=(30,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout0): Dropout(p=0.2, inplace=False)\n",
      "            (cbcr1_0): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(10,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_1): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(5,), stride=(1,), padding=(20,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (cbcr1_2): TemporalConvLayer(\n",
      "              (net): Sequential(\n",
      "                (0): Conv1d(384, 128, kernel_size=(7,), stride=(1,), padding=(30,), dilation=(5,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): Chomp1d()\n",
      "                (3): PReLU(num_parameters=128)\n",
      "              )\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (downsample): Conv1d(1280, 384, kernel_size=(1,), stride=(1,))\n",
      "            (relu_final): PReLU(num_parameters=384)\n",
      "          )\n",
      "        )\n",
      "        (norm5): BatchNorm1d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (tcn_output): Linear(in_features=1664, out_features=238, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initializing the hyper-parameters\n",
    "densetcn_options = {\n",
    "    'block_config': [3, 3, 3, 3],               # Number of layers in each dense block\n",
    "    'growth_rate_set': [384, 384, 384, 384],    # Growth rate for each block (must be divisible by len(kernel_size_set))\n",
    "    'reduced_size': 512,                        # Reduced size between blocks (must be divisible by len(kernel_size_set))\n",
    "    'kernel_size_set': [3, 5, 7],               # Kernel sizes for multi-scale processing\n",
    "    'dilation_size_set': [1, 2, 5],             # Dilation rates for increasing receptive field\n",
    "    'squeeze_excitation': True,                 # Whether to use SE blocks for channel attention\n",
    "    'dropout': 0.2                              # Dropout rate\n",
    "}\n",
    "initial_lr = 3e-4\n",
    "total_epochs = 80\n",
    "scheduler = CosineScheduler(initial_lr, total_epochs)\n",
    "\n",
    "# Build reverse mapping for decoding\n",
    "idx2char = {v: k for k, v in mapped_classes.items()}\n",
    "idx2char[0] = \"\"  # Blank token for CTC\n",
    "\n",
    "# Initializing the model\n",
    "model = Lipreading(densetcn_options=densetcn_options, hidden_dim=512, num_classes=len(mapped_classes) + 1, relu_type='prelu').to(device)\n",
    "print(model)\n",
    "\n",
    "# Defining the loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    \n",
    "    for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(train_loader):\n",
    "        # Print input shape for debugging\n",
    "        print(f\"Batch {batch_idx+1} - Input shape: {inputs.shape}\")\n",
    "        \n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        labels_flat = labels_flat.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass - get sequence logits\n",
    "        logits = model(inputs, input_lengths)\n",
    "        output_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Print shape to verify sequence output\n",
    "        print(f\"Batch {batch_idx+1} - Logits shape: {logits.shape}\")\n",
    "        \n",
    "        # Apply log_softmax for CTC\n",
    "        log_probs = F.log_softmax(logits, dim=2)  # (B, T, C)\n",
    "        \n",
    "        # Prepare for CTC loss - requires (T, B, C) format\n",
    "        outputs_for_ctc = log_probs.transpose(0, 1)  # from (B, T, C) to (T, B, C)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        loss = ctc_loss(outputs_for_ctc, labels_flat, output_lengths, label_lengths)\n",
    "        print(f\"Batch {batch_idx+1} - Loss: {loss.item()}\")\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(data_loader):\n",
    "    model.eval()\n",
    "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    # Track statistics\n",
    "    total_cer = 0\n",
    "    total_loss = 0\n",
    "    total_edit_distance = 0\n",
    "    \n",
    "    # Process all batches in the test loader\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(data_loader):\n",
    "            # Move to device\n",
    "            inputs = inputs.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            labels_flat = labels_flat.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "            \n",
    "            # Forward pass through the entire model\n",
    "            batch_size = inputs.size(0)\n",
    "            logits = model(inputs, input_lengths)\n",
    "            \n",
    "            # Apply log_softmax for CTC\n",
    "            log_probs = F.log_softmax(logits, dim=2)  # (B, T, C)\n",
    "            \n",
    "            # For CTC loss we need (T, N, C) format\n",
    "            log_probs_ctc = log_probs.transpose(0, 1)  # (T, B, C)\n",
    "            \n",
    "            # Make sure output_lengths are not greater than the input lengths\n",
    "            output_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=device)\n",
    "            # if output_lengths.max() > input_lengths.min():\n",
    "            #     scale_factor = input_lengths.min().float() / output_lengths.max().float()\n",
    "            #     output_lengths = (output_lengths.float() * scale_factor).long()\n",
    "            \n",
    "            # Calculate CTC loss\n",
    "            loss = ctc_loss(log_probs_ctc, labels_flat, output_lengths, label_lengths)\n",
    "            \n",
    "            # Decode predictions - we convert back to numpy for greedy decoding\n",
    "            logits_np = log_probs.cpu().detach().numpy()  # (B, T, C)\n",
    "            \n",
    "            # Process each batch item\n",
    "            for b in range(batch_size):\n",
    "                # Get batch item logits\n",
    "                batch_logits = logits_np[b]  # (T, C)\n",
    "                \n",
    "                # Decode using CTC\n",
    "                pred_indices = greedy_ctc_decoder(batch_logits)\n",
    "                \n",
    "                # Get target indices\n",
    "                start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                end_idx = start_idx + label_lengths[b].item()\n",
    "                target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                \n",
    "                # Convert indices to text\n",
    "                pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                target_text = indices_to_text(target_idx, idx2char)\n",
    "                \n",
    "                # Compute CER directly using token indices\n",
    "                cer, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                \n",
    "                # Update statistics\n",
    "                total_cer += cer\n",
    "                total_loss += loss.item()\n",
    "                total_edit_distance += edit_distance\n",
    "                \n",
    "                # Print info\n",
    "                print(\"-\" * 50)\n",
    "                print(f\"Sample {i * batch_size + b + 1}:\")\n",
    "                print(f\"Predicted text: {pred_text}\")\n",
    "                print(f\"Target text: {target_text}\")\n",
    "                print(f\"Edit Distance: {edit_distance}\")\n",
    "                print(f\"CER: {cer:.4f}\")\n",
    "                print(f\"CTC Loss: {loss.item():.4f}\")\n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "        # Write summary statistics\n",
    "        n_samples = len(data_loader.dataset)\n",
    "        avg_cer = total_cer / n_samples\n",
    "        avg_loss = total_loss / n_samples\n",
    "        avg_edit_distance = total_edit_distance / n_samples\n",
    "        \n",
    "        print(\"=== Summary Statistics ===\")\n",
    "        print(f\"Total samples: {n_samples}\")\n",
    "        print(f\"Average CER: {avg_cer:.4f}\")\n",
    "        print(f\"Average Edit Distance: {avg_edit_distance:.2f}\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Train and validate\n",
    "    for epoch in range(total_epochs):\n",
    "        train_one_epoch()\n",
    "        scheduler.adjust_lr(optimizer, epoch)\n",
    "        val_loss = evaluate_model(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Input shape: torch.Size([32, 1, 31, 128, 128])\n",
      "Batch 1 - Logits shape: torch.Size([32, 31, 238])\n",
      "Batch 1 - Loss: 10.8089599609375\n",
      "Batch 2 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 2 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 2 - Loss: 12.21811294555664\n",
      "Batch 3 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 3 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 3 - Loss: 6.976871490478516\n",
      "Batch 4 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 4 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 4 - Loss: 5.949538230895996\n",
      "Batch 5 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 5 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 5 - Loss: 5.508829116821289\n",
      "Batch 6 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 6 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 6 - Loss: 5.248185157775879\n",
      "Batch 7 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 7 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 7 - Loss: 5.169351577758789\n",
      "Batch 8 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 8 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 8 - Loss: 5.118333339691162\n",
      "Batch 9 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 9 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 9 - Loss: 5.157809257507324\n",
      "Batch 10 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 10 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 10 - Loss: 5.176445960998535\n",
      "Batch 11 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 11 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 11 - Loss: 5.1952314376831055\n",
      "Batch 12 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 12 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 12 - Loss: 5.19429349899292\n",
      "Batch 13 - Input shape: torch.Size([32, 1, 36, 128, 128])\n",
      "Batch 13 - Logits shape: torch.Size([32, 36, 238])\n",
      "Batch 13 - Loss: 5.093280792236328\n",
      "Batch 14 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 14 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 14 - Loss: 5.208019256591797\n",
      "Batch 15 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 15 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 15 - Loss: 5.026012420654297\n",
      "Batch 16 - Input shape: torch.Size([32, 1, 31, 128, 128])\n",
      "Batch 16 - Logits shape: torch.Size([32, 31, 238])\n",
      "Batch 16 - Loss: 5.204648971557617\n",
      "Batch 17 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 17 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 17 - Loss: 5.014163494110107\n",
      "Batch 18 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 18 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 18 - Loss: 4.974117279052734\n",
      "Batch 19 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 19 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 19 - Loss: 4.869850158691406\n",
      "Batch 20 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 20 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 20 - Loss: 5.0078125\n",
      "Batch 21 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 21 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 21 - Loss: 4.816829681396484\n",
      "Batch 22 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 22 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 22 - Loss: 4.911100387573242\n",
      "Batch 23 - Input shape: torch.Size([32, 1, 36, 128, 128])\n",
      "Batch 23 - Logits shape: torch.Size([32, 36, 238])\n",
      "Batch 23 - Loss: 4.9757161140441895\n",
      "Batch 24 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 24 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 24 - Loss: 4.868496894836426\n",
      "Batch 25 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 25 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 25 - Loss: 4.888541221618652\n",
      "Batch 26 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 26 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 26 - Loss: 4.948186874389648\n",
      "Batch 27 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 27 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 27 - Loss: 4.905889511108398\n",
      "Batch 28 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 28 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 28 - Loss: 4.686751365661621\n",
      "Batch 29 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 29 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 29 - Loss: 4.8650922775268555\n",
      "Batch 30 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 30 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 30 - Loss: 4.58071756362915\n",
      "Batch 31 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 31 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 31 - Loss: 4.783773422241211\n",
      "Batch 32 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 32 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 32 - Loss: 4.6206746101379395\n",
      "Batch 33 - Input shape: torch.Size([32, 1, 34, 128, 128])\n",
      "Batch 33 - Logits shape: torch.Size([32, 34, 238])\n",
      "Batch 33 - Loss: 5.049490451812744\n",
      "Batch 34 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 34 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 34 - Loss: 4.910240173339844\n",
      "Batch 35 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 35 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 35 - Loss: 4.660910606384277\n",
      "Batch 36 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 36 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 36 - Loss: 4.663401126861572\n",
      "Batch 37 - Input shape: torch.Size([32, 1, 31, 128, 128])\n",
      "Batch 37 - Logits shape: torch.Size([32, 31, 238])\n",
      "Batch 37 - Loss: 4.664681434631348\n",
      "Batch 38 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 38 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 38 - Loss: 4.787237167358398\n",
      "Batch 39 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 39 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 39 - Loss: 4.575640678405762\n",
      "Batch 40 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 40 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 40 - Loss: 4.9808759689331055\n",
      "Batch 41 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 41 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 41 - Loss: 4.827370643615723\n",
      "Batch 42 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 42 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 42 - Loss: 4.862814903259277\n",
      "Batch 43 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 43 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 43 - Loss: 4.5505757331848145\n",
      "Batch 44 - Input shape: torch.Size([32, 1, 36, 128, 128])\n",
      "Batch 44 - Logits shape: torch.Size([32, 36, 238])\n",
      "Batch 44 - Loss: 4.580716609954834\n",
      "Batch 45 - Input shape: torch.Size([32, 1, 31, 128, 128])\n",
      "Batch 45 - Logits shape: torch.Size([32, 31, 238])\n",
      "Batch 45 - Loss: 4.845269203186035\n",
      "Batch 46 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 46 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 46 - Loss: 4.666727542877197\n",
      "Batch 47 - Input shape: torch.Size([32, 1, 36, 128, 128])\n",
      "Batch 47 - Logits shape: torch.Size([32, 36, 238])\n",
      "Batch 47 - Loss: 4.528948783874512\n",
      "Batch 48 - Input shape: torch.Size([32, 1, 31, 128, 128])\n",
      "Batch 48 - Logits shape: torch.Size([32, 31, 238])\n",
      "Batch 48 - Loss: 4.751773834228516\n",
      "Batch 49 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 49 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 49 - Loss: 4.697354316711426\n",
      "Batch 50 - Input shape: torch.Size([32, 1, 36, 128, 128])\n",
      "Batch 50 - Logits shape: torch.Size([32, 36, 238])\n",
      "Batch 50 - Loss: 4.72569465637207\n",
      "Batch 51 - Input shape: torch.Size([2, 1, 31, 128, 128])\n",
      "Batch 51 - Logits shape: torch.Size([2, 31, 238])\n",
      "Batch 51 - Loss: 4.7120161056518555\n",
      "Debug - Reference tokens (16 tokens): [ 71 219  20  44   1 124  84 213   6  23   1  48  10  44  33   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 1:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَاوَمَٱشَّعْبِيَّهْٱلْيَمَنِيّْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 71 219  55 193  44 118 219 163 148  44 177  50   6 207]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 2:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَتْمَصَادِرُمَحَلِّيَّةٌ\n",
      "Edit Distance: 13\n",
      "CER: 0.9286\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 20 149  80 109  20 140  12 148   1  48 172 219 147 183   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 3:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَرَفَضَوَزِيرُٱلْخَارِجِيّْ\n",
      "Edit Distance: 11\n",
      "CER: 0.7333\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [  1  48 176  64  22  44  23   1  48  10  44  33   6  23 232  75 127  55\n",
      " 193]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 4:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلْحُكُومَهْٱلْيَمَنِيَّهْأَفْشَلَتْ\n",
      "Edit Distance: 16\n",
      "CER: 0.8421\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (11 tokens): [ 71 219  55 193 224  32  43  20 219  99  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 5:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَتْإِنَّمُوَاطِنْ\n",
      "Edit Distance: 10\n",
      "CER: 0.9091\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [215   2  35   1  48  43  71 219  20  44  23   1 124  84 213   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 6:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: بَيْنَٱلْمُقَاوَمَهْٱشَّعْبِيَّهْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [199 177 219  54  75   1 161  14  53   3 107 161]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 7:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تَحَالُفْٱدَّوْلِيّْضِدَّ\n",
      "Edit Distance: 11\n",
      "CER: 0.9167\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (9 tokens): [ 20  70  17 219 198 118 219  53 173]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 8:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَقُوَّاتُصَالِحْ\n",
      "Edit Distance: 8\n",
      "CER: 0.8889\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [197  55 219  53   1  48 224 129 149 219 221  12  53   3 199 185 219  24\n",
      "  55]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 9:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تِلَالِٱلْإِسْرَائِيلِيّْتَجَاهُّلَ\n",
      "Edit Distance: 16\n",
      "CER: 0.8421\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 44  84 141  22  48  43 177  41 158  43 143 134  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 10:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَعْزُولْمُحَمَّدْمُرْسِي\n",
      "Edit Distance: 12\n",
      "CER: 0.9231\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (9 tokens): [ 70  17 219 197 118 219  53 173  20]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 11:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قُوَّاتِصَالِحْوَ\n",
      "Edit Distance: 8\n",
      "CER: 0.8889\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 86 121 148  22  35 127 169 120  42  29  26  37 232 143 210]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 12:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: عِشْرُونَشَخْصًمِنْهُمْأَرْبْ\n",
      "Edit Distance: 15\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (9 tokens): [ 83 143 213   3   1  48  10  44  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 13:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: غَرْبِيّْٱلْيَمَنْ\n",
      "Edit Distance: 7\n",
      "CER: 0.7778\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (8 tokens): [  1  52 155  12   9  44 187  54]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 14:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلَّذِييُمَثِّلُ\n",
      "Edit Distance: 6\n",
      "CER: 0.7500\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 20 140  12 147   1 161  14  55 203  53 126 227  22  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 15:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَزِيرِٱدَّوْلَةِلِشُؤُونْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [224  32  70  17 219 197   1  30  95 219  42   1 132  22 147   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 16:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: إِنَّقُوَّاتِٱنِّظَامِٱسُّورِيّْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8750\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 43  22 185 138   1 133  55 219  43  88  55   2  64  37]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 17:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مُوجَزْٱسَّلَامُعَلَيْكُمْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 88  55  13  44  20 219  69  88  88 129  65 147   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 18:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: عَلَىمَوَاقِعَعَسْكَرِيّْ\n",
      "Edit Distance: 12\n",
      "CER: 0.9231\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [165  88 219  20 140  12 148   1  48 172 219 147 183   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 19:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: دَعَاوَزِيرُٱلْخَارِجِيَّهْ\n",
      "Edit Distance: 11\n",
      "CER: 0.7333\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [ 44 210  87  22 191   1 161  14  53   5 224  55  13  53  12 210  10 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 20:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَبْعُوثُٱدَّوْلِيُّإِلَىلِيبْيَا\n",
      "Edit Distance: 15\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [  1  48 232  42  12  29   1  48  88 219  37  53  48 231  44  42]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 21:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلْأَمِينْٱلْعَامْلِلْأُمَمِ\n",
      "Edit Distance: 11\n",
      "CER: 0.6875\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 34  22 215  44 163  12  35 203  35 219 210  53 129]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 22:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: نُوبَمَدِينَةِنَابْلِسْ\n",
      "Edit Distance: 11\n",
      "CER: 0.8462\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 65 232  32  27 219  55  37 199  64  29 213   1  30 129]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 23:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: كَأَنَّهَالَمْتَكُنْبِٱنِّسْ\n",
      "Edit Distance: 13\n",
      "CER: 0.9286\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (11 tokens): [ 80   2 118  48 213  29  80 143 177 219  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 24:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فَيْصَلْبِنْفَرْحَانْ\n",
      "Edit Distance: 9\n",
      "CER: 0.8182\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [185 146 219 236  83 219 149 219 200 148  22 134   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 25:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: جَرَّاءَغَارَاتٍرُوسِيَّهْ\n",
      "Edit Distance: 13\n",
      "CER: 0.9286\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [ 20 157  53  65  78  12  88  44  53   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 26:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَذَلِكَفِيعَمَلِيّْ\n",
      "Edit Distance: 9\n",
      "CER: 0.9000\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [199  29  93  12  42   1 161  14  55 203   1  48 224 129  55 219  42   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 27:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تَنْظِيمِٱدَّوْلَةِٱلْإِسْلَامِيّْ\n",
      "Edit Distance: 14\n",
      "CER: 0.7778\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [ 43  71 219  20  44 203   1  48 224 129  55 219  42   6  23 177  44 219\n",
      " 129]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 28:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مُقَاوَمَةِٱلْإِسْلَامِيَّهْحَمَاسْ\n",
      "Edit Distance: 16\n",
      "CER: 0.8421\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 80 219 165 193  44 118 219 163 148  99 211   6 207]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 29:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فَادَتْمَصَادِرُطِبِّيَّةٌ\n",
      "Edit Distance: 12\n",
      "CER: 0.9231\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [213 232  32  70  17 219 197   1  48 224 173 197]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 30:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: بِأَنَّقُوَّاتِٱلْإِحْتِ\n",
      "Edit Distance: 10\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (20 tokens): [  1  48 224  84  59 219  37   1 132  87  22 163   6  23 232  32   1  48\n",
      "  70  17]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 31:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلْإِعْلامْٱسُّعُودِيَّهْأَنَّٱلْقُوَّ\n",
      "Edit Distance: 17\n",
      "CER: 0.8500\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [232  28 215 219 234  42  29  71  35 219 203   1  48 185 140  12 149 203]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 32:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَنۢبَاءْمِنْقَنَاةِٱلْجَزِيرَةِ\n",
      "Edit Distance: 16\n",
      "CER: 0.8889\n",
      "CTC Loss: 4.7601\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [142 219 200  88 129  65 147   6  23 199 219 213  88  23  53]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 33:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: زَاتٍعَسْكَرِيَّهْتَابِعَهْلِ\n",
      "Edit Distance: 14\n",
      "CER: 0.9333\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [ 71 161  44 193  53  12 210  10 219 101  55 217]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 34:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَدَّمَتْلِيبْيَاطَلَبً\n",
      "Edit Distance: 10\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 65 219  35 193 101 219 221 149 219 201 177 143 213   6 207]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 35:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: كَانَتْطَائِرَاتٌحَرْبِيَّةٌ\n",
      "Edit Distance: 14\n",
      "CER: 0.9333\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (9 tokens): [ 71 219  55 193  44 118 219 163 151]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 36:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَتْمَصَادِرٌ\n",
      "Edit Distance: 8\n",
      "CER: 0.8889\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [ 44 129 227  22  53  12  35  88  29 157  53  60]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 37:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَسْؤُولِينَعَنْذَلِكْ\n",
      "Edit Distance: 11\n",
      "CER: 0.9167\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (6 tokens): [ 70 197  54  22  20 184]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 38:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قُتِلُووَجُ\n",
      "Edit Distance: 6\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [  1  48  44 210  87  22 192   1 161  14  53   6  53  48  10  44  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 39:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلْمَبْعُوثَٱدَّوْلِيَّلِلْيَمَنْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [224  32   1 196 178 219  54  80   1  52 155  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 40:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: إِنَّٱتَّحالُفَٱلَّذِي\n",
      "Edit Distance: 10\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [ 20  65 219  35  20 140  12 148   1  48 172 219 147 183   6 203]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 41:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَكَانَوَزِيرُٱلْخَارِجِيَّةِ\n",
      "Edit Distance: 12\n",
      "CER: 0.7500\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 20 177  41  55 149 221  12 129 215  55 163   6 205]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 42:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَحَمَّلَرَئِيسْبَلَدِيَّةَ\n",
      "Edit Distance: 12\n",
      "CER: 0.9231\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (21 tokens): [ 19 142 149 219 234   1  48 224 129 149 219 221  12  53   3 213  29  10\n",
      " 219  42  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 43:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وُزَرَاءْٱلْإِسْرَائِيلِيّْبِنْيَامِي\n",
      "Edit Distance: 18\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [172 219 147 183   6 203   1 145  22 134   3 134  12 143 183  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 44:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: خَارِجِيَّةِٱرُّوسِيّْسِيرْجِي\n",
      "Edit Distance: 14\n",
      "CER: 0.8750\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (11 tokens): [213 143  35 219  44 179  44 219 172  78  10]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 45:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: بِرْنَامَجْمَاخَفِيَ\n",
      "Edit Distance: 10\n",
      "CER: 0.9091\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 20  65 219  35  20 140  12 148   1 196 143 213  10 203]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 46:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَكَانَوَزِيرُٱتَّرْبِيَةِ\n",
      "Edit Distance: 12\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (11 tokens): [ 78  12   1  48  86 149 219  67  70 197  55]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 47:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فِيٱلْعِرَاقْقُتِلَ\n",
      "Edit Distance: 7\n",
      "CER: 0.6364\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 71 219  55 149 221  12 135   1  48  19 142 149 219 235]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 48:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَرَئِيسُٱلْوُزَرَاءِ\n",
      "Edit Distance: 10\n",
      "CER: 0.7143\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [199 149 219 183  86 215  84 165   1  48 224 121 197 215 219  63]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 49:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تَرَاجِعِبَعْدَٱلْإِشْتِبَاكِ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 44 169  54  22  84 118 219  53 173  78  12  43 158]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 50:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَخْلُوعْصَالِحْفِيمُدْ\n",
      "Edit Distance: 12\n",
      "CER: 0.9231\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [175  48  80 127  44 219  53   1  48 232  96  55 129]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 51:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: حِلْفَشَمَالِٱلْأَطْلَسْ\n",
      "Edit Distance: 11\n",
      "CER: 0.8462\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [  1 122  12 169  43 177  41 158 213  29 142 219   8 158]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 52:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱشِّيخْمُحَمَّدْبِنْزَايِدْ\n",
      "Edit Distance: 11\n",
      "CER: 0.7857\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [  1 133  55 219  43  88  55   2  64  37]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 53:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱسَّلَامُعَلَيْكُمْ\n",
      "Edit Distance: 9\n",
      "CER: 0.9000\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [197  80 219  88  88 165 163   1  48 224 118 219 215 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 54:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تِفَاعَعَدَدِٱلْإِصَابَا\n",
      "Edit Distance: 11\n",
      "CER: 0.7857\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [ 12  53  12 121  10 219   1  48 176  22 190  12  20  70  17 219 193]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 55:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: يلِيشْيَاٱلْحُوثِيوَقُوَّاتْ\n",
      "Edit Distance: 13\n",
      "CER: 0.7647\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 20 224  32  42  29  83   2 147   1  48  44  84  70  22]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 56:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَإِنَّمِنْغَيْرِٱلْمَعْقُو\n",
      "Edit Distance: 12\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 55  63  32  20 140  12 149   1  48 172 219 147 183   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 57:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: لَكِنَّوَزِيرَٱلْخَارِجِيّْ\n",
      "Edit Distance: 10\n",
      "CER: 0.7143\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [177 136  29  35 111 149   1  52  23  71 158 149  80 109]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 58:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: حَسَنْنَصْرَٱلَّهْقَدْرَفَضَ\n",
      "Edit Distance: 13\n",
      "CER: 0.9286\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [199  29  93  12  42   1 161  14  55  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 59:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تَنْظِيمِٱدَّوْلَهْ\n",
      "Edit Distance: 8\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 43  22 185 138   1 133  55 219  43  88  55   2  64  37]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 60:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مُوجَزْٱسَّلَامُعَلَيْكُمْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [101  10 149 219  35   1 145  22 134   4   1 129 199  23 165  75]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 61:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: طَيَرَانَٱرُّوسِيِّٱسْتَهْدَفْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8750\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [ 44 179  53 134   1  48  19 142 149 219 234   1 132  22 165 219  33   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 62:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَجْلِسِٱلْوُزَرَاءْٱسُّودَانِيّْ\n",
      "Edit Distance: 15\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [136  52 175  12   1  48 176  22 190  12  20  70  17 219 193]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 63:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: سَلَّحِيٱلْحُوثِيوَقُوَّاتْ\n",
      "Edit Distance: 11\n",
      "CER: 0.7333\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 78  12   1  48  86 149 219  67 199  95 219  27 143]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 64:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فِيٱلْعِرَاقْتَظَاهَرْ\n",
      "Edit Distance: 9\n",
      "CER: 0.6923\n",
      "CTC Loss: 4.7421\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [176  22 190  12  20  70  17 219 197   1 146 221  12 134   1  48  44]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 65:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: حُوثِيوَقُوَّاتِٱرَّئِيسِٱلْمَ\n",
      "Edit Distance: 13\n",
      "CER: 0.7647\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [ 88 219 116  44 203   1  48  10  44  33   6  23 118  29  88 219 234]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 66:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: عَاصِمَةِٱلْيَمَنِيَّهْصَنْعَاءْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 55 219 158  35 173  20   1  48 224 129 197  67 149 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 67:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: لَادْنَحْوَٱلْإِسْتِقْرَا\n",
      "Edit Distance: 11\n",
      "CER: 0.7857\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [224  12 149 219  33   3  88  53  12 149 213  12  86  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 68:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: إِيرَانِيّْعَلِيرَبِيعِي\n",
      "Edit Distance: 13\n",
      "CER: 0.9286\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [199 149  65 213 127 230  33   1  48 215 143  35 219  44 179]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 69:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تَرَكَبِشَأْنِٱلْبَرْنَامَجْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [  1 146 221  12 136   1 145  22 134   3  75  55 219 163  12  42 143]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 70:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱرَّئِيسَٱرُّوسِيّْفْلَادِيمِرْ\n",
      "Edit Distance: 13\n",
      "CER: 0.7647\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [149 174 215 193   1  48 231  44  43   1  48  43 196 175 158]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 71:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: رَحَّبَتْٱلْأُمَمُٱلْمُتَّحِدْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [232  42  12 147  12  63   3 164  22  35 219  48 163 193 149 219  37 210]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 72:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَمِيرِيكِيّْدُونَالْدِتْرَامْبْ\n",
      "Edit Distance: 15\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [185  44 219  88 203   1  48 176  22 190  12  78  12   1  48  10  44  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 73:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: جَمَاعَةِٱلْحُوثِيفِيٱلْيَمَنْ\n",
      "Edit Distance: 14\n",
      "CER: 0.7778\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [  1 146 221  12 135   1 195 143  63   3 149 185 210 101   4 210]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 74:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱرَّئِيسُٱتُّرْكِيّْرَجَبْطَيِّبْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [ 44 210  87  22 190   1 161  14  53   3 224  55  13 135  22 143  10 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 75:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَبْعُوثِٱدَّوْلِيّْإِلَىسُورْيَا\n",
      "Edit Distance: 16\n",
      "CER: 0.8889\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [232 111 165 149   1  48  44 179  53 135   1 144 222 219 134   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 76:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَصْدَرَٱلْمَجْلِسُٱرِّئَاسِيّْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [101 219 221 149 219 197   1 196 177 219  54  78   1 161  14  53   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 77:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: طَائِرَاتِٱتَّحَالُفِٱدَّوْلِيّْ\n",
      "Edit Distance: 15\n",
      "CER: 0.8824\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [192  44 219  33  12  35 127 169 120  43  29 156   1  29 163  55 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 78:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ثَمَانِينَشَخْصًمُنْذُٱنْدِلَا\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [  1 146 221  12 135   1 195 143  63   5 149 185 210]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 79:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱرَّئِيسُٱتُّرْكِيُّرَجَبْ\n",
      "Edit Distance: 10\n",
      "CER: 0.7692\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [ 35  93  12 149  26   1  48 224  12 149 219  33   3 177 136  29 148  22\n",
      " 173]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 80:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: نَظِيرَهُٱلْإِيرَانِيّْحَسَنْرُوحْ\n",
      "Edit Distance: 15\n",
      "CER: 0.7895\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [165  14  55 203   1  48 224 129  55 219  42   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 81:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: دَوْلَةِٱلْإِسْلَامِيَّهْ\n",
      "Edit Distance: 10\n",
      "CER: 0.7692\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [163  80 219  86   1  48  86 149 219  69   6  23 232  84  55  35 193]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 82:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: دِفَاعِٱلْعِرَاقِيَّهْأَعْلَنَتْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [ 70  17 219 198   1  48  43  88 219 149 109  23   1  48  43 136  52 177\n",
      "  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 83:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قُوَّاتُٱلْمُعَارَضَهْٱلْمُسَلَّحَهْ\n",
      "Edit Distance: 16\n",
      "CER: 0.8421\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [185 140  12 149  23 213 232  32  70  17 219 200]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 84:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: جَزِيرَهْبِأَنَّقُوَّاتٍ\n",
      "Edit Distance: 10\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [232  62 165   1 146 221  12 135   1  48 232  37 147  12  63   5]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 85:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَكَّدَٱرَّئِيسُٱلْأَمْرِيكِيُّ\n",
      "Edit Distance: 12\n",
      "CER: 0.7500\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [199  29  93  12  42   1 161  14  55  23  78  12  44 163  12  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 86:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تَنْظِيمِٱدَّوْلَهْفِيمَدِينْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8750\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [232  84  55  35 232  32 149 221  12 136   1  48 176  64  22  44 203]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 87:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَعْلَنَأَنَّرَئِيسَٱلْحُكُومَةِ\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (9 tokens): [185  29  22 215   1  48  10  44  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 88:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: جَنْوبَٱلْيَمَنْ\n",
      "Edit Distance: 7\n",
      "CER: 0.7778\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 55  63  32  20  65 219  55 205 148  22   2 199 143]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 89:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: لَكِنَّوَكَالَةَرُويْتَرْ\n",
      "Edit Distance: 12\n",
      "CER: 0.9231\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [149 221  12 129   1  48  44 179  53 134   1  48  88 129  65 147  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 90:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: رَئِيسْٱلْمَجْلِسِٱلْعَسْكَرِي\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 71 219  55   1  48  43 199 177 159 191 213 129  42]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 91:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَٱلْمُتَحَدِّثُبِسْمِ\n",
      "Edit Distance: 11\n",
      "CER: 0.8462\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 71 219  55 149 221  12 135   1  48  19 142 149 219 234]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 92:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَرَئِيسُٱلْوُزَرَاءْ\n",
      "Edit Distance: 10\n",
      "CER: 0.7143\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 43 177  41 158  88  53  12   1  48  44  67 163 125  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 93:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مُحَمَّدْعَلِيٱلْمَقْدِشِي\n",
      "Edit Distance: 11\n",
      "CER: 0.7857\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (9 tokens): [ 20 231 116  12 215  88 127 149 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 94:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَأُصِيبَعَشَرَا\n",
      "Edit Distance: 7\n",
      "CER: 0.7778\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 71 111  78 101 219 221 149 219 197   1 196 177 219  54  78]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 95:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَصْفِطَائِرَاتِٱتَّحَالُفِ\n",
      "Edit Distance: 13\n",
      "CER: 0.8667\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (20 tokens): [176  22 190  12  20  70  17 219 197   1 146 221  12 134   1  48  44 169\n",
      "  54  22]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 96:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: حُوثِيوَقُوَّاتِٱرَّئِيسِٱلْمَخْلُو\n",
      "Edit Distance: 16\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.4160\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (20 tokens): [  1 159  80 219  86   1  48 224  12 149 219  33   3 232  32   1 115 219\n",
      " 148  22]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 97:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱدِّفَاعِٱلْإِيرَانِيّْأَنَّٱصَّارُو\n",
      "Edit Distance: 16\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [  1  32 219  99  70 213 129  42   1  48 172 219 147 183   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 98:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱنَّاطِقُبِسْمِٱلْخَارِجِيَّهْ\n",
      "Edit Distance: 12\n",
      "CER: 0.7500\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [  1 146 221  12 135   1 145  22 134   5  75  55 219 163  12  42 143]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 99:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱرَّئِيسُٱرُّوسِيُّفْلَادِيمِرْ\n",
      "Edit Distance: 13\n",
      "CER: 0.7647\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [127 149 219 201  42  35   1  48  78  55 129  96]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 100:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: شَرَاتٌمِنَٱلْفِلَسْطْ\n",
      "Edit Distance: 10\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (7 tokens): [78 12  1 48 10 44 29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 101:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فِيٱلْيَمَنْ\n",
      "Edit Distance: 4\n",
      "CER: 0.5714\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (11 tokens): [ 78  12   1  48  86 149 219  67  71 219  55]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 102:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فِيٱلْعِرَاقْقَالَ\n",
      "Edit Distance: 7\n",
      "CER: 0.6364\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [213 232  32   1 132  87  22 163   6  23  71 158]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 103:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: بِأَنَّٱسُّعُودِيَّهْقَدْ\n",
      "Edit Distance: 11\n",
      "CER: 0.9167\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [ 18 142 219 149 204   1 159  80 219  86   1  48 232  42  12 147  12  63\n",
      "  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 104:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وِزَارَةُٱدِّفَاعِٱلْأَمِيرِيكِي\n",
      "Edit Distance: 16\n",
      "CER: 0.8421\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [232  84  55  35   1  48 185   2 126   1  48  20 101  33   5]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 105:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَعْلَنَٱلْجَيْشُٱلْوَطَنِيُّ\n",
      "Edit Distance: 12\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (22 tokens): [ 71 219  55   1 146 221  12 135   1 145  22 134   5  75  55 219 163  12\n",
      "  42 143 214  22]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 106:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَٱرَّئِيسُٱرُّوسِيُّفْلَادِيمِرْبُو\n",
      "Edit Distance: 18\n",
      "CER: 0.8182\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [199  29  93  12  42   1 161  14  55  23  78  12 135  22 143  10 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 107:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تَنْظِيمِٱدَّوْلَهْفِيسُورْيَا\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [232  23  58 213  64  37 224  55  13  43  22 185 138]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 108:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَهْلًبِكُمْإِلَىمُوجَزْ\n",
      "Edit Distance: 13\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [ 88  55  13 224 194  80 219  72   9 197  12 176]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 109:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: عَلَىإِتِّفَاقٍيُتِيحُ\n",
      "Edit Distance: 11\n",
      "CER: 0.9167\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [ 44 118 219 163 148   1  48 185 140  12 149  23 232  32  70  17 219 197]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 110:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَصَادِرُٱلْجَزِيرَهْأَنَّقُوَّاتِ\n",
      "Edit Distance: 15\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [136 215 213  44 219  20 118  80 193  26]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 111:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: سَبَبِمَاوَصَفَتْهُ\n",
      "Edit Distance: 9\n",
      "CER: 0.9000\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [185   2 121   1  48  20 101  33   3 215  55  83 193]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 112:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: جَيْشْٱلْوَطَنِيّْبَلَغَتْ\n",
      "Edit Distance: 11\n",
      "CER: 0.8462\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [ 71 219  55  20 140  12 148   1 161  14  55 203   1 132  87  22 163   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 113:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَوَزِيرُٱدَّوْلَةِٱسُّعُودِيّْ\n",
      "Edit Distance: 16\n",
      "CER: 0.8889\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (6 tokens): [  1 132  22 147   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 114:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱسُّورِيَّهْ\n",
      "Edit Distance: 5\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [ 20 140  12 148   1  48 172 219 147 183   6 203   1  48 231 143 164  33\n",
      "  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 115:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَزِيرُٱلْخَارِجِيَّةِٱلْأُرْدُنِي\n",
      "Edit Distance: 15\n",
      "CER: 0.7895\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 20  78  12  43 177 219  80  95 203   1  48 184  22  75]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 116:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَفِيمُحَافَظَةِٱلْجُوفْ\n",
      "Edit Distance: 11\n",
      "CER: 0.7857\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [232 138  44 203   1  48  10  44  33   6  23 199  35 219 141  48]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 117:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَزْمَةِٱلْيَمَنِيَّهْتَنَازُلْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [ 20  70  17 219 193   1 146 221  12 129]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 118:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَقُوَّاتْٱرَّئِيسْ\n",
      "Edit Distance: 9\n",
      "CER: 0.9000\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [ 20 231 116  12 215   1  48  42 222 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 119:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَأُصِيبَٱلْمِئَا\n",
      "Edit Distance: 6\n",
      "CER: 0.6000\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [177 118 219 163   1  48  10  14  37 224  55  13   1  50  71 219 234]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 120:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: حَصَادِٱلْيَوْمْإِلَىٱلِّقَاءْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (7 tokens): [42 35  1 48 10 14 37]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 121:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مِنَٱلْيَوْمْ\n",
      "Edit Distance: 5\n",
      "CER: 0.7143\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (8 tokens): [224  55  13   1  50  71 219 234]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 122:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: إِلَىٱلِّقَاءْ\n",
      "Edit Distance: 6\n",
      "CER: 0.7500\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [170  55 219  55  27 219   1  48 185   2 126   1  48  20 101  33   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 123:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: خِلَالَهَاٱلْجَيْشُٱلْوَطَنِيّْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [199  78  12   1  48  18  55 219  10 219 198   1  48  43 199 173]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 124:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تَفِيٱلْوِلَايَاتُٱلْمُتَحْ\n",
      "Edit Distance: 12\n",
      "CER: 0.7500\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [  1  48 232  42  12  33   1  48  88 219  37  53  48 231  44  37]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 125:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلْأَمِينِٱلْعَامْلِلْأُمَمْ\n",
      "Edit Distance: 11\n",
      "CER: 0.6875\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [149 221  12 129 164  22  35 219  48 163 193 149 219  37 210]\n",
      "Debug - Hypothesis tokens (6 tokens): [np.int64(1), np.int64(23), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 126:\n",
      "Predicted text: ٱهْيٱلْا\n",
      "Target text: رَئِيسْدُونَالْدِتْرَامْبْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [127  44 219  53   3   1  48 213  55 219 158 185 146 219 236]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 127:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: شَمَالِيّْٱلْبِلَادْجَرَّاءَ\n",
      "Edit Distance: 12\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [ 43 149 219 134  54   1  48 185 140  12 149  23  78  12 232  29  71 149\n",
      "  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 128:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مُرَاسِلُٱلْجَزِيرَهْفِيأَنْقَرَهْ\n",
      "Edit Distance: 17\n",
      "CER: 0.8947\n",
      "CTC Loss: 4.6344\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (11 tokens): [127  44 219  53   1  48 232  96  55 134  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 129:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: شَمَالِٱلْأَطْلَسِي\n",
      "Edit Distance: 9\n",
      "CER: 0.8182\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [224  32  55 165  13   1  48  18  55 219  10 219 197   1  48  43 196 175\n",
      " 158]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 130:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: إِنَّلَدَىٱلْوِلَايَاتِٱلْمُتَّحِدْ\n",
      "Edit Distance: 16\n",
      "CER: 0.8421\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [ 42  12  35 219 221  27 219   1  52 155  12  10  84 199  42 164]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 131:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مِينَائِهَاٱلَّذِييَعْتَمِدُ\n",
      "Edit Distance: 14\n",
      "CER: 0.8750\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [215 219 234  42  29  71  35 219 203   1  48 185 140  12 149  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 132:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: بَاءْمِنْقَنَاةِٱلْجَزِيرَهْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8750\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [  1  48  69  10 219 163   6  23  78  12   1 196  29  93  12  37]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 133:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلْقِيَادِيَّهْفِيٱتَّنْظِيمْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 20  68  88 219   1  48  10  14  37 232 143 215  88 202]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 134:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَقَّعَاٱلْيَوْمْأَرْبَعَةْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [177 219  54  78   1 161  14  53   3 213  69  10 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 135:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: حَالُفِٱدَّوْلِيّْبِقِيَا\n",
      "Edit Distance: 11\n",
      "CER: 0.8462\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 20 118  80   1  48  44 179  53 135   1  48 232  84  55  13]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 136:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَصَفَٱلْمَجْلِسُٱلْأَعْلَى\n",
      "Edit Distance: 12\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [177 136 215  44 219  35  71  55  43 149 219 134  54]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 137:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: حَسَبَمَانَقَلَمُرَاسِلُ\n",
      "Edit Distance: 12\n",
      "CER: 0.9231\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (7 tokens): [ 70 197  55 172  37 136 204]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 138:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قُتِلَخَمْسَةُ\n",
      "Edit Distance: 7\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [ 44  67 199  53  35 173  14  42 222 203]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 139:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَقْتَلِنَحْوْمِئَةِ\n",
      "Edit Distance: 10\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [  1 146 221  12 135   1 195 143  63   3 232 143 164  22  83 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 140:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱرَّئِيسُٱتُّرْكِيّْأَرْدُوغَا\n",
      "Edit Distance: 12\n",
      "CER: 0.7500\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [177 118  55 193   1  48 185 140  12 149  23  88  55  13  33 111  75]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 141:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: حَصَلَتْٱلْجَزِيرَهْعَلَىنِصْفْ\n",
      "Edit Distance: 15\n",
      "CER: 0.8824\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 71 219  55 149 221  12 135   1  48  19 142 149 219 235]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 142:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَرَئِيسُٱلْوُزَرَاءِ\n",
      "Edit Distance: 10\n",
      "CER: 0.7143\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [  1  48  26 184  22  44  55  37   9 129  78 149  88  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 143:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلْهُجُومَلَمْيُسْفِرَعَنْ\n",
      "Edit Distance: 13\n",
      "CER: 0.9286\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [231 210 147  44   1  48  88 219  44   1  48  44 219 107  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 144:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أُبْرِمَٱلْعَامَٱلْمَاضِي\n",
      "Edit Distance: 11\n",
      "CER: 0.7333\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 53  70  17 219 193   1  30  95 219  42   1 132  22 147   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 145:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: لِقُوَّاتْٱنِّظَامِٱسُّورِيّْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8667\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [232  29 199  55  37 199 129 199  42  84]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 146:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَنْتَلَمْتَسْتَمِعْ\n",
      "Edit Distance: 10\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 65  44  88 199  29  93  12  42   1 161  14  55 203]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 147:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: كَمَعَتَنْظِيمِٱدَّوْلَةِ\n",
      "Edit Distance: 11\n",
      "CER: 0.8462\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [  9 163  12  34   1 146 221  12 129   1  48 232  37 147  12  63   5]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 148:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: يُدِينُٱرَّئِيسْٱلْأَمْرِيكِيُّ\n",
      "Edit Distance: 13\n",
      "CER: 0.7647\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (7 tokens): [  1  52 197  12  70 197  55]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 149:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلَّتِيقُتِلَ\n",
      "Edit Distance: 5\n",
      "CER: 0.7143\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [ 43  71 219  14  44   1 124  84 213   6  23   1  48  10  44  33   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 150:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مُقَاوْمَٱشَّعْبِيَّهْٱلْيَمَنِيَّهْ\n",
      "Edit Distance: 15\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [ 20  75 163   1  48  43  88 219 149 109 203   1 132  22 147   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 151:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَفْدِٱلْمُعَارَضَةِٱسُّورِيَّهْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8235\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [ 78  12  42  29 101  71 206  83 143 210]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 152:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فِيمِنْطَقَةٍغَرْبْ\n",
      "Edit Distance: 9\n",
      "CER: 0.9000\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 78  12  44 163  12  35 203   1  48 176 165   2 165  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 153:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فِيمَدِينَةِٱلْحُدَيْدَهْ\n",
      "Edit Distance: 11\n",
      "CER: 0.7857\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 78  12   1  48  88 219 116  44 203 101 149 219 210  54 129]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 154:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فِيٱلْعَاصِمَةِطَرَابْلُسْ\n",
      "Edit Distance: 11\n",
      "CER: 0.7333\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 71 219  55   1  48 185   2 126   1  48  20 101  33]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 155:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَٱلْجَيْشُٱلْوَطَنِ\n",
      "Edit Distance: 10\n",
      "CER: 0.7692\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 86 121 148  22  35 127 169 120  78  12   1  29  78 179]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 156:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: عِشْرُونَشَخْصًفِيٱنْفِجْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (22 tokens): [ 19 142 149 219 234   1  48 224 129 149 219 221  12  53   3 213  29  10\n",
      " 219  42  12  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 157:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وُزَرَاءْٱلْإِسْرَائِيلِيّْبِنْيَامِينْ\n",
      "Edit Distance: 19\n",
      "CER: 0.8636\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 10  29  80 153 213 127 230  33   1  48 224 194  80 219  67]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 158:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: يَنْفَذْبِشَأْنِٱلْإِتِّفَاقْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8000\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [ 44 210  87  22 191   1  48 231  44  37   1  48  43 196 175 158]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 159:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَبْعُوثُٱلْأُمَمْٱلْمُتَّحِدْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (20 tokens): [224 196  27  44 193   1  48 172 219 147 183   6 204   1  48  25  29 163\n",
      "   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 160:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: إِتَّهَمَتْٱلْخَارِجِيَّةُٱلْهِنْدِيَّهْ\n",
      "Edit Distance: 17\n",
      "CER: 0.8500\n",
      "CTC Loss: 4.6973\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (11 tokens): [ 88 127 149 127 169 120 215   2  35  26  37]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 161:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: عَشَرَشَخْصًبَيْنَهُمْ\n",
      "Edit Distance: 11\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [ 44  88   1  48  43  88 219 149 109  23   1 132  22 147   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 162:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَعَٱلْمُعَارَضَهْٱسُّورِيَّهْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [ 43  88 219 149 109 198  26  78  12   1  48  44 163  12  35  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 163:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مُعَارَضَتُهُفِيٱلْمَدِينَهْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [ 44  67 199  53   1 124   2 169  20 175  12 158]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 164:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَقْتَلِٱشَّيْخْوَحِيدْ\n",
      "Edit Distance: 11\n",
      "CER: 0.9167\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 43  55  68 210  88  53  12   1  48  65  10 219  53  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 165:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مُلَقَّبْعَلِيٱلْكَيَالِي\n",
      "Edit Distance: 10\n",
      "CER: 0.7143\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (7 tokens): [ 88  29 232  29 125 101  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 166:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: عَنْأَنْشِطَهْ\n",
      "Edit Distance: 7\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [ 95  27 149   1  48  10  14  37  53  88 127 143]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 167:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ظَهَرَٱلْيَوْمْلِعَشَرْ\n",
      "Edit Distance: 10\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [149 221  12 135   1  48  19 142 149 219 234   1  51 210  35 219  33   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 168:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: رَئِيسُٱلْوُزَرَاءْٱلُّبْنَانِيّْ\n",
      "Edit Distance: 14\n",
      "CER: 0.7778\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 69 101 219 149 219 193  78  12  64  22 211  29  27 219 179]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 169:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قِطَارَاتْفِيكُوبِّنْهَاجْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8667\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (19 tokens): [232  84  55  35 193   1  48 176  64  22  44 204   1  48 231 143 164  33\n",
      "  12]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 170:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَعْلَنَتْٱلْحُكُومَةُٱلْأُرْدُنِي\n",
      "Edit Distance: 16\n",
      "CER: 0.8421\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [232  80 219 165  43 149 219 134  54   1  48 185 140  12 149  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 171:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَفَادَمُرَاسِلُٱلْجَزِيرَهْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8750\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 70  17 219 200  88 129  65 147   6  23   1  67 199 177  37]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 172:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قُوَّاتٍعَسْكَرِيَّهْٱقْتَحَمْ\n",
      "Edit Distance: 14\n",
      "CER: 0.9333\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (15 tokens): [ 44 210  87  22 191   1 161  14  53   5 224  55  13 135  22]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 173:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَبْعُوثُٱدَّوْلِيُّإِلَىسُو\n",
      "Edit Distance: 14\n",
      "CER: 0.9333\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [190   4  12  35  20  70  17 219 197   1 146 221  12 134   1  48  44 169]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 174:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ثِيِّينَوَقُوَّاتِٱرَّئِيسِٱلْمَخْ\n",
      "Edit Distance: 14\n",
      "CER: 0.7778\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (7 tokens): [ 20 157  53  65  55 165  13]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 175:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَذَلِكَلَدَى\n",
      "Edit Distance: 7\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [199  29  93  12  42   1 161  14  55  23  78  12 135  22]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 176:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: تَنْظِيمِٱدَّوْلَهْفِيسُو\n",
      "Edit Distance: 12\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [ 88  55  13   1 132  87  22 163   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 177:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: عَلَىٱسُّعُودِيَّهْ\n",
      "Edit Distance: 9\n",
      "CER: 0.9000\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [ 42  29 183  27 197  25 232  84  55  35]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 178:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مِنْجِهَتِهِأَعْلَنَ\n",
      "Edit Distance: 10\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (9 tokens): [ 20  70  17 219 197 118 219  53 173]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 179:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وَقُوَّاتِصَالِحْ\n",
      "Edit Distance: 8\n",
      "CER: 0.8889\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [  1  28 215 219 234  42  29  71  35 219 203   1  48 185 140  12 149  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 180:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱنۢبَاءْمِنْقَنَاةِٱلْجَزِيرَهْ\n",
      "Edit Distance: 15\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [  1  48  88 219  41  23   1 132  87  22 163   6  23 224 111 165 219 149]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 181:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱلْعَامَّهْٱسُّعُودِيَّهْإِصْدَارَ\n",
      "Edit Distance: 15\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [ 18 142 219 149 203   1 159  80 219  86   1  48 232  75  83 219  33   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 182:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وِزَارَةِٱدِّفَاعِٱلْأَفْغَانِيّْ\n",
      "Edit Distance: 14\n",
      "CER: 0.7778\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [149 174 215 193  18 142 219 149 204   1  48 172 219 147 183   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 183:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: رَحَّبَتْوِزَارَةُٱلْخَارِجِيّْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (9 tokens): [ 78  12 232  44 219  33   1  52  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 184:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: فِيأَمَانِٱلَّهْ\n",
      "Edit Distance: 7\n",
      "CER: 0.7778\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (21 tokens): [  1 143  65 219  29   1  48 232  42  12 147  12  63   6  23   1  48  43\n",
      " 121 199 143]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 185:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱرْكَانْٱلْأَمِيرِيكِيَّهْٱلْمُشْتَرْ\n",
      "Edit Distance: 17\n",
      "CER: 0.8095\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [147 222 219 136 203   1  48  19 142 149 219 234  20 157  53  65]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 186:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: رِئَاسَةِٱلْوُزَرَاءْوَذَلِكَ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 44  88   1  48  86 149 219  67  20 135  22 143  10 219]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 187:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: مَعَٱلْعِرَاقْوَسُورْيَا\n",
      "Edit Distance: 11\n",
      "CER: 0.7857\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (17 tokens): [224  32  70  17 219 197   1  48 232  37  33   1  84 199  71  55 193]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 188:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: إِنَّقُوَّاتِٱلْأَمْنِٱعْتَقَلَتْ\n",
      "Edit Distance: 15\n",
      "CER: 0.8824\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 88 149 213   6 203  78  12 224 129 149 219 221  12  48]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 189:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: عَرَبِيَّةِفِيإِسْرَائِيلْ\n",
      "Edit Distance: 12\n",
      "CER: 0.8571\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (12 tokens): [ 55 165  13   1  48 231  44  42   1  48  43 196]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 190:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: لَدَىٱلْأُمَمِٱلْمُتَّ\n",
      "Edit Distance: 9\n",
      "CER: 0.7500\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (18 tokens): [232  75 149 219 158   1  48  70  17 219 197   1  48 232  37  33   6  23]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 191:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَفْرَادْٱلْقُوَّاتِٱلْأَمْنِيَّهْ\n",
      "Edit Distance: 15\n",
      "CER: 0.8333\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (10 tokens): [127 169 111  70 197  54  22  88  55  13]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 192:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: شَخْصْقُتِلُوعَلَى\n",
      "Edit Distance: 10\n",
      "CER: 1.0000\n",
      "CTC Loss: 4.6998\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [183   6 203   1  48 224  12 149 219  33   3  43 177  38]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 55:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: جِيَّةِٱلْإِيرَانِيّْمُحَمّْ\n",
      "Edit Distance: 11\n",
      "CER: 0.7857\n",
      "CTC Loss: 4.5985\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [ 19 142 219 149 204   1 159  80 219  86   1 132  87  22 163   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 56:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وُزَارَةُٱدِّفَاعِٱسُّعُودِيّْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8750\n",
      "CTC Loss: 4.5985\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [172  37 134  12  35 127 169 120  78  12 135  22 147   2]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 57:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: خَمْسِينَشَخْصًفِيسُورِيْ\n",
      "Edit Distance: 13\n",
      "CER: 0.9286\n",
      "CTC Loss: 4.5985\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [232  29  71 149   1  48  10  14  37  44  88   1 146 221  12 129]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 58:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: أَنْقَرَٱلْيَوْمْمَعَٱرَّئِيسْ\n",
      "Edit Distance: 14\n",
      "CER: 0.8750\n",
      "CTC Loss: 4.5985\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (13 tokens): [ 71 219  55  20 140  12 148   1  48 172 219 147 179]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 59:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَوَزِيرُٱلْخَارِجْ\n",
      "Edit Distance: 9\n",
      "CER: 0.6923\n",
      "CTC Loss: 4.5985\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (14 tokens): [ 27 157   1  48  88 219  37  78  12   1  48  10  44  29]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 60:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: هَذَٱلْعَامْفِيٱلْيَمَنْ\n",
      "Edit Distance: 10\n",
      "CER: 0.7143\n",
      "CTC Loss: 4.5985\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (16 tokens): [ 18 142 219 149 204   1 159  80 219  86   1  48  10  44  33   3]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 61:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: وِزَارَةُٱدِّفَاعِٱلْيَمَنِيّْ\n",
      "Edit Distance: 13\n",
      "CER: 0.8125\n",
      "CTC Loss: 4.5985\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (20 tokens): [  1 123 143 101  23   1  48 232  42  12 147  12  63   6  23  53  44 210\n",
      "  35  13]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 62:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: ٱشُّرْطَهْٱلْأَمِيرِيكِيَّهْلِمَبْنَى\n",
      "Edit Distance: 17\n",
      "CER: 0.8500\n",
      "CTC Loss: 4.5985\n",
      "--------------------------------------------------\n",
      "Debug - Reference tokens (22 tokens): [ 71 219  55   1 146 221  12 135   1  48 224  12 149 219  33   3 177 136\n",
      "  29 148  22 173]\n",
      "Debug - Hypothesis tokens (5 tokens): [np.int64(1), np.int64(12), np.int64(1), np.int64(48), np.int64(219)]\n",
      "--------------------------------------------------\n",
      "Sample 63:\n",
      "Predicted text: ٱيٱلْا\n",
      "Target text: قَالَٱرَّئِيسُٱلْإِيرَانِيّْحَسَنْرُوحْ\n",
      "Edit Distance: 17\n",
      "CER: 0.7727\n",
      "CTC Loss: 4.5985\n",
      "--------------------------------------------------\n",
      "=== Summary Statistics ===\n",
      "Total samples: 201\n",
      "Average CER: 0.8339\n",
      "Average Edit Distance: 12.02\n",
      "Average Loss: 4.6556\n",
      "Epoch 1/80, Val Loss: 133.6822\n",
      "Batch 1 - Input shape: torch.Size([32, 1, 36, 128, 128])\n",
      "Batch 1 - Logits shape: torch.Size([32, 36, 238])\n",
      "Batch 1 - Loss: 4.9407453536987305\n",
      "Batch 2 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 2 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 2 - Loss: 4.933895587921143\n",
      "Batch 3 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 3 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 3 - Loss: 4.5987348556518555\n",
      "Batch 4 - Input shape: torch.Size([32, 1, 35, 128, 128])\n",
      "Batch 4 - Logits shape: torch.Size([32, 35, 238])\n",
      "Batch 4 - Loss: 4.567309379577637\n",
      "Batch 5 - Input shape: torch.Size([32, 1, 36, 128, 128])\n",
      "Batch 5 - Logits shape: torch.Size([32, 36, 238])\n",
      "Batch 5 - Loss: 4.610547065734863\n",
      "Batch 6 - Input shape: torch.Size([32, 1, 38, 128, 128])\n",
      "Batch 6 - Logits shape: torch.Size([32, 38, 238])\n",
      "Batch 6 - Loss: 4.74992561340332\n",
      "Batch 7 - Input shape: torch.Size([32, 1, 36, 128, 128])\n",
      "Batch 7 - Logits shape: torch.Size([32, 36, 238])\n",
      "Batch 7 - Loss: 4.582650184631348\n",
      "Batch 8 - Input shape: torch.Size([32, 1, 36, 128, 128])\n",
      "Batch 8 - Logits shape: torch.Size([32, 36, 238])\n",
      "Batch 8 - Loss: 4.714024543762207\n",
      "Batch 9 - Input shape: torch.Size([32, 1, 31, 128, 128])\n",
      "Batch 9 - Logits shape: torch.Size([32, 31, 238])\n",
      "Batch 9 - Loss: 4.71729850769043\n",
      "Batch 10 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 10 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 10 - Loss: 4.521176815032959\n",
      "Batch 11 - Input shape: torch.Size([32, 1, 37, 128, 128])\n",
      "Batch 11 - Logits shape: torch.Size([32, 37, 238])\n",
      "Batch 11 - Loss: 4.756474494934082\n"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master(pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
