{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch, os, cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from lipreading.model import Lipreading\n",
    "from lipreading.optim_utils import CosineScheduler\n",
    "\n",
    "# Add the greedy CTC decoder functions\n",
    "def greedy_ctc_decoder(logits, blank_index=0):\n",
    "    \"\"\"\n",
    "    Greedy decoding for CTC.\n",
    "    Assumes logits shape is (T, C) (log probabilities).\n",
    "    Returns a list of predicted indices (for one sample).\n",
    "    \"\"\"\n",
    "    # Convert to numpy if it's a tensor\n",
    "    if isinstance(logits, torch.Tensor):\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    # Get the highest probability index at each timestep\n",
    "    indices = np.argmax(logits, axis=1)  # (T,)\n",
    "    \n",
    "    # Remove duplicates and blanks\n",
    "    filtered_indices = []\n",
    "    prev_idx = -1\n",
    "    for idx in indices:\n",
    "        if idx != blank_index and idx != prev_idx:  # Skip blanks and duplicates\n",
    "            filtered_indices.append(idx)\n",
    "        prev_idx = idx\n",
    "    \n",
    "    return filtered_indices\n",
    "\n",
    "def indices_to_text(indices, idx2char):\n",
    "    \"\"\"\n",
    "    Converts a list of indices to text using the reverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    return ''.join([idx2char.get(i, '') for i in indices])\n",
    "\n",
    "def normalize_arabic_text(text):\n",
    "    \"\"\"\n",
    "    Normalizes Arabic text by combining base characters with their diacritics.\n",
    "    Returns a list of complete characters (base + diacritics).\n",
    "    \"\"\"\n",
    "    chars = []\n",
    "    current_char = ''\n",
    "    \n",
    "    diacritics = {\n",
    "        '\\u064B', '\\u064C', '\\u064D', '\\u064E', '\\u064F',\n",
    "        '\\u0650', '\\u0651', '\\u0652', '\\u0670', '\\u06E2',\n",
    "        '\\u0640'  # tatweel\n",
    "    }\n",
    "    \n",
    "    for c in text:\n",
    "        if c in diacritics:\n",
    "            current_char += c\n",
    "        else:\n",
    "            if current_char:\n",
    "                chars.append(current_char)\n",
    "            current_char = c\n",
    "    \n",
    "    # Don't forget the last character\n",
    "    if current_char:\n",
    "        chars.append(current_char)\n",
    "    \n",
    "    return chars\n",
    "\n",
    "def compute_cer(reference_indices, hypothesis_indices):\n",
    "    \"\"\"\n",
    "    Computes Character Error Rate (CER) directly using token indices.\n",
    "    Takes raw token indices from our vocabulary (class_mapping.txt) rather than Unicode text.\n",
    "    \n",
    "    Returns a tuple of (CER, reference_len, hypothesis_len, edit_distance)\n",
    "    \"\"\"\n",
    "    # Use the indices directly - each index is one token in our vocabulary\n",
    "    ref_tokens = reference_indices\n",
    "    hyp_tokens = hypothesis_indices\n",
    "    \n",
    "    print(f\"Debug - Reference tokens ({len(ref_tokens)} tokens): {ref_tokens}\")\n",
    "    print(f\"Debug - Hypothesis tokens ({len(hyp_tokens)} tokens): {hyp_tokens}\")\n",
    "    \n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    \n",
    "    # Initialize the distance matrix\n",
    "    dp = [[0 for _ in range(n+1)] for _ in range(m+1)]\n",
    "    \n",
    "    # Base cases: empty hypothesis or reference\n",
    "    for i in range(m+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n+1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Fill the distance matrix\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            # If tokens match, no operation needed\n",
    "            if ref_tokens[i-1] == hyp_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                # Minimum of:\n",
    "                # 1. Substitution: dp[i-1][j-1] + 1\n",
    "                # 2. Insertion: dp[i][j-1] + 1\n",
    "                # 3. Deletion: dp[i-1][j] + 1\n",
    "                dp[i][j] = min(dp[i-1][j-1] + 1,  # substitution\n",
    "                              dp[i][j-1] + 1,      # insertion\n",
    "                              dp[i-1][j] + 1)      # deletion\n",
    "    \n",
    "    edit_distance = dp[m][n]\n",
    "    cer = edit_distance / max(m, 1)  # Avoid division by zero\n",
    "    \n",
    "    return cer, m, n, edit_distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2. Initialize the seed and the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Setting the seed for reproducibility\n",
    "seed = 0\n",
    "def reset_seed():\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Setting the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.1. List of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Arabic-Lib-Reading/Dataset/Csv (with Diacritics)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m label\n\u001b[0;32m     26\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArabic-Lib-Reading/Dataset/Csv (with Diacritics)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     28\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArabic-Lib-Reading/Dataset/Csv (with Diacritics)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m i\n\u001b[0;32m     29\u001b[0m     label \u001b[38;5;241m=\u001b[39m extract_label(file)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Arabic-Lib-Reading/Dataset/Csv (with Diacritics)'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def extract_label(file):\n",
    "    label = []\n",
    "    diacritics = {\n",
    "        '\\u064B',  # Fathatan\n",
    "        '\\u064C',  # Dammatan\n",
    "        '\\u064D',  # Kasratan\n",
    "        '\\u064E',  # Fatha\n",
    "        '\\u064F',  # Damma\n",
    "        '\\u0650',  # Kasra\n",
    "        '\\u0651',  # Shadda\n",
    "        '\\u0652',  # Sukun\n",
    "        '\\u06E2',  # Small High meem\n",
    "    }\n",
    "\n",
    "    sentence = pd.read_csv(file)\n",
    "    for word in sentence.word:\n",
    "        for char in word:\n",
    "            if char not in diacritics:\n",
    "                label.append(char)\n",
    "            else:\n",
    "                label[-1] += char\n",
    "\n",
    "    return label\n",
    "\n",
    "classes = set()\n",
    "for i in os.listdir('Arabic-Lib-Reading/Dataset/Csv (with Diacritics)'):\n",
    "    file = 'Arabic-Lib-Reading/Dataset/Csv (with Diacritics)' + i\n",
    "    label = extract_label(file)\n",
    "    classes.update(label)\n",
    "\n",
    "# Create mapping while safely handling Arabic characters\n",
    "mapped_classes = {}\n",
    "for i, c in enumerate(sorted(classes), 1):  \n",
    "    mapped_classes[c] = i\n",
    "\n",
    "# Print in a way that handles encoding properly\n",
    "with open('class_mapping.txt', 'w', encoding='utf-8') as f:\n",
    "    for char, idx in mapped_classes.items():\n",
    "        f.write(f\"{char}: {idx}\\n\")\n",
    "    \n",
    "# Just print count rather than the actual characters to avoid console encoding issues\n",
    "print(f\"Total characters in vocabulary: {len(mapped_classes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.2. Video Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Defining the video dataset class\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_paths, label_paths, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        frames = self.load_frames(video_path=video_path)\n",
    "        label = list(map(lambda x: mapped_classes[x], extract_label(label_path)))\n",
    "        \n",
    "        # Get the number of frames for sequence length\n",
    "        input_length = torch.tensor(len(frames), dtype=torch.long)\n",
    "        label_length = torch.tensor(len(label), dtype=torch.long)\n",
    "        \n",
    "        # Stack frames into a tensor of shape [C, T, H, W]\n",
    "        if len(frames) > 0:\n",
    "            # Stack the list of tensors into a single tensor\n",
    "            stacked_frames = torch.stack(frames)  # Shape: [T, C, H, W]\n",
    "            stacked_frames = stacked_frames.permute(1, 0, 2, 3)  # Shape: [C, T, H, W]\n",
    "        else:\n",
    "            # Handle empty frame list (shouldn't happen but just in case)\n",
    "            stacked_frames = torch.zeros((1, 1, 112, 112))  # Single channel\n",
    "        \n",
    "        return stacked_frames, input_length, torch.tensor(label, dtype=torch.long), label_length\n",
    "    \n",
    "    def load_frames(self, video_path):\n",
    "        frames = []\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        for i in range(total_frames):\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = video.read()\n",
    "            if ret:\n",
    "                # Convert to grayscale as the Lipreading model expects single-channel input\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                # Create PIL Image\n",
    "                frame_pil = Image.fromarray(frame, 'L')  # 'L' is for grayscale\n",
    "                frames.append(frame_pil)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            frames = [self.transform(frame) for frame in frames] \n",
    "        \n",
    "        return frames  # Return a list of frame tensors\n",
    "\n",
    "# Defining the video transform\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.421], std=[0.165])  # For grayscale, single channel\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.2. Load & Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Limit to 30 samples total for testing\n",
    "videos_dir = \"Dataset/Video\"\n",
    "labels_dir = \"Dataset/Csv (with Diacritics)\"\n",
    "videos, labels = [], []\n",
    "file_names = [file_name[:-4] for file_name in os.listdir(videos_dir)]\n",
    "for file_name in file_names:\n",
    "    videos.append(os.path.join(videos_dir, file_name + \".mp4\"))\n",
    "    labels.append(os.path.join(labels_dir, file_name + \".csv\"))\n",
    "\n",
    "if len(videos) > 30:\n",
    "    videos = videos[:30]\n",
    "    labels = labels[:30]\n",
    "    \n",
    "# Split the dataset into training, validation, test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(videos, labels, test_size=0.2, random_state=seed)  # 20% test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=seed)  # 25% of remaining for validation\n",
    "\n",
    "print(f\"Dataset sizes: Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.3. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads data and labels with different lengths in the same batch\n",
    "    \"\"\"\n",
    "    # Unpack the batch - each item is (frames, input_length, label, label_length)\n",
    "    frames_list, input_lengths, labels_list, label_lengths = zip(*batch)\n",
    "    \n",
    "    # Get the max sequence length in this batch\n",
    "    max_len = max(seq_len.item() for seq_len in input_lengths)\n",
    "    \n",
    "    # Get dimensions from the first item\n",
    "    c, t, h, w = frames_list[0].shape  # c, t, h, w = channels, frames, height, width\n",
    "    batch_size = len(frames_list)\n",
    "    \n",
    "    # Create a padded tensor for all sequences\n",
    "    padded_frames = torch.zeros((batch_size, c, max_len, h, w))\n",
    "    \n",
    "    # Copy each sequence to the padded tensor\n",
    "    for i, frames in enumerate(frames_list):\n",
    "        seq_len = input_lengths[i].item()\n",
    "        padded_frames[i, :, :seq_len, :, :] = frames[:, :seq_len, :, :]\n",
    "    \n",
    "    # Flatten labels for CTC loss\n",
    "    labels_flat = []\n",
    "    for label in labels_list:\n",
    "        labels_flat.extend(label)\n",
    "    labels_flat = torch.LongTensor(labels_flat)\n",
    "    \n",
    "    # Convert lengths to tensor\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    label_lengths = torch.LongTensor(label_lengths)\n",
    "    \n",
    "    return padded_frames, input_lengths, labels_flat, label_lengths\n",
    "\n",
    "\n",
    "# Defining the video dataloaders (train, validation, test)\n",
    "train_dataset = VideoDataset(X_train, y_train, transform=transforms)\n",
    "val_dataset = VideoDataset(X_val, y_val, transform=transforms)\n",
    "test_dataset = VideoDataset(X_test, y_test, transform=transforms)\n",
    "batch_size = 2  # Changed from 4 to 2 for testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define a custom version of Lipreading that fixes the dimension issue\n",
    "class FixedLipreading(Lipreading):\n",
    "    def forward(self, x, lengths):\n",
    "        B, C, T, H, W = x.size()\n",
    "        # Process through frontend\n",
    "        x = self.frontend(x)  # Shape: [B, frontend_nout=64, T, H/4, W/4]\n",
    "        \n",
    "        # Get new time dimension after frontend\n",
    "        Tnew = x.shape[2]\n",
    "        frontend_channels = x.shape[1]  # Should be 64\n",
    "        \n",
    "        # Reshape and permute for ResNet processing\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()  # Shape: [B, T, frontend_nout, H/4, W/4]\n",
    "        x = x.view(-1, frontend_channels, x.size(3), x.size(4))  # Shape: [B*T, frontend_nout, H/4, W/4]\n",
    "        \n",
    "        # Process through ResNet trunk\n",
    "        x = self.trunk(x)  # Shape: [B*T, backend_out=512]\n",
    "        \n",
    "        # Reshape back to sequence form\n",
    "        x = x.view(B, Tnew, -1)  # Shape: [B, T, backend_out]\n",
    "        \n",
    "        # Return features or process through TCN\n",
    "        if not self.extract_feats:\n",
    "            # DenseTCN expects input of shape (B, T, C)\n",
    "            # The transpose is handled inside the DenseTCN forward method\n",
    "            return self.tcn(x, lengths, B)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def initialize_model():\n",
    "    # Define DenseTCN options tailored for character-level sequence recognition\n",
    "    densetcn_options = {\n",
    "        'block_config': [3, 3, 3],  # Configuration for DenseTCN blocks\n",
    "        'growth_rate_set': [9, 9, 9],  # Ensure it's divisible by number of kernels (3)\n",
    "        'reduced_size': 120,  # Ensure it's divisible by number of kernels (3)\n",
    "        'kernel_size_set': [3, 5, 7],  # Multiple kernel sizes for different features\n",
    "        'dilation_size_set': [1, 2, 4],  # Increasing dilation for longer dependencies\n",
    "        'dropout': 0.2,  # Regularization\n",
    "        'squeeze_excitation': True,  # Use SE for feature refinement\n",
    "    }\n",
    "    \n",
    "    # Create the model with the appropriate number of classes (characters + blank)\n",
    "    model = FixedLipreading(\n",
    "        modality='video',\n",
    "        hidden_dim=512,  # Match ResNet output size\n",
    "        backbone_type='resnet',\n",
    "        num_classes=len(mapped_classes) + 1,  # Number of characters in vocabulary + blank\n",
    "        relu_type='prelu',\n",
    "        densetcn_options=densetcn_options,\n",
    "        extract_feats=False,  # We want predictions, not features\n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "# Build reverse mapping for decoding\n",
    "idx2char = {v: k for k, v in mapped_classes.items()}\n",
    "idx2char[0] = \"\"  # Blank token for CTC\n",
    "\n",
    "# Defining the loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Training the model\n",
    "def train_model():\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    \n",
    "    # Process all batches (limited since we restricted the dataset size)\n",
    "    \n",
    "    for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(train_loader):\n",
    "        # Print input shape for debugging\n",
    "        print(f\"Batch {batch_idx+1} - Input shape: {inputs.shape}\")\n",
    "        \n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        labels_flat = labels_flat.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass - get sequence logits\n",
    "        logits = model(inputs, input_lengths)\n",
    "        \n",
    "        # Print shape to verify sequence output\n",
    "        print(f\"Batch {batch_idx+1} - Logits shape: {logits.shape}\")\n",
    "        \n",
    "        # Apply log_softmax for CTC\n",
    "        log_probs = F.log_softmax(logits, dim=2)  # (B, T, C)\n",
    "        \n",
    "        # Prepare for CTC loss - requires (T, B, C) format\n",
    "        outputs_for_ctc = log_probs.transpose(0, 1)  # from (B, T, C) to (T, B, C)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        loss = ctc_loss(outputs_for_ctc, labels_flat, input_lengths, label_lengths)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Show detailed diagnostic for first sample in batch\n",
    "        with torch.no_grad():  # Don't track gradients for decoding\n",
    "            sample_idx = 0\n",
    "            sample_logits = log_probs[sample_idx]  # Shape: [T, C]\n",
    "            seq_len = input_lengths[sample_idx].item()\n",
    "            \n",
    "            # Only use valid timesteps (up to seq_len)\n",
    "            valid_logits = sample_logits[:seq_len]\n",
    "            \n",
    "            # Decode using greedy CTC\n",
    "            pred_indices = greedy_ctc_decoder(valid_logits, blank_index=0)\n",
    "            pred_text = indices_to_text(pred_indices, idx2char)\n",
    "            \n",
    "            # Get target text\n",
    "            start_idx = 0  # Start of first sample in batch\n",
    "            end_idx = label_lengths[sample_idx].item()\n",
    "            target_indices = labels_flat[start_idx:end_idx].cpu().tolist()\n",
    "            target_text = indices_to_text(target_indices, idx2char)\n",
    "            \n",
    "            print(f\"Training Sample - Batch {batch_idx+1}:\")\n",
    "            print(f\"  Pred indices: {pred_indices}\")\n",
    "            print(f\"  Target indices: {target_indices}\")\n",
    "            print(f\"  Pred text: {pred_text}\")\n",
    "            print(f\"  Target text: {target_text}\")\n",
    "            print(f\"  CTC Loss: {loss.item():.4f}\")\n",
    "            print(f\"  Sequence length: {seq_len}, Logits shape: {valid_logits.shape}\")\n",
    "        \n",
    "    return running_loss / (batch_idx + 1)\n",
    "\n",
    "# Define a separate testing function that uses our implementation\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Tests the model on the test set. \n",
    "    Uses our token-based CER calculation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n",
    "    \n",
    "    # Open the predictions file for writing with proper encoding for Arabic\n",
    "    with open(\"predictions.txt\", \"w\", encoding=\"utf-8-sig\") as f:\n",
    "        # Write header with Arabic support\n",
    "        f.write(\"=== New Evaluation Run ===\\n\\n\")\n",
    "        f.write(\"Format: UTF-8 with Arabic support\\n\")\n",
    "        f.write(\"Note: Lengths shown are token counts from class_mapping.txt\\n\\n\")\n",
    "        \n",
    "        # Track statistics\n",
    "        total_cer = 0\n",
    "        total_loss = 0\n",
    "        total_edit_distance = 0\n",
    "        \n",
    "        pred_lengths = []\n",
    "        target_lengths = []\n",
    "        \n",
    "        # Process all batches in the test loader\n",
    "        with torch.no_grad():\n",
    "            for i, (frames, input_lengths, labels_flat, label_lengths) in enumerate(test_loader):\n",
    "                # Move to device\n",
    "                frames = frames.to(device)\n",
    "                input_lengths = input_lengths.to(device)\n",
    "                labels_flat = labels_flat.to(device)\n",
    "                label_lengths = label_lengths.to(device)\n",
    "                \n",
    "                # Forward pass through the entire model\n",
    "                batch_size = frames.size(0)\n",
    "                logits = model(frames, input_lengths)\n",
    "                \n",
    "                # Apply log_softmax for CTC\n",
    "                log_probs = F.log_softmax(logits, dim=2)  # (B, T, C)\n",
    "                \n",
    "                # For CTC loss we need (T, N, C) format\n",
    "                log_probs_ctc = log_probs.transpose(0, 1)  # (T, B, C)\n",
    "                \n",
    "                # Make sure output_lengths are not greater than the input lengths\n",
    "                output_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=device)\n",
    "                if output_lengths.max() > input_lengths.min():\n",
    "                    scale_factor = input_lengths.min().float() / output_lengths.max().float()\n",
    "                    output_lengths = (output_lengths.float() * scale_factor).long()\n",
    "                \n",
    "                # Calculate CTC loss\n",
    "                loss = ctc_loss(log_probs_ctc, labels_flat, output_lengths, label_lengths)\n",
    "                \n",
    "                # Decode predictions - we convert back to numpy for greedy decoding\n",
    "                logits_np = log_probs.cpu().detach().numpy()  # (B, T, C)\n",
    "                \n",
    "                # Process each batch item\n",
    "                for b in range(batch_size):\n",
    "                    # Get batch item logits\n",
    "                    batch_logits = logits_np[b]  # (T, C)\n",
    "                    \n",
    "                    # Decode using CTC\n",
    "                    pred_indices = greedy_ctc_decoder(batch_logits)\n",
    "                    \n",
    "                    # Get target indices\n",
    "                    start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                    end_idx = start_idx + label_lengths[b].item()\n",
    "                    target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                    \n",
    "                    # Convert indices to text\n",
    "                    pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                    target_text = indices_to_text(target_idx, idx2char)\n",
    "                    \n",
    "                    # Compute CER directly using token indices\n",
    "                    cer, ref_len, hyp_len, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    total_cer += cer\n",
    "                    total_loss += loss.item()\n",
    "                    total_edit_distance += edit_distance\n",
    "                    \n",
    "                    pred_lengths.append(hyp_len)\n",
    "                    target_lengths.append(ref_len)\n",
    "                    \n",
    "                    # Print info\n",
    "                    print(\"-\" * 50)\n",
    "                    print(f\"Sample {i * batch_size + b + 1}:\")\n",
    "                    print(f\"Predicted indices: {pred_indices}\")\n",
    "                    print(f\"Target indices: {list(target_idx)}\")\n",
    "                    print(f\"Predicted text: {pred_text}\")\n",
    "                    print(f\"Target text: {target_text}\")\n",
    "                    print(f\"Lengths: pred={hyp_len} tokens, target={ref_len} tokens\")\n",
    "                    print(f\"Edit Distance: {edit_distance}\")\n",
    "                    print(f\"CER: {cer:.4f}\")\n",
    "                    print(f\"CTC Loss: {loss.item():.4f}\")\n",
    "                    print(\"-\" * 50)\n",
    "                    \n",
    "                    # Write to file with detailed information\n",
    "                    f.write(f\"Sample {i * batch_size + b + 1}:\\n\")\n",
    "                    f.write(\"Prediction:\\n\")\n",
    "                    f.write(f\"  Text ({hyp_len} tokens): {pred_text}\\n\")\n",
    "                    f.write(f\"  Indices: {pred_indices}\\n\")\n",
    "                    f.write(\"Target:\\n\")\n",
    "                    f.write(f\"  Text ({ref_len} tokens): {target_text}\\n\")\n",
    "                    f.write(f\"  Indices: {list(target_idx)}\\n\")\n",
    "                    f.write(\"Metrics:\\n\")\n",
    "                    f.write(f\"  Edit Distance: {edit_distance}\\n\")\n",
    "                    f.write(f\"  CER: {cer:.4f}\\n\")\n",
    "                    f.write(f\"  CTC Loss: {loss.item():.4f}\\n\")\n",
    "                    f.write(\"--------------------------------------------------\\n\\n\")\n",
    "            \n",
    "            # Write summary statistics\n",
    "            n_samples = len(test_loader.dataset)\n",
    "            avg_cer = total_cer / n_samples\n",
    "            avg_loss = total_loss / n_samples\n",
    "            avg_edit_distance = total_edit_distance / n_samples\n",
    "            \n",
    "            f.write(\"=== Summary Statistics ===\\n\")\n",
    "            f.write(f\"Total samples: {n_samples}\\n\")\n",
    "            f.write(f\"Average CER: {avg_cer:.4f}\\n\")\n",
    "            f.write(f\"Average Edit Distance: {avg_edit_distance:.2f}\\n\")\n",
    "            f.write(f\"Average Loss: {avg_loss:.4f}\\n\\n\")\n",
    "            \n",
    "            # Length statistics\n",
    "            avg_pred_len = sum(pred_lengths) / len(pred_lengths)\n",
    "            min_pred_len = min(pred_lengths)\n",
    "            max_pred_len = max(pred_lengths)\n",
    "            \n",
    "            avg_target_len = sum(target_lengths) / len(target_lengths)\n",
    "            min_target_len = min(target_lengths)\n",
    "            max_target_len = max(target_lengths)\n",
    "            \n",
    "            f.write(\"Length Statistics:\\n\")\n",
    "            f.write(f\"Predictions:\\n\")\n",
    "            f.write(f\"  Average: {avg_pred_len:.1f} tokens\\n\")\n",
    "            f.write(f\"  Range: {min_pred_len} to {max_pred_len} tokens\\n\")\n",
    "            f.write(f\"Targets:\\n\")\n",
    "            f.write(f\"  Average: {avg_target_len:.1f} tokens\\n\")\n",
    "            f.write(f\"  Range: {min_target_len} to {max_target_len} tokens\\n\")\n",
    "        \n",
    "    return avg_cer, avg_loss\n",
    "\n",
    "# Fix the main function to call our test_model function\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the model\n",
    "    model = initialize_model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Test with the updated CER calculation\n",
    "    avg_cer, avg_loss = test_model()\n",
    "    print(f\"Test CER: {avg_cer:.4f}, Test Loss: {avg_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
