{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lipreading'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlipreading\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lipreading\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlipreading\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CosineScheduler\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lipreading'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch, os, cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from lipreading.model import Lipreading\n",
    "from lipreading.optim_utils import CosineScheduler\n",
    "import io\n",
    "import sys\n",
    "import dlib\n",
    "from collections import deque\n",
    "from skimage import transform as tf\n",
    "\n",
    "\n",
    "# Constants for face and mouth ROI extraction\n",
    "STD_SIZE = (256, 256)\n",
    "STABLE_POINTS_IDS = [33, 36, 39, 42, 45]  # Stable facial landmarks for alignment\n",
    "MOUTH_POINTS_START_IDX = 48  # Start index for mouth landmarks\n",
    "MOUTH_POINTS_END_IDX = 68    # End index for mouth landmarks\n",
    "CROP_WIDTH = 96   # Width of the mouth ROI\n",
    "CROP_HEIGHT = 96  # Height of the mouth ROI\n",
    "\n",
    "# Path to the mean face landmarks file from lrwar\n",
    "MEAN_FACE_LANDMARKS_PATH = \"mean_face_landmarks.txt\"\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "# Function to extract landmarks from a single frame\n",
    "def extract_landmarks_from_frame(image, detector, predictor):\n",
    "    \"\"\"Extract facial landmarks from a single frame\"\"\"\n",
    "    # Convert the image color to grayscale if it's not already\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "        \n",
    "    # Detect the face\n",
    "    rects = detector(gray, 1)\n",
    "    if not rects:\n",
    "        return None\n",
    "    \n",
    "    # Get the face with maximum area\n",
    "    ref_rect = None\n",
    "    max_area = 0\n",
    "    for rect in rects:\n",
    "        startX = rect.left()\n",
    "        startY = rect.top()\n",
    "        endX = rect.right()\n",
    "        endY = rect.bottom()\n",
    "        startX = max(0, startX)\n",
    "        startY = max(0, startY)\n",
    "        endX = min(endX, image.shape[1])\n",
    "        endY = min(endY, image.shape[0])\n",
    "        w = endX - startX\n",
    "        h = endY - startY\n",
    "        area = w * h\n",
    "        if area > max_area:\n",
    "            ref_rect = rect\n",
    "            max_area = area\n",
    "    \n",
    "    if ref_rect is None:\n",
    "        return None\n",
    "    \n",
    "    # Get the landmark points\n",
    "    shape = predictor(gray, ref_rect)\n",
    "    # Convert it to the NumPy Array\n",
    "    shape_np = np.zeros((68, 2), dtype=\"int\")\n",
    "    for i in range(0, 68):\n",
    "        shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    \n",
    "    return shape_np\n",
    "\n",
    "# Function to linearly interpolate landmarks between two frames\n",
    "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
    "    \"\"\"Linearly interpolate missing landmarks between start_idx and stop_idx\"\"\"\n",
    "    start_landmarks = landmarks[start_idx]\n",
    "    stop_landmarks = landmarks[stop_idx]\n",
    "    delta = stop_landmarks - start_landmarks\n",
    "    for idx in range(1, stop_idx - start_idx):\n",
    "        landmarks[start_idx + idx] = start_landmarks + idx / float(stop_idx - start_idx) * delta\n",
    "    return landmarks\n",
    "\n",
    "# Function to interpolate landmarks for all frames\n",
    "def landmarks_interpolate(landmarks):\n",
    "    \"\"\"Interpolate landmarks for frames where detection failed\"\"\"\n",
    "    valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
    "    if not valid_frames_idx:\n",
    "        return None\n",
    "    \n",
    "    # Interpolate between valid frames\n",
    "    for idx in range(1, len(valid_frames_idx)):\n",
    "        if valid_frames_idx[idx] - valid_frames_idx[idx - 1] == 1:\n",
    "            continue\n",
    "        else:\n",
    "            landmarks = linear_interpolate(landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx])\n",
    "    \n",
    "    # Handle frames at the beginning or end where detection failed\n",
    "    valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
    "    if valid_frames_idx:\n",
    "        landmarks[:valid_frames_idx[0]] = [landmarks[valid_frames_idx[0]]] * valid_frames_idx[0]\n",
    "        landmarks[valid_frames_idx[-1]:] = [landmarks[valid_frames_idx[-1]]] * (len(landmarks) - valid_frames_idx[-1])\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "# Function to warp an image using similarity transform\n",
    "def warp_img(src, dst, img, std_size):\n",
    "    \"\"\"Warp an image using similarity transform to align facial landmarks\"\"\"\n",
    "    tform = tf.estimate_transform('similarity', src, dst)  # find the transformation matrix\n",
    "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)  # wrap the frame image\n",
    "    warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "    warped = warped.astype('uint8')\n",
    "    return warped, tform\n",
    "\n",
    "# Function to cut a patch around the mouth region\n",
    "def cut_patch(img, landmarks, height, width, threshold=5):\n",
    "    \"\"\"Cut a patch around the mouth region based on landmarks\"\"\"\n",
    "    center_x, center_y = np.mean(landmarks, axis=0)\n",
    "\n",
    "    if center_y - height < 0:\n",
    "        center_y = height\n",
    "    if center_y - height < 0 - threshold:\n",
    "        raise Exception('too much bias in height')\n",
    "    if center_x - width < 0:\n",
    "        center_x = width\n",
    "    if center_x - width < 0 - threshold:\n",
    "        raise Exception('too much bias in width')\n",
    "\n",
    "    if center_y + height > img.shape[0]:\n",
    "        center_y = img.shape[0] - height\n",
    "    if center_y + height > img.shape[0] + threshold:\n",
    "        raise Exception('too much bias in height')\n",
    "    if center_x + width > img.shape[1]:\n",
    "        center_x = img.shape[1] - width\n",
    "    if center_x + width > img.shape[1] + threshold:\n",
    "        raise Exception('too much bias in width')\n",
    "\n",
    "    cutted_img = np.copy(img[int(round(center_y) - round(height)): int(round(center_y) + round(height)),\n",
    "                         int(round(center_x) - round(width)): int(round(center_x) + round(width))])\n",
    "    return cutted_img\n",
    "\n",
    "# Add the greedy CTC decoder functions\n",
    "def greedy_ctc_decoder(logits, blank_index=0):\n",
    "    \"\"\"\n",
    "    Greedy decoding for CTC.\n",
    "    Assumes logits shape is (T, C) (log probabilities).\n",
    "    Returns a list of predicted indices (for one sample).\n",
    "    \"\"\"\n",
    "    # Convert to numpy if it's a tensor\n",
    "    if isinstance(logits, torch.Tensor):\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    # Get the highest probability index at each timestep\n",
    "    indices = np.argmax(logits, axis=1)  # (T,)\n",
    "    \n",
    "    # Remove duplicates and blanks\n",
    "    filtered_indices = []\n",
    "    prev_idx = -1\n",
    "    for idx in indices:\n",
    "        if idx != blank_index and idx != prev_idx:  # Skip blanks and duplicates\n",
    "            filtered_indices.append(idx)\n",
    "        prev_idx = idx\n",
    "    \n",
    "    return filtered_indices\n",
    "\n",
    "def indices_to_text(indices, idx2char):\n",
    "    \"\"\"\n",
    "    Converts a list of indices to text using the reverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    return ''.join([idx2char.get(i, '') for i in indices])\n",
    "\n",
    "def normalize_arabic_text(text):\n",
    "    \"\"\"\n",
    "    Normalizes Arabic text by combining base characters with their diacritics.\n",
    "    Returns a list of complete characters (base + diacritics).\n",
    "    \"\"\"\n",
    "    chars = []\n",
    "    current_char = ''\n",
    "    \n",
    "    diacritics = {\n",
    "        '\\u064B', '\\u064C', '\\u064D', '\\u064E', '\\u064F',\n",
    "        '\\u0650', '\\u0651', '\\u0652', '\\u0670', '\\u06E2',\n",
    "        '\\u0640'  # tatweel\n",
    "    }\n",
    "    \n",
    "    for c in text:\n",
    "        if c in diacritics:\n",
    "            current_char += c\n",
    "        else:\n",
    "            if current_char:\n",
    "                chars.append(current_char)\n",
    "            current_char = c\n",
    "    \n",
    "    # Don't forget the last character\n",
    "    if current_char:\n",
    "        chars.append(current_char)\n",
    "    \n",
    "    return chars\n",
    "\n",
    "def compute_cer(reference_indices, hypothesis_indices):\n",
    "    \"\"\"\n",
    "    Computes Character Error Rate (CER) directly using token indices.\n",
    "    Takes raw token indices from our vocabulary (class_mapping.txt) rather than Unicode text.\n",
    "    \n",
    "    Returns a tuple of (CER, reference_len, hypothesis_len, edit_distance)\n",
    "    \"\"\"\n",
    "    # Use the indices directly - each index is one token in our vocabulary\n",
    "    ref_tokens = reference_indices\n",
    "    hyp_tokens = hypothesis_indices\n",
    "    \n",
    "    print(f\"Debug - Reference tokens ({len(ref_tokens)} tokens): {ref_tokens}\")\n",
    "    print(f\"Debug - Hypothesis tokens ({len(hyp_tokens)} tokens): {hyp_tokens}\")\n",
    "    \n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    \n",
    "    # Initialize the distance matrix\n",
    "    dp = [[0 for _ in range(n+1)] for _ in range(m+1)]\n",
    "    \n",
    "    # Base cases: empty hypothesis or reference\n",
    "    for i in range(m+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n+1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Fill the distance matrix\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            # If tokens match, no operation needed\n",
    "            if ref_tokens[i-1] == hyp_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                # Minimum of:\n",
    "                # 1. Substitution: dp[i-1][j-1] + 1\n",
    "                # 2. Insertion: dp[i][j-1] + 1\n",
    "                # 3. Deletion: dp[i-1][j] + 1\n",
    "                dp[i][j] = min(dp[i-1][j-1] + 1,  # substitution\n",
    "                              dp[i][j-1] + 1,      # insertion\n",
    "                              dp[i-1][j] + 1)      # deletion\n",
    "    \n",
    "    edit_distance = dp[m][n]\n",
    "    cer = edit_distance / max(m, 1)  # Avoid division by zero\n",
    "    \n",
    "    return cer, m, n, edit_distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2. Initialize the seed and the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Setting the seed for reproducibility\n",
    "seed = 0\n",
    "def reset_seed():\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Setting the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.1. List of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def extract_label(file):\n",
    "    label = []\n",
    "    diacritics = {\n",
    "        '\\u064B',  # Fathatan\n",
    "        '\\u064C',  # Dammatan\n",
    "        '\\u064D',  # Kasratan\n",
    "        '\\u064E',  # Fatha\n",
    "        '\\u064F',  # Damma\n",
    "        '\\u0650',  # Kasra\n",
    "        '\\u0651',  # Shadda\n",
    "        '\\u0652',  # Sukun\n",
    "        '\\u06E2',  # Small High meem\n",
    "    }\n",
    "\n",
    "    sentence = pd.read_csv(file)\n",
    "    for word in sentence.word:\n",
    "        for char in word:\n",
    "            if char not in diacritics:\n",
    "                label.append(char)\n",
    "            else:\n",
    "                label[-1] += char\n",
    "\n",
    "    return label\n",
    "\n",
    "classes = set()\n",
    "for i in os.listdir('Dataset/Csv (with Diacritics)'):\n",
    "    file = 'Dataset/Csv (with Diacritics)/' + i\n",
    "    label = extract_label(file)\n",
    "    classes.update(label)\n",
    "\n",
    "# Create mapping while safely handling Arabic characters\n",
    "mapped_classes = {}\n",
    "for i, c in enumerate(sorted(classes), 1):  \n",
    "    mapped_classes[c] = i\n",
    "\n",
    "# Print in a way that handles encoding properly\n",
    "with open('class_mapping.txt', 'w', encoding='utf-8') as f:\n",
    "    for char, idx in mapped_classes.items():\n",
    "        f.write(f\"{char}: {idx}\\n\")\n",
    "    \n",
    "# Just print count rather than the actual characters to avoid console encoding issues\n",
    "print(f\"Total characters in vocabulary: {len(mapped_classes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.2. Video Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Defining the video dataset class\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_paths, label_paths, transform=None, use_roi_cropping=True):\n",
    "        self.video_paths = video_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "        self.use_roi_cropping = use_roi_cropping\n",
    "        \n",
    "        # Initialize face detector and landmark predictor if ROI cropping is enabled\n",
    "        if self.use_roi_cropping:\n",
    "            self.detector = dlib.get_frontal_face_detector()\n",
    "            self.predictor = dlib.shape_predictor(predictor_path)\n",
    "            self.mean_face_landmarks = np.loadtxt(MEAN_FACE_LANDMARKS_PATH)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        \n",
    "        if self.use_roi_cropping:\n",
    "            frames = self.load_frames_with_roi(video_path)\n",
    "        else:\n",
    "            frames = self.load_frames(video_path)\n",
    "            \n",
    "        label = list(map(lambda x: mapped_classes[x], extract_label(label_path)))\n",
    "        \n",
    "        # Get the number of frames for sequence length\n",
    "        input_length = torch.tensor(len(frames), dtype=torch.long)\n",
    "        label_length = torch.tensor(len(label), dtype=torch.long)\n",
    "        \n",
    "        # Stack frames into a tensor of shape [C, T, H, W]\n",
    "        if len(frames) > 0:\n",
    "            # Stack the list of tensors into a single tensor\n",
    "            stacked_frames = torch.stack(frames)  # Shape: [T, C, H, W]\n",
    "            stacked_frames = stacked_frames.permute(1, 0, 2, 3)  # Shape: [C, T, H, W]\n",
    "        else:\n",
    "            # Handle empty frame list (shouldn't happen but just in case)\n",
    "            stacked_frames = torch.zeros((1, 1, 112, 112))  # Single channel\n",
    "        \n",
    "        return stacked_frames, input_length, torch.tensor(label, dtype=torch.long), label_length\n",
    "    \n",
    "    def load_frames_with_roi(self, video_path):\n",
    "        \"\"\"Load video frames with ROI cropping around the mouth region using lrwar utilities\"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # First pass: extract landmarks for all frames\n",
    "        video_landmarks = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Extract landmarks using our function for single frame\n",
    "            landmarks = extract_landmarks_from_frame(frame, self.detector, self.predictor)\n",
    "            video_landmarks.append(landmarks)\n",
    "        \n",
    "        # Reset video capture for second pass\n",
    "        cap.release()\n",
    "        \n",
    "        # Interpolate missing landmarks\n",
    "        video_landmarks = landmarks_interpolate(video_landmarks)\n",
    "        if video_landmarks is None or len(video_landmarks) == 0:\n",
    "            print(f\"Warning: No face detected in {video_path}. Falling back to regular frame loading.\")\n",
    "            return self.load_frames(video_path)\n",
    "        \n",
    "        # Second pass: process frames with landmarks\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_idx = 0\n",
    "        \n",
    "        # For smoothing landmarks\n",
    "        smoothing_window = 5\n",
    "        q_landmarks = deque(maxlen=smoothing_window)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Convert to grayscale\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Add current landmarks to queue for smoothing\n",
    "            if frame_idx < len(video_landmarks):\n",
    "                q_landmarks.append(video_landmarks[frame_idx])\n",
    "            \n",
    "            # Process frame if we have enough landmarks for smoothing\n",
    "            if len(q_landmarks) == smoothing_window:\n",
    "                # Use mean of landmarks for stability\n",
    "                smoothed_landmarks = np.mean(list(q_landmarks), axis=0)\n",
    "                \n",
    "                try:\n",
    "                    # Align face using stable points\n",
    "                    trans_frame, _ = warp_img(\n",
    "                        smoothed_landmarks[STABLE_POINTS_IDS, :],\n",
    "                        self.mean_face_landmarks[STABLE_POINTS_IDS, :],\n",
    "                        gray_frame,\n",
    "                        STD_SIZE\n",
    "                    )\n",
    "                    \n",
    "                    # Cut mouth region\n",
    "                    mouth_roi = cut_patch(\n",
    "                        trans_frame,\n",
    "                        smoothed_landmarks[MOUTH_POINTS_START_IDX:MOUTH_POINTS_END_IDX],\n",
    "                        CROP_HEIGHT//2,\n",
    "                        CROP_WIDTH//2\n",
    "                    )\n",
    "                    \n",
    "                    # Resize to target size (112x112)\n",
    "                    mouth_roi = cv2.resize(mouth_roi, (112, 112))\n",
    "                    \n",
    "                    # Convert to PIL Image and apply transforms if any\n",
    "                    mouth_roi_pil = Image.fromarray(mouth_roi)\n",
    "                    \n",
    "                    if self.transform is not None:\n",
    "                        mouth_roi_tensor = self.transform(mouth_roi_pil)\n",
    "                    else:\n",
    "                        mouth_roi_tensor = transforms.ToTensor()(mouth_roi_pil)\n",
    "                        \n",
    "                    frames.append(mouth_roi_tensor)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # If ROI extraction fails, use the full frame\n",
    "                    print(f\"ROI extraction failed: {e}. Using full frame.\")\n",
    "                    frame_pil = Image.fromarray(gray_frame)\n",
    "                    if self.transform is not None:\n",
    "                        frame_tensor = self.transform(frame_pil)\n",
    "                    else:\n",
    "                        frame_tensor = transforms.ToTensor()(frame_pil)\n",
    "                    frames.append(frame_tensor)\n",
    "            \n",
    "            frame_idx += 1\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    def load_frames(self, video_path):\n",
    "        \"\"\"Original method to load frames without ROI cropping\"\"\"\n",
    "        frames = []\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        for i in range(total_frames):\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = video.read()\n",
    "            if ret:\n",
    "                # Convert to grayscale as the Lipreading model expects single-channel input\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                # Create PIL Image\n",
    "                frame_pil = Image.fromarray(frame, 'L')  # 'L' is for grayscale\n",
    "                frames.append(frame_pil)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            frames = [self.transform(frame) for frame in frames] \n",
    "        \n",
    "        return frames  # Return a list of frame tensors\n",
    "\n",
    "# Defining the video transform\n",
    "video_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.421], std=[0.165])  # For grayscale, single channel\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.2. Load & Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Limit to 30 samples total for testing\n",
    "videos_dir = \"Dataset/Video\"\n",
    "labels_dir = \"Dataset/Csv (with Diacritics)\"\n",
    "videos, labels = [], []\n",
    "file_names = [file_name[:-4] for file_name in os.listdir(videos_dir)]\n",
    "for file_name in file_names:\n",
    "    videos.append(os.path.join(videos_dir, file_name + \".mp4\"))\n",
    "    labels.append(os.path.join(labels_dir, file_name + \".csv\"))\n",
    "\n",
    "if len(videos) > 30:  # Increased from 10 to 30\n",
    "    videos = videos[:30]\n",
    "    labels = labels[:30]\n",
    "    \n",
    "# Split the dataset into training, validation, test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(videos, labels, test_size=0.2, random_state=seed)  # 20% test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=seed)  # 25% of remaining for validation\n",
    "\n",
    "print(f\"Dataset sizes: Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.4. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads data and labels with different lengths in the same batch\n",
    "    \"\"\"\n",
    "    # Unpack the batch - each item is (frames, input_length, label, label_length)\n",
    "    frames_list, input_lengths, labels_list, label_lengths = zip(*batch)\n",
    "    \n",
    "    # Get the max sequence length in this batch\n",
    "    max_len = max(seq_len.item() for seq_len in input_lengths)\n",
    "    \n",
    "    # Get dimensions from the first item\n",
    "    c, t, h, w = frames_list[0].shape  # c, t, h, w = channels, frames, height, width\n",
    "    batch_size = len(frames_list)\n",
    "    \n",
    "    # Create a padded tensor for all sequences\n",
    "    padded_frames = torch.zeros((batch_size, c, max_len, h, w))\n",
    "    \n",
    "    # Copy each sequence to the padded tensor\n",
    "    for i, frames in enumerate(frames_list):\n",
    "        seq_len = input_lengths[i].item()\n",
    "        padded_frames[i, :, :seq_len, :, :] = frames[:, :seq_len, :, :]\n",
    "    \n",
    "    # Flatten labels for CTC loss\n",
    "    labels_flat = []\n",
    "    for label in labels_list:\n",
    "        labels_flat.extend(label)\n",
    "    labels_flat = torch.LongTensor(labels_flat)\n",
    "    \n",
    "    # Convert lengths to tensor\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    label_lengths = torch.LongTensor(label_lengths)\n",
    "    \n",
    "    return padded_frames, input_lengths, labels_flat, label_lengths\n",
    "\n",
    "\n",
    "# Defining the video dataloaders (train, validation, test)\n",
    "train_dataset = VideoDataset(X_train, y_train, transform=video_transform)\n",
    "val_dataset = VideoDataset(X_val, y_val, transform=video_transform)\n",
    "test_dataset = VideoDataset(X_test, y_test, transform=video_transform)\n",
    "batch_size = 2  # Changed from 4 to 2 for testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define a custom version of Lipreading that fixes the dimension issue\n",
    "class FixedLipreading(Lipreading):\n",
    "    def forward(self, x, lengths):\n",
    "        B, C, T, H, W = x.size()\n",
    "        # Process through frontend\n",
    "        x = self.frontend(x)  # Shape: [B, frontend_nout=64, T, H/4, W/4]\n",
    "        \n",
    "        # Get new time dimension after frontend\n",
    "        Tnew = x.shape[2]\n",
    "        frontend_channels = x.shape[1]  # Should be 64\n",
    "        \n",
    "        # Reshape and permute for ResNet processing\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()  # Shape: [B, T, frontend_nout, H/4, W/4]\n",
    "        x = x.view(-1, frontend_channels, x.size(3), x.size(4))  # Shape: [B*T, frontend_nout, H/4, W/4]\n",
    "        \n",
    "        # Process through ResNet trunk\n",
    "        x = self.trunk(x)  # Shape: [B*T, backend_out=512]\n",
    "        \n",
    "        # Reshape back to sequence form\n",
    "        x = x.view(B, Tnew, -1)  # Shape: [B, T, backend_out]\n",
    "        \n",
    "        # Return features or process through TCN\n",
    "        if not self.extract_feats:\n",
    "            # DenseTCN expects input of shape (B, T, C)\n",
    "            # The transpose is handled inside the DenseTCN forward method\n",
    "            return self.tcn(x, lengths, B)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def initialize_model():\n",
    "    # Define DenseTCN options tailored for character-level sequence recognition\n",
    "    densetcn_options = {\n",
    "        'block_config': [3, 3, 3],  # Configuration for DenseTCN blocks\n",
    "        'growth_rate_set': [9, 9, 9],  # Ensure it's divisible by number of kernels (3)\n",
    "        'reduced_size': 120,  # Ensure it's divisible by number of kernels (3)\n",
    "        'kernel_size_set': [3, 5, 7],  # Multiple kernel sizes for different features\n",
    "        'dilation_size_set': [1, 2, 4],  # Increasing dilation for longer dependencies\n",
    "        'dropout': 0.2,  # Regularization\n",
    "        'squeeze_excitation': True,  # Use SE for feature refinement\n",
    "    }\n",
    "    \n",
    "    # Create the model with the appropriate number of classes (characters + blank)\n",
    "    model = FixedLipreading(\n",
    "        modality='video',\n",
    "        hidden_dim=512,  # Match ResNet output size\n",
    "        backbone_type='resnet',\n",
    "        num_classes=len(mapped_classes) + 1,  # Number of characters in vocabulary + blank\n",
    "        relu_type='prelu',\n",
    "        densetcn_options=densetcn_options,\n",
    "        extract_feats=False,  # We want predictions, not features\n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "# Build reverse mapping for decoding\n",
    "idx2char = {v: k for k, v in mapped_classes.items()}\n",
    "idx2char[0] = \"\"  # Blank token for CTC\n",
    "\n",
    "# Defining the loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Training the model\n",
    "def train_model():\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    \n",
    "    for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        labels_flat = labels_flat.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs, input_lengths)\n",
    "        log_probs = F.log_softmax(logits, dim=2)\n",
    "        outputs_for_ctc = log_probs.transpose(0, 1)\n",
    "        \n",
    "        loss = ctc_loss(outputs_for_ctc, labels_flat, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Show only essential info for first sample in batch\n",
    "        with torch.no_grad():\n",
    "            sample_idx = 0\n",
    "            sample_logits = log_probs[sample_idx]\n",
    "            seq_len = input_lengths[sample_idx].item()\n",
    "            valid_logits = sample_logits[:seq_len]\n",
    "            pred_indices = greedy_ctc_decoder(valid_logits, blank_index=0)\n",
    "            pred_text = indices_to_text(pred_indices, idx2char)\n",
    "            \n",
    "            start_idx = 0\n",
    "            end_idx = label_lengths[sample_idx].item()\n",
    "            target_indices = labels_flat[start_idx:end_idx].cpu().tolist()\n",
    "            target_text = indices_to_text(target_indices, idx2char)\n",
    "            \n",
    "            print(f\"Batch {batch_idx+1}:\")\n",
    "            print(f\"  Pred text: {pred_text}\")\n",
    "            print(f\"  Target text: {target_text}\")\n",
    "            print(f\"  Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    return running_loss / (batch_idx + 1)\n",
    "\n",
    "# Testing the model\n",
    "def test_model():\n",
    "    model.eval()\n",
    "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n",
    "    \n",
    "    with open(\"predictions_with_roi.txt\", \"w\", encoding=\"utf-8-sig\") as f:\n",
    "        f.write(\"=== New Evaluation Run ===\\n\\n\")\n",
    "        f.write(\"Format: UTF-8 with Arabic support\\n\")\n",
    "        f.write(\"Note: Lengths shown are token counts from class_mapping.txt\\n\\n\")\n",
    "        \n",
    "        total_cer = 0\n",
    "        total_loss = 0\n",
    "        total_edit_distance = 0\n",
    "        pred_lengths = []\n",
    "        target_lengths = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (frames, input_lengths, labels_flat, label_lengths) in enumerate(test_loader):\n",
    "                frames = frames.to(device)\n",
    "                input_lengths = input_lengths.to(device)\n",
    "                labels_flat = labels_flat.to(device)\n",
    "                label_lengths = label_lengths.to(device)\n",
    "                \n",
    "                batch_size = frames.size(0)\n",
    "                logits = model(frames, input_lengths)\n",
    "                log_probs = F.log_softmax(logits, dim=2)\n",
    "                log_probs_ctc = log_probs.transpose(0, 1)\n",
    "                \n",
    "                output_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=device)\n",
    "                if output_lengths.max() > input_lengths.min():\n",
    "                    scale_factor = input_lengths.min().float() / output_lengths.max().float()\n",
    "                    output_lengths = (output_lengths.float() * scale_factor).long()\n",
    "                \n",
    "                loss = ctc_loss(log_probs_ctc, labels_flat, output_lengths, label_lengths)\n",
    "                logits_np = log_probs.cpu().detach().numpy()\n",
    "                \n",
    "                for b in range(batch_size):\n",
    "                    batch_logits = logits_np[b]\n",
    "                    pred_indices = greedy_ctc_decoder(batch_logits)\n",
    "                    \n",
    "                    start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                    end_idx = start_idx + label_lengths[b].item()\n",
    "                    target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                    \n",
    "                    print(\"Debug - Reference tokens ({} tokens): {}\".format(len(target_idx), target_idx))\n",
    "                    print(\"Debug - Hypothesis tokens ({} tokens): {}\".format(len(pred_indices), pred_indices))\n",
    "                    \n",
    "                    pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                    target_text = indices_to_text(target_idx, idx2char)\n",
    "                    \n",
    "                    cer, ref_len, hyp_len, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                    \n",
    "                    total_cer += cer\n",
    "                    total_loss += loss.item()\n",
    "                    total_edit_distance += edit_distance\n",
    "                    pred_lengths.append(hyp_len)\n",
    "                    target_lengths.append(ref_len)\n",
    "                    \n",
    "                    f.write(\"--------------------------------------------------\\n\")\n",
    "                    f.write(f\"Sample {i * batch_size + b + 1}:\\n\")\n",
    "                    f.write(f\"Predicted indices: {pred_indices}\\n\")\n",
    "                    f.write(f\"Target indices: {list(target_idx)}\\n\")\n",
    "                    f.write(f\"Predicted text: {pred_text}\\n\")\n",
    "                    f.write(f\"Target text: {target_text}\\n\")\n",
    "                    f.write(f\"Lengths: pred={hyp_len} tokens, target={ref_len} tokens\\n\")\n",
    "                    f.write(f\"Edit Distance: {edit_distance}\\n\")\n",
    "                    f.write(f\"CER: {cer:.4f}\\n\")\n",
    "                    f.write(f\"CTC Loss: {loss.item():.4f}\\n\")\n",
    "                    f.write(\"--------------------------------------------------\\n\\n\")\n",
    "            \n",
    "            n_samples = len(test_loader.dataset)\n",
    "            avg_cer = total_cer / n_samples\n",
    "            avg_loss = total_loss / n_samples\n",
    "            \n",
    "            f.write(\"=== Summary Statistics ===\\n\")\n",
    "            f.write(f\"Total samples: {n_samples}\\n\")\n",
    "            f.write(f\"Average CER: {avg_cer:.4f}\\n\")\n",
    "            f.write(f\"Average Loss: {avg_loss:.4f}\\n\")\n",
    "        \n",
    "    return avg_cer, avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 6. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    # Defining the video transform\n",
    "    video_transform = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.421], std=[0.165])  # For grayscale, single channel\n",
    "    ])\n",
    "    \n",
    "    # Defining the video dataloaders with ROI cropping\n",
    "    train_dataset = VideoDataset(X_train, y_train, transform=video_transform, use_roi_cropping=True)\n",
    "    val_dataset = VideoDataset(X_val, y_val, transform=video_transform, use_roi_cropping=True)\n",
    "    test_dataset = VideoDataset(X_test, y_test, transform=video_transform, use_roi_cropping=True)\n",
    "    \n",
    "    batch_size = 2  # Small batch size for testing\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"Testing model with ROI cropping...\")\n",
    "    avg_cer, avg_loss = test_model()\n",
    "    print(f\"Test CER: {avg_cer:.4f}, Test Loss: {avg_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
