{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Setup Environment\n",
        "\n",
        "This section clones the Arabic-Lip-Reading repository, installs required dependencies, and configures the model path for subsequent steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Clone Repository\n",
        "\n",
        "In this step, we clone the Arabic-Lip-Reading repository from GitHub to access the dataset and model code required for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-07-03T13:44:07.236283Z",
          "iopub.status.busy": "2025-07-03T13:44:07.235981Z",
          "iopub.status.idle": "2025-07-03T13:45:18.198982Z",
          "shell.execute_reply": "2025-07-03T13:45:18.197715Z",
          "shell.execute_reply.started": "2025-07-03T13:44:07.236258Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Executing Command: git clone --progress https://github.com/Essa-Ramzy/Arabic-Lip-Reading\n",
            "--------------------------------------------------------------------------------------------\n",
            "Cloning into 'Arabic-Lip-Reading'...\n",
            "remote: Enumerating objects: 119232, done.        \n",
            "remote: Counting objects: 100% (98/98), done.        \n",
            "remote: Compressing objects: 100% (51/51), done.        \n",
            "remote: Total 119232 (delta 69), reused 65 (delta 47), pack-reused 119134 (from 5)        \n",
            "Receiving objects: 100% (119232/119232), 2.05 GiB | 37.33 MiB/s, done.\n",
            "Resolving deltas: 100% (6678/6678), done.\n",
            "Updating files: 100% (61602/61602), done.\n",
            "--------------------------------------------------------------------------------------------\n",
            "✅ Successfully cloned https://github.com/Essa-Ramzy/Arabic-Lip-Reading\n",
            "📁 Repository saved to: /kaggle/working/Arabic-Lip-Reading\n",
            "📁 Added model path to system path: Arabic-Lip-Reading/model\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone the repository using subprocess\n",
        "repo_url = \"https://github.com/Essa-Ramzy/Arabic-Lip-Reading\"\n",
        "repo_name = \"Arabic-Lip-Reading\"\n",
        "\n",
        "# Check if the repository already exists\n",
        "if not os.path.exists(repo_name):\n",
        "    try:\n",
        "        print(f\"🔄 Executing Command: git clone --progress {repo_url}\")\n",
        "        print(\"-\" * 92)\n",
        "        \n",
        "        # Use subprocess.call so Git writes inline progress and we capture exit code\n",
        "        return_code = subprocess.call(\n",
        "            [\"git\", \"clone\", \"--progress\", repo_url],\n",
        "            stderr=subprocess.STDOUT\n",
        "        )\n",
        "        \n",
        "        print(\"-\" * 92)\n",
        "        if return_code == 0:\n",
        "            print(f\"✅ Successfully cloned {repo_url}\")\n",
        "            print(f\"📁 Repository saved to: {os.path.abspath(repo_name)}\")\n",
        "        else:\n",
        "            print(f\"❌ Git clone failed with return code: {return_code}\")\n",
        "            \n",
        "    except FileNotFoundError:\n",
        "        print(\"❌ Error: Git is not installed or not in PATH\")\n",
        "        print(\"💡 Please install Git first: https://git-scm.com/downloads\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"ℹ️  Repository {repo_name} already exists - skipping clone\")\n",
        "    print(f\"📁 Location: {os.path.abspath(repo_name)}\")\n",
        "\n",
        "# Add the model directory to the system path\n",
        "model_path = os.path.join(repo_name, 'model')\n",
        "if model_path not in sys.path:\n",
        "    sys.path.append(model_path)\n",
        "    print(f\"📁 Added model path to system path: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Install Dependencies\n",
        "\n",
        "In this step, we install the required Python libraries (e.g., kornia, editdistance) needed for data preprocessing, model training, and evaluation using pip.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:45:18.201684Z",
          "iopub.status.busy": "2025-07-03T13:45:18.201172Z",
          "iopub.status.idle": "2025-07-03T13:47:15.094177Z",
          "shell.execute_reply": "2025-07-03T13:47:15.092808Z",
          "shell.execute_reply.started": "2025-07-03T13:45:18.201657Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Executing Command: pip install kornia editdistance\n",
            "------------------------------------------------------------------------------------------\n",
            "Requirement already satisfied: kornia in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.11/dist-packages (from kornia) (0.1.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kornia) (25.0)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from kornia) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.1->kornia) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 2.4 MB/s eta 0:00:00\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 745.8 kB/s eta 0:00:00\n",
            "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 4.4 MB/s eta 0:00:00\n",
            "Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 17.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 7.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 3.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 67.1 MB/s eta 0:00:00\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
            "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
            "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
            "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "------------------------------------------------------------------------------------------\n",
            "✅ Successfully installed kornia and editdistance\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# Install Python dependencies\n",
        "try:\n",
        "    print(f\"🔄 Executing Command: pip install kornia editdistance\")\n",
        "    print(\"-\" * 90)\n",
        "    return_code = subprocess.call(\n",
        "        [\"pip\", \"install\", \"kornia\", \"editdistance\"],\n",
        "        stderr=subprocess.STDOUT\n",
        "    )\n",
        "    print(\"-\" * 90)\n",
        "    if return_code == 0:\n",
        "        print(\"✅ Successfully installed kornia and editdistance\")\n",
        "    else:\n",
        "        print(f\"❌ pip install failed with return code: {return_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Unexpected error during pip install: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Data Preparation\n",
        "\n",
        "This section outlines steps to download and extract the dataset, extract tokens and labels, and prepare PyTorch DataLoaders to serve batches during training and validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Configure Environment and Logging\n",
        "\n",
        "In this subsection, we set dataset normalization parameters, configure logging to record training progress and debug information, and initialize random seeds for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:47:15.096161Z",
          "iopub.status.busy": "2025-07-03T13:47:15.095781Z",
          "iopub.status.idle": "2025-07-03T13:47:28.498839Z",
          "shell.execute_reply": "2025-07-03T13:47:28.497525Z",
          "shell.execute_reply.started": "2025-07-03T13:47:15.096110Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated normalization parameters: MEAN=0.40947135433671134, STD=0.15003469454968454\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import shutil\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "from utils import *\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import traceback\n",
        "from e2e_vsr import E2EVSR\n",
        "import random\n",
        "from math import ceil\n",
        "from torch.amp import GradScaler, autocast\n",
        "import subprocess\n",
        "import zipfile\n",
        "\n",
        "# Dataset configuration\n",
        "DATASET_ROOT = 'Arabic-Lip-Reading/dataset'\n",
        "DATASET_NAME = 'LRC-AR'\n",
        "WITH_SPACES = True\n",
        "WITH_DIARITICS = True\n",
        "MANUAL_ONLY = False\n",
        "\n",
        "set_normalization_params(mean=0.40589704064965376 if MANUAL_ONLY else 0.40947135433671134,\n",
        "                         std=0.14899824732759526 if MANUAL_ONLY else 0.15003469454968454)\n",
        "\n",
        "os.makedirs('Logs', exist_ok=True)\n",
        "log_filename = f'Logs/training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
        "\n",
        "for h in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(h)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=log_filename,\n",
        "    level=logging.INFO,\n",
        "    format='%(message)s',\n",
        "    encoding='utf-8',\n",
        "    force=True \n",
        ")\n",
        "\n",
        "# Helper to print and log in one call\n",
        "def log_print(msg):\n",
        "    print(msg)\n",
        "    logging.info(msg)\n",
        "\n",
        "# Setting the seed for reproducibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(seed)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set the start method to 'spawn' for CUDA safety.\n",
        "torch.multiprocessing.set_start_method('spawn', force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Download and Extract Dataset\n",
        "\n",
        "In this subsection, we download the LRC-AR dataset from a Google Drive link if it is not already present, extract the archive into the dataset directory, and clean up temporary files to prepare for token extraction and loader setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:57:51.852474Z",
          "iopub.status.busy": "2025-07-03T13:57:51.852184Z",
          "iopub.status.idle": "2025-07-03T13:58:21.319246Z",
          "shell.execute_reply": "2025-07-03T13:58:21.318101Z",
          "shell.execute_reply.started": "2025-07-03T13:57:51.852455Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Executing Command: gdown https://drive.google.com/uc?id=1tX5YYTPbpWnOmj5zYEt8vG76iiq0kO8e\n",
            "---------------------------------------------------------------------------------------------\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1tX5YYTPbpWnOmj5zYEt8vG76iiq0kO8e\n",
            "From (redirected): https://drive.google.com/uc?id=1tX5YYTPbpWnOmj5zYEt8vG76iiq0kO8e&confirm=t&uuid=e931e166-7e70-40c5-9242-ad22a7c02168\n",
            "To: /kaggle/working/Arabic-Lip-Reading/dataset/LRC-AR.zip\n",
            "100%|██████████| 512M/512M [00:03<00:00, 147MB/s]  \n",
            "---------------------------------------------------------------------------------------------\n",
            "✅ Downloaded dataset\n",
            "{'ٱ': 1, 'يْ': 2, 'يّْ': 3, 'يِّ': 4, 'يُّ': 5, 'يَّ': 6, 'يٌّ': 7, 'يِ': 8, 'يُ': 9, 'يَ': 10, 'يٌ': 11, 'ي': 12, 'ى': 13, 'وْ': 14, 'وِّ': 15, 'وُّ': 16, 'وَّ': 17, 'وِ': 18, 'وُ': 19, 'وَ': 20, 'وً': 21, 'و': 22, 'هْ': 23, 'هُّ': 24, 'هِ': 25, 'هُ': 26, 'هَ': 27, 'نۢ': 28, 'نْ': 29, 'نِّ': 30, 'نُّ': 31, 'نَّ': 32, 'نِ': 33, 'نُ': 34, 'نَ': 35, 'مْ': 36, 'مّْ': 37, 'مِّ': 38, 'مُّ': 39, 'مَّ': 40, 'مِ': 41, 'مُ': 42, 'مَ': 43, 'مٍ': 44, 'مٌ': 45, 'مً': 46, 'لْ': 47, 'لّْ': 48, 'لِّ': 49, 'لُّ': 50, 'لَّ': 51, 'لِ': 52, 'لُ': 53, 'لَ': 54, 'لٍ': 55, 'لٌ': 56, 'لً': 57, 'كْ': 58, 'كِّ': 59, 'كَّ': 60, 'كِ': 61, 'كُ': 62, 'كَ': 63, 'قْ': 64, 'قَّ': 65, 'قِ': 66, 'قُ': 67, 'قَ': 68, 'قٍ': 69, 'قً': 70, 'فْ': 71, 'فِّ': 72, 'فَّ': 73, 'فِ': 74, 'فُ': 75, 'فَ': 76, 'غْ': 77, 'غِ': 78, 'غَ': 79, 'عْ': 80, 'عَّ': 81, 'عِ': 82, 'عُ': 83, 'عَ': 84, 'عٍ': 85, 'ظْ': 86, 'ظِّ': 87, 'ظَّ': 88, 'ظِ': 89, 'ظُ': 90, 'ظَ': 91, 'طْ': 92, 'طِّ': 93, 'طَّ': 94, 'طِ': 95, 'طُ': 96, 'طَ': 97, 'ضْ': 98, 'ضِّ': 99, 'ضُّ': 100, 'ضَّ': 101, 'ضِ': 102, 'ضُ': 103, 'ضَ': 104, 'ضً': 105, 'صْ': 106, 'صّْ': 107, 'صِّ': 108, 'صُّ': 109, 'صَّ': 110, 'صِ': 111, 'صُ': 112, 'صَ': 113, 'صٍ': 114, 'صً': 115, 'شْ': 116, 'شِّ': 117, 'شُّ': 118, 'شَّ': 119, 'شِ': 120, 'شُ': 121, 'شَ': 122, 'سْ': 123, 'سّْ': 124, 'سِّ': 125, 'سُّ': 126, 'سَّ': 127, 'سِ': 128, 'سُ': 129, 'سَ': 130, 'سٍ': 131, 'زْ': 132, 'زَّ': 133, 'زِ': 134, 'زُ': 135, 'زَ': 136, 'رْ': 137, 'رِّ': 138, 'رُّ': 139, 'رَّ': 140, 'رِ': 141, 'رُ': 142, 'رَ': 143, 'رٍ': 144, 'رٌ': 145, 'رً': 146, 'ذْ': 147, 'ذَّ': 148, 'ذِ': 149, 'ذُ': 150, 'ذَ': 151, 'دْ': 152, 'دِّ': 153, 'دُّ': 154, 'دَّ': 155, 'دًّ': 156, 'دِ': 157, 'دُ': 158, 'دَ': 159, 'دٍ': 160, 'دٌ': 161, 'دً': 162, 'خْ': 163, 'خِ': 164, 'خُ': 165, 'خَ': 166, 'حْ': 167, 'حَّ': 168, 'حِ': 169, 'حُ': 170, 'حَ': 171, 'جْ': 172, 'جِّ': 173, 'جُّ': 174, 'جَّ': 175, 'جِ': 176, 'جُ': 177, 'جَ': 178, 'ثْ': 179, 'ثِّ': 180, 'ثُّ': 181, 'ثَّ': 182, 'ثِ': 183, 'ثُ': 184, 'ثَ': 185, 'تْ': 186, 'تِّ': 187, 'تُّ': 188, 'تَّ': 189, 'تِ': 190, 'تُ': 191, 'تَ': 192, 'تٍ': 193, 'تٌ': 194, 'ةْ': 195, 'ةِ': 196, 'ةُ': 197, 'ةَ': 198, 'ةٍ': 199, 'ةٌ': 200, 'ةً': 201, 'بْ': 202, 'بِّ': 203, 'بَّ': 204, 'بِ': 205, 'بُ': 206, 'بَ': 207, 'بٍ': 208, 'بً': 209, 'ا': 210, 'ئْ': 211, 'ئِ': 212, 'ئَ': 213, 'ئً': 214, 'إِ': 215, 'ؤْ': 216, 'ؤُ': 217, 'ؤَ': 218, 'أْ': 219, 'أُ': 220, 'أَ': 221, 'آ': 222, 'ءْ': 223, 'ءِ': 224, 'ءَ': 225, 'ءً': 226, ' ': 227}\n"
          ]
        }
      ],
      "source": [
        "# Ensure dataset is available\n",
        "dataset_dir = os.path.join(DATASET_ROOT, DATASET_NAME)\n",
        "dataset_zip = os.path.join(DATASET_ROOT, f'{DATASET_NAME}.zip')\n",
        "os.makedirs(DATASET_ROOT, exist_ok=True)\n",
        "if not os.path.exists(dataset_dir):\n",
        "    try:\n",
        "        print(f\"🔄 Executing Command: gdown https://drive.google.com/uc?id=1tX5YYTPbpWnOmj5zYEt8vG76iiq0kO8e\")\n",
        "        print(\"-\" * 93)\n",
        "        return_code = subprocess.call(\n",
        "            [\"gdown\", \"https://drive.google.com/uc?id=1tX5YYTPbpWnOmj5zYEt8vG76iiq0kO8e\", \"-O\", dataset_zip],\n",
        "            stderr=subprocess.STDOUT\n",
        "        )\n",
        "        print(\"-\" * 93)\n",
        "        if return_code == 0:\n",
        "            print(\"✅ Downloaded dataset\")\n",
        "        else:\n",
        "            print(f\"❌ Download failed with return code: {return_code}\")\n",
        "        with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "            zip_ref.extractall(DATASET_ROOT)\n",
        "        os.remove(dataset_zip)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error during dataset download: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "tokens = set()\n",
        "# Collect tokens from all CSVs in new dataset structure\n",
        "for split in ['Train/Manually_Verified', 'Train/Gemini_Transcribed', 'Val/Manually_Verified']:\n",
        "    csv_dir = os.path.join(DATASET_ROOT, DATASET_NAME, split, 'Csv')\n",
        "    for fname in os.listdir(csv_dir):\n",
        "        path = os.path.join(csv_dir, fname)\n",
        "        label = extract_label(path, with_spaces=WITH_SPACES, with_diaritics=WITH_DIARITICS)\n",
        "        tokens.update(label)\n",
        "\n",
        "mapped_tokens = {}\n",
        "for i, c in enumerate(sorted(tokens, reverse=True), 1):\n",
        "    mapped_tokens[c] = i\n",
        "\n",
        "log_print(mapped_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Prepare Data Splits and Loaders\n",
        "\n",
        "This subsection handles the creation of training, validation, and test splits, including manual and auto-labeled data integration, and sets up PyTorch DataLoaders for efficient batch loading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:58:26.704532Z",
          "iopub.status.busy": "2025-07-03T13:58:26.704159Z",
          "iopub.status.idle": "2025-07-03T13:58:26.716071Z",
          "shell.execute_reply": "2025-07-03T13:58:26.715207Z",
          "shell.execute_reply.started": "2025-07-03T13:58:26.704486Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_dataloaders(manual_only=MANUAL_ONLY):\n",
        "    # Helper to gather video and csv file paths from directories\n",
        "    def load_paths_from_dir(video_dir, csv_dir):\n",
        "        videos, labels = [], []\n",
        "        for fname in sorted(os.listdir(video_dir)):\n",
        "            if not fname.endswith('.mp4'):\n",
        "                continue\n",
        "            base = os.path.splitext(fname)[0]\n",
        "            videos.append(os.path.join(video_dir, fname))\n",
        "            labels.append(os.path.join(csv_dir, base + \".csv\"))\n",
        "        return videos, labels\n",
        "    \n",
        "    # --- Step 1: Prepare paths using existing dataset directory ---\n",
        "    X_train, y_train, is_manual = [], [], []\n",
        "    # Select subsets based on manual_only flag\n",
        "    subsets = [('Manually_Verified', True)]\n",
        "    if not manual_only:\n",
        "        subsets.append(('Gemini_Transcribed', False))\n",
        "    for subset, manual_flag in subsets:\n",
        "        vid_dir = os.path.join(DATASET_ROOT, DATASET_NAME, 'Train', subset, 'Video')\n",
        "        csv_dir = os.path.join(DATASET_ROOT, DATASET_NAME, 'Train', subset, 'Csv')\n",
        "        v_paths, l_paths = load_paths_from_dir(vid_dir, csv_dir)\n",
        "        X_train.extend(v_paths)\n",
        "        y_train.extend(l_paths)\n",
        "        is_manual.extend([manual_flag] * len(v_paths))\n",
        "    # Validation from Val/Manually_Verified\n",
        "    val_vid_dir = os.path.join(DATASET_ROOT, DATASET_NAME, 'Val', 'Manually_Verified', 'Video')\n",
        "    val_csv_dir = os.path.join(DATASET_ROOT, DATASET_NAME, 'Val', 'Manually_Verified', 'Csv')\n",
        "    X_val, y_val = load_paths_from_dir(val_vid_dir, val_csv_dir)\n",
        "\n",
        "    # --- Step 2: Loader Setup ---\n",
        "    train_transform = VideoAugmentation(is_train=True)\n",
        "    train_dataset = VideoDataset(X_train, y_train, mapped_tokens, with_spaces=WITH_SPACES, with_diaritics=WITH_DIARITICS, transform=train_transform)\n",
        "    val_transform = VideoAugmentation(is_train=False)\n",
        "    val_dataset = VideoDataset(X_val, y_val, mapped_tokens, with_spaces=WITH_SPACES, with_diaritics=WITH_DIARITICS, transform=val_transform)\n",
        "\n",
        "    if manual_only:\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True,\n",
        "            collate_fn=pad_packed_collate, generator=generator, worker_init_fn=seed_worker\n",
        "        )\n",
        "    else:\n",
        "        weights = [5.0 if manual else 1.0 for manual in is_manual]\n",
        "        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True, generator=generator)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=64, sampler=sampler, num_workers=4, pin_memory=True,\n",
        "            collate_fn=pad_packed_collate, generator=generator, worker_init_fn=seed_worker\n",
        "        )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=64, shuffle=False, num_workers=4,\n",
        "        pin_memory=True, collate_fn=pad_packed_collate,\n",
        "        worker_init_fn=seed_worker\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Model Configuration\n",
        "\n",
        "This section defines the vocabulary indices, sets up different temporal encoder architectures (DenseTCN, MSTCN, Conformer), and initializes the end-to-end E2EVSR model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Build Token Mappings\n",
        "\n",
        "In this subsection, we construct the reverse index-to-character mapping (including blank, SOS, and EOS tokens) which is essential for decoding the model's output sequences back into readable text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:58:29.580261Z",
          "iopub.status.busy": "2025-07-03T13:58:29.579968Z",
          "iopub.status.idle": "2025-07-03T13:58:29.587304Z",
          "shell.execute_reply": "2025-07-03T13:58:29.586182Z",
          "shell.execute_reply.started": "2025-07-03T13:58:29.580241Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total vocabulary size: 230\n",
            "SOS token index: 228\n",
            "EOS token index: 229\n"
          ]
        }
      ],
      "source": [
        "# Build reverse mapping for decoding\n",
        "idx2char = [\"\"]  # Blank token for CTC\n",
        "idx2char.extend(mapped_tokens.keys())\n",
        "sos_token_idx = len(idx2char)\n",
        "idx2char.append(\"<sos>\")  # SOS token\n",
        "eos_token_idx = sos_token_idx + 1\n",
        "idx2char.append(\"<eos>\")  # EOS token\n",
        "full_vocab_size = eos_token_idx + 1\n",
        "accumulation_steps = 4\n",
        "log_print(f\"Total vocabulary size: {full_vocab_size}\")\n",
        "log_print(f\"SOS token index: {sos_token_idx}\")\n",
        "log_print(f\"EOS token index: {eos_token_idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Temporal Encoder Options\n",
        "\n",
        "In this subsection, we define and configure the temporal encoder modules—including DenseTCN, MSTCN, and Conformer—by specifying layer blocks, growth rates, kernel sizes, and other hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:58:31.102830Z",
          "iopub.status.busy": "2025-07-03T13:58:31.102482Z",
          "iopub.status.idle": "2025-07-03T13:58:31.111567Z",
          "shell.execute_reply": "2025-07-03T13:58:31.110410Z",
          "shell.execute_reply.started": "2025-07-03T13:58:31.102809Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected temporal encoder: conformer with hidden dimension 768\n"
          ]
        }
      ],
      "source": [
        "# DenseTCN configuration\n",
        "densetcn_options = {\n",
        "    'block_config': [4, 4, 4, 4],\n",
        "    'growth_rate_set': [512, 512, 512, 512],\n",
        "    'reduced_size': 768,\n",
        "    'kernel_size_set': [3, 5, 7, 9],\n",
        "    'dilation_size_set': [1, 2, 4, 8],\n",
        "    'squeeze_excitation': True,\n",
        "    'dropout': 0.1,\n",
        "    'hidden_dim': 768,\n",
        "}\n",
        "\n",
        "# MSTCN configuration\n",
        "mstcn_options = {\n",
        "    'tcn_type': 'multiscale',\n",
        "    'hidden_dim': 768,\n",
        "    'num_channels': [512, 512, 512, 512],\n",
        "    'kernel_size': [3, 5, 7, 9],                   \n",
        "    'dropout': 0.1,\n",
        "    'stride': 1,\n",
        "    'width_mult': 1.0,\n",
        "}\n",
        "\n",
        "# Conformer configuration (our default backbone)\n",
        "conformer_options = {\n",
        "    'attention_dim': 768,\n",
        "    'attention_heads': 12,\n",
        "    'linear_units': 3072,\n",
        "    'num_blocks': 12,\n",
        "    'dropout_rate': 0.1,\n",
        "    'positional_dropout_rate': 0.1,\n",
        "    'attention_dropout_rate': 0.1,\n",
        "    'cnn_module_kernel': 31\n",
        "}\n",
        "\n",
        "\n",
        "# Choose temporal encoder type: 'densetcn', 'mstcn', or 'conformer'\n",
        "TEMPORAL_ENCODER = 'conformer'\n",
        "\n",
        "# Determine hidden_dim for E2EVSR based on the chosen temporal encoder\n",
        "if TEMPORAL_ENCODER == 'densetcn':\n",
        "    e2e_hidden_dim = densetcn_options['hidden_dim']\n",
        "elif TEMPORAL_ENCODER == 'mstcn':\n",
        "    e2e_hidden_dim = mstcn_options['hidden_dim']\n",
        "elif TEMPORAL_ENCODER == 'conformer':\n",
        "    e2e_hidden_dim = conformer_options['attention_dim']\n",
        "else:\n",
        "    raise ValueError(f\"Unknown TEMPORAL_ENCODER: {TEMPORAL_ENCODER}\")\n",
        "\n",
        "log_print(f\"Selected temporal encoder: {TEMPORAL_ENCODER} with hidden dimension {e2e_hidden_dim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Initialize Model\n",
        "\n",
        "Here we initialize the E2EVSR model with chosen encoder, decoder, and set CTC weight and label smoothing. The model is then moved to the appropriate compute device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:58:32.863634Z",
          "iopub.status.busy": "2025-07-03T13:58:32.863246Z",
          "iopub.status.idle": "2025-07-03T13:58:39.475754Z",
          "shell.execute_reply": "2025-07-03T13:58:39.474671Z",
          "shell.execute_reply.started": "2025-07-03T13:58:32.863608Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing E2EVSR end-to-end model...\n",
            "E2EVSR(\n",
            "  (frontend): VisualFrontend(\n",
            "    (frontend3D): Sequential(\n",
            "      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\n",
            "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): SiLU(inplace=True)\n",
            "      (3): Dropout3d(p=0.0, inplace=False)\n",
            "      (4): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (resnet_trunk): ResNet(\n",
            "      (layer1): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): SiLU()\n",
            "          (relu2): SiLU()\n",
            "          (dropout): Identity()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): BasicBlock(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): SiLU()\n",
            "          (relu2): SiLU()\n",
            "          (dropout): Identity()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): SiLU()\n",
            "          (relu2): SiLU()\n",
            "          (dropout): Identity()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BasicBlock(\n",
            "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): SiLU()\n",
            "          (relu2): SiLU()\n",
            "          (dropout): Identity()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): SiLU()\n",
            "          (relu2): SiLU()\n",
            "          (dropout): Identity()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BasicBlock(\n",
            "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): SiLU()\n",
            "          (relu2): SiLU()\n",
            "          (dropout): Identity()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): SiLU()\n",
            "          (relu2): SiLU()\n",
            "          (dropout): Identity()\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BasicBlock(\n",
            "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): SiLU()\n",
            "          (relu2): SiLU()\n",
            "          (dropout): Identity()\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "    )\n",
            "  )\n",
            "  (proj_encoder): Linear(in_features=512, out_features=768, bias=True)\n",
            "  (encoder): ConformerEncoder(\n",
            "    (embed): Sequential(\n",
            "      (0): RelPositionalEncoding(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (encoders): MultiSequential(\n",
            "      (0): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (6): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (7): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (8): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (9): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (10): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (11): EncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_pos): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_cov1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
            "          (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_cov2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU(inplace=True)\n",
            "        )\n",
            "        (norm_ff): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm_ff_macaron): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (after_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed): Sequential(\n",
            "      (0): Embedding(230, 768)\n",
            "      (1): PositionalEncoding(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (decoders): MultiSequential(\n",
            "      (0): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (after_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (output_layer): Linear(in_features=768, out_features=230, bias=True)\n",
            "  )\n",
            "  (ctc): CTC(\n",
            "    (ctc_lo): Linear(in_features=768, out_features=230, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (ctc_loss): CTCLoss()\n",
            "  )\n",
            "  (criterion): LabelSmoothingLoss(\n",
            "    (criterion): KLDivLoss()\n",
            "  )\n",
            "  (att_loss): LabelSmoothingLoss(\n",
            "    (criterion): KLDivLoss()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialize the E2EVSR end-to-end model\n",
        "log_print(\"Initializing E2EVSR end-to-end model...\")\n",
        "\n",
        "e2e_model = E2EVSR(\n",
        "    encoder_type=TEMPORAL_ENCODER,\n",
        "    vocab_size=full_vocab_size,\n",
        "    token_list=idx2char,\n",
        "    sos=sos_token_idx,\n",
        "    eos=eos_token_idx,\n",
        "    pad=0,\n",
        "    enc_options={\n",
        "        'densetcn_options': densetcn_options,\n",
        "        'mstcn_options': mstcn_options,\n",
        "        'conformer_options': conformer_options,\n",
        "        'hidden_dim': e2e_hidden_dim,\n",
        "        'frontend3d_dropout_rate': 0.0,\n",
        "        'resnet_dropout_rate': 0.0\n",
        "    },\n",
        "    dec_options={\n",
        "        'attention_dim': e2e_hidden_dim,\n",
        "        'attention_heads': 12,\n",
        "        'linear_units': 3072,\n",
        "        'num_blocks': 6,\n",
        "        'dropout_rate': 0.1,\n",
        "        'positional_dropout_rate': 0.1,\n",
        "        'self_attention_dropout_rate': 0.1,\n",
        "        'src_attention_dropout_rate': 0.1,\n",
        "        'normalize_before': True,\n",
        "    },\n",
        "    ctc_weight=0.5,\n",
        "    label_smoothing=0.1,\n",
        ").to(device)\n",
        "\n",
        "log_print(repr(e2e_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Load Pretrained Weights\n",
        "\n",
        "We download and load pretrained frontend weights into the model to leverage pre-trained visual features before training the full end-to-end network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:58:39.477969Z",
          "iopub.status.busy": "2025-07-03T13:58:39.477608Z",
          "iopub.status.idle": "2025-07-03T13:58:52.658200Z",
          "shell.execute_reply": "2025-07-03T13:58:52.657252Z",
          "shell.execute_reply.started": "2025-07-03T13:58:39.477938Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pretrained frontend weights...\n",
            "🔄 Executing Command: gdown https://drive.google.com/uc?id=1r1kx7l9sWnDOCnaFHIGvOtzuhFyFA88_\n",
            "---------------------------------------------------------------------------------------------\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1r1kx7l9sWnDOCnaFHIGvOtzuhFyFA88_\n",
            "From (redirected): https://drive.google.com/uc?id=1r1kx7l9sWnDOCnaFHIGvOtzuhFyFA88_&confirm=t&uuid=0b19b0e5-9608-4df4-8ec5-33cb2aee9d8c\n",
            "To: /kaggle/working/vsr_trlrs2lrs3vox2avsp_base.pth\n",
            "100%|██████████| 1.00G/1.00G [00:09<00:00, 109MB/s] \n",
            "---------------------------------------------------------------------------------------------\n",
            "✅ Successfully downloaded pretrained weights\n",
            "Loaded pretrained weights from vsr_trlrs2lrs3vox2avsp_base.pth\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained frontend weights\n",
        "log_print(\"Loading pretrained frontend weights...\")\n",
        "\n",
        "pretrained_path = 'vsr_trlrs2lrs3vox2avsp_base.pth'\n",
        "if not os.path.exists(pretrained_path):\n",
        "    try:\n",
        "        print(f\"🔄 Executing Command: gdown https://drive.google.com/uc?id=1r1kx7l9sWnDOCnaFHIGvOtzuhFyFA88_\")\n",
        "        print(\"-\" * 93)\n",
        "        return_code = subprocess.call(\n",
        "            [\"gdown\", \"https://drive.google.com/uc?id=1r1kx7l9sWnDOCnaFHIGvOtzuhFyFA88_\"],\n",
        "            stderr=subprocess.STDOUT\n",
        "        )\n",
        "        print(\"-\" * 93)\n",
        "        if return_code == 0:\n",
        "            print(\"✅ Successfully downloaded pretrained weights\")\n",
        "        else:\n",
        "            print(f\"❌ Download failed with return code: {return_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error during pretrained weights download: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "pretrained_weights = torch.load(pretrained_path, map_location='cpu', weights_only=False)#['e2e_model_state_dict']\n",
        "pretrained_weights = {\n",
        "    k.replace('trunk', 'resnet_trunk'): v for k, v in pretrained_weights.items()\n",
        "    if 'decoder.embed.0.weight' not in k and\n",
        "       'decoder.output_layer.weight' not in k and\n",
        "       'decoder.output_layer.bias' not in k and\n",
        "       'ctc.ctc_lo.weight' not in k and\n",
        "       'ctc.ctc_lo.bias' not in k\n",
        "}\n",
        "\n",
        "# Load weights into frontend\n",
        "e2e_model.load_state_dict(pretrained_weights, strict=False)\n",
        "log_print(f\"Loaded pretrained weights from {pretrained_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Training Hyperparameters and Scheduler\n",
        "\n",
        "Here we configure the training hyperparameters, including learning rate setup, optimizer weight decay, warmup-cosine scheduler, and gradient clipping to ensure stable and effective model convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:59:05.553287Z",
          "iopub.status.busy": "2025-07-03T13:59:05.552992Z",
          "iopub.status.idle": "2025-07-03T13:59:05.624615Z",
          "shell.execute_reply": "2025-07-03T13:59:05.623420Z",
          "shell.execute_reply.started": "2025-07-03T13:59:05.553269Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial learning rate: 0.00010340380070341652\n",
            "Total epochs: 60, Warmup epochs: 5\n",
            "Steps per epoch: 51\n"
          ]
        }
      ],
      "source": [
        "suggested_lr = 0.00010340380070341652\n",
        "total_epochs = 60\n",
        "warmup_epochs = 5\n",
        "alpha = 0.0\n",
        "overshoot = 1 + alpha * (warmup_epochs / total_epochs)\n",
        "initial_lr = suggested_lr * overshoot\n",
        "\n",
        "# Use a smaller weight decay, and disable decay on normalization layers\n",
        "decay_params = []\n",
        "no_decay_params = []\n",
        "for name, param in e2e_model.named_parameters():\n",
        "    if \"ln\" in name.lower() or \"norm\" in name.lower() or \"bias\" in name.lower():\n",
        "        no_decay_params.append(param)\n",
        "    else:\n",
        "        decay_params.append(param)\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    [\n",
        "      {\"params\": decay_params,    \"weight_decay\": 0.02},\n",
        "      {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
        "    ],\n",
        "    lr=initial_lr,\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9\n",
        ")\n",
        "\n",
        "train_loader, val_loader = load_dataloaders()\n",
        "steps_per_epoch = ceil(len(train_loader) / accumulation_steps)\n",
        "scheduler = WarmupCosineScheduler(optimizer, warmup_epochs, total_epochs, steps_per_epoch)\n",
        "\n",
        "# Add gradient clipping during training\n",
        "grad_clip_value = 1.0\n",
        "\n",
        "log_print(f\"Initial learning rate: {initial_lr}\")\n",
        "log_print(f\"Total epochs: {total_epochs}, Warmup epochs: {warmup_epochs}\")\n",
        "log_print(f\"Steps per epoch: {steps_per_epoch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Training and Evaluation Helpers\n",
        "\n",
        "This section provides utility functions for seed management, single-epoch training, model evaluation, and an orchestrated training routine with checkpointing and early stopping mechanisms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 RNG State Management\n",
        "\n",
        "These functions save and restore the random number generator states across PyTorch, NumPy, Python's random module, and CUDA to ensure reproducible training and evaluation runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:59:10.157676Z",
          "iopub.status.busy": "2025-07-03T13:59:10.157160Z",
          "iopub.status.idle": "2025-07-03T13:59:10.164352Z",
          "shell.execute_reply": "2025-07-03T13:59:10.163220Z",
          "shell.execute_reply.started": "2025-07-03T13:59:10.157648Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_rng_state():\n",
        "    return {\n",
        "        'torch': torch.get_rng_state(),\n",
        "        'numpy': np.random.get_state(),\n",
        "        'random': random.getstate(),\n",
        "        'generator': generator.get_state(),\n",
        "        'cuda': torch.cuda.get_rng_state() if torch.cuda.is_available() else None\n",
        "    }\n",
        "\n",
        "def set_rng_state(state):\n",
        "    torch.set_rng_state(state['torch'].cpu())\n",
        "    np.random.set_state(state['numpy'])\n",
        "    random.setstate(state['random'])\n",
        "    generator.set_state(state['generator'])\n",
        "    if torch.cuda.is_available() and 'cuda' in state and state['cuda'] is not None:\n",
        "        torch.cuda.set_rng_state(state['cuda'].cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Single Epoch Training Loop\n",
        "\n",
        "This function handles one full epoch of training, utilizing mixed-precision forward and backward passes, gradient accumulation, gradient clipping, and detailed logging of batch metrics and learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:59:12.914635Z",
          "iopub.status.busy": "2025-07-03T13:59:12.914285Z",
          "iopub.status.idle": "2025-07-03T13:59:12.928362Z",
          "shell.execute_reply": "2025-07-03T13:59:12.927218Z",
          "shell.execute_reply.started": "2025-07-03T13:59:12.914610Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(data_loader, scaler):\n",
        "    running_loss = 0.0\n",
        "    equal_loss = 0.0\n",
        "    e2e_model.train()\n",
        "    \n",
        "    # Settings for gradient accumulation\n",
        "    processed_batches = 0\n",
        "\n",
        "    for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(data_loader):\n",
        "        # Print input shape for debugging\n",
        "        logging.info(f\"\\nBatch {batch_idx+1} - Input shape: {inputs.shape}\")\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        input_lengths = input_lengths.to(device)\n",
        "        labels_flat = labels_flat.to(device)\n",
        "        label_lengths = label_lengths.to(device)\n",
        "\n",
        "        # Only zero gradients at the start of accumulation cycle\n",
        "        if processed_batches % accumulation_steps == 0:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        try:\n",
        "            # Use mixed precision for forward pass\n",
        "            with autocast(device.type):\n",
        "                # End-to-end forward (CTC+Attention)\n",
        "                out = e2e_model(inputs, input_lengths, ys=labels_flat, ys_lengths=label_lengths)\n",
        "                if batch_idx % 10 == 0:\n",
        "                    log_print(f\"decoder_loss: {out['att_loss']}, ctc_loss: {out['ctc_loss']}\")\n",
        "                # Scale loss by accumulation steps\n",
        "                loss = out['loss'] / accumulation_steps\n",
        "            \n",
        "            # Backward with scaled gradients\n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            # Update weights if we've completed an accumulation cycle\n",
        "            if (processed_batches + 1) % accumulation_steps == 0:\n",
        "                # Apply gradient clipping before stepping\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(e2e_model.parameters(), max_norm=grad_clip_value)\n",
        "                \n",
        "                # Step optimizer and update scaler\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                scheduler.step()\n",
        "            \n",
        "            processed_batches += 1\n",
        "            running_loss += loss.item() * accumulation_steps  # Re-scale loss for reporting\n",
        "            equal_loss += 0.5 * out['ctc_loss'] + 0.5 * out['att_loss']\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                lr = optimizer.param_groups[0]['lr']\n",
        "                logging.info(f\"Batch {batch_idx+1}, Loss: {loss.item()*accumulation_steps:.4f}, LR: {lr:.7f}\")\n",
        "\n",
        "            del out, loss\n",
        "                \n",
        "        except Exception as e:\n",
        "            logging.info(f\"Error in training loop for batch {batch_idx}: {str(e)}\") \n",
        "            logging.info(f\"Error type: {type(e).__name__}\")\n",
        "            import traceback\n",
        "            traceback_str = traceback.format_exc()\n",
        "            logging.info(traceback_str)\n",
        "\n",
        "            log_print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "        del inputs, input_lengths, labels_flat, label_lengths\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            logging.info(f\"Memory cleared. Current GPU memory: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
        "\n",
        "    print(f\"Final LR: {optimizer.param_groups[0]['lr']}\")\n",
        "    return running_loss / len(train_loader) if len(train_loader) > 0 else 0.0, equal_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Model Evaluation\n",
        "\n",
        "This function evaluates the trained model using greedy and beam-search decoding on validation or test sets, computes character error rate (CER) and edit distance metrics, and logs both aggregate statistics and individual sample predictions for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:59:15.250482Z",
          "iopub.status.busy": "2025-07-03T13:59:15.250162Z",
          "iopub.status.idle": "2025-07-03T13:59:15.271530Z",
          "shell.execute_reply": "2025-07-03T13:59:15.270443Z",
          "shell.execute_reply.started": "2025-07-03T13:59:15.250454Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def evaluate_model(data_loader, epoch=None, print_samples=True, mode='hybrid'):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given data loader using greedy decoding.\n",
        "    Returns the average CER.\n",
        "    \"\"\"\n",
        "    e2e_model.eval()\n",
        "\n",
        "    # Track statistics\n",
        "    total_cer = 0\n",
        "    sample_count = 0\n",
        "    all_predictions = []\n",
        "\n",
        "    # Determine if we should print samples in this epoch\n",
        "    show_samples = (epoch is None or epoch == 0 or (epoch+1) % 5 == 0) and print_samples\n",
        "    max_samples_to_print = 10\n",
        "\n",
        "    # Process all batches in the test loader\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(data_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            input_lengths = input_lengths.to(device)\n",
        "            labels_flat = labels_flat.to(device)\n",
        "            label_lengths = label_lengths.to(device)\n",
        "            \n",
        "            if show_samples and i == 0:\n",
        "                log_print(f\"\\nRunning {mode} beam decoding for validation...\")\n",
        "            \n",
        "            try:\n",
        "                with autocast(device.type, enabled=False):\n",
        "                    if mode == 'transformer':\n",
        "                        # Pure attention decoding\n",
        "                        all_results = e2e_model.beam_search(inputs, input_lengths, ctc_weight=0.0)\n",
        "                    elif mode == 'ctc':\n",
        "                        # Pure CTC decoding\n",
        "                        all_results = e2e_model.beam_search(inputs, input_lengths, ctc_weight=1.0)\n",
        "                    elif mode == 'hybrid':\n",
        "                        # Hybrid decoding with a specific weight\n",
        "                        all_results = e2e_model.beam_search(inputs, input_lengths, ctc_weight=0.3)\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown decoding mode: {mode}\")\n",
        "                \n",
        "                logging.info(f\"Beam search decoding completed for batch {i+1}\")\n",
        "                logging.info(f\"Received {len(all_results)} result sequences using mode {mode} beam search\")\n",
        "                \n",
        "                # Process each batch item\n",
        "                for b in range(label_lengths.size(0)):\n",
        "                    logging.info(f\"\\nProcessing batch item {b+1}/{label_lengths.size(0)}\")\n",
        "                    sample_count += 1\n",
        "                    \n",
        "                    if b < len(all_results):\n",
        "                        # Get predicted token indices\n",
        "                        pred_indices = all_results[b]\n",
        "                    \n",
        "                    if len(pred_indices) == 0:\n",
        "                        log_print(\"WARNING: Prediction sequence is empty!\")\n",
        "                    \n",
        "                    # Get target indices\n",
        "                    start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
        "                    end_idx = start_idx + label_lengths[b].item()\n",
        "                    target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
        "\n",
        "                    # Log debug information for reference and hypothesis tokens\n",
        "                    logging.info(f\"Reference tokens ({len(target_idx)} tokens): {target_idx}\")\n",
        "                    logging.info(f\"Hypothesis tokens ({len(pred_indices)} tokens): {pred_indices}\")\n",
        "                    \n",
        "                    # Reference sequence\n",
        "                    ref_seq = target_idx.tolist()\n",
        "                    # Direct greedy output without cleaning\n",
        "                    cleaned_seq = list(pred_indices)\n",
        "                    \n",
        "                    # compute CER and edit distance on cleaned sequence\n",
        "                    cer, edit_dist = compute_cer(ref_seq, cleaned_seq)\n",
        "                    pred_text = indices_to_text(cleaned_seq, idx2char)\n",
        "                    \n",
        "                    target_text = indices_to_text(target_idx, idx2char)\n",
        "                    \n",
        "                    # Log using the filtered best sequence\n",
        "                    # Update statistics\n",
        "                    total_cer += cer\n",
        "                    \n",
        "                    # Store prediction details\n",
        "                    all_predictions.append({\n",
        "                        'sample_id': sample_count,\n",
        "                        'pred_text': pred_text,\n",
        "                        'target_text': target_text,\n",
        "                        'cer': cer,\n",
        "                        'edit_distance': edit_dist,\n",
        "                    })\n",
        "                    \n",
        "                    # Log complete info\n",
        "                    logging.info(\"-\" * 50)\n",
        "                    logging.info(f\"Sample {sample_count}:\")\n",
        "                    try:\n",
        "                        logging.info(f\"Predicted text: {pred_text}\")\n",
        "                        logging.info(f\"Target text: {target_text}\")\n",
        "                    except UnicodeEncodeError:\n",
        "                        logging.info(\"Predicted text: [Contains characters that can't be displayed in console]\")\n",
        "                        logging.info(\"Target text: [Contains characters that can't be displayed in console]\")\n",
        "                        logging.info(f\"Predicted indices: {pred_indices}\")\n",
        "                        logging.info(f\"Target indices: {target_idx}\")\n",
        "                        \n",
        "                    logging.info(f\"Edit distance: {edit_dist}\")\n",
        "                    logging.info(f\"CER: {cer:.4f}\")\n",
        "                    logging.info(\"-\" * 50)\n",
        "                    \n",
        "                    # Print to console if this is a sample we should show\n",
        "                    if show_samples and sample_count <= max_samples_to_print:\n",
        "                        print(\"-\" * 50)\n",
        "                        print(f\"Sample {sample_count}:\")\n",
        "                        try:\n",
        "                            print(f\"Predicted text: {pred_text}\")\n",
        "                            print(f\"Target text: {target_text}\")\n",
        "                        except UnicodeEncodeError:\n",
        "                            print(\"Predicted text: [Contains characters that can't be displayed in console]\")\n",
        "                            print(\"Target text: [Contains characters that can't be displayed in console]\")\n",
        "                            \n",
        "                        print(f\"Edit distance: {edit_dist}\")\n",
        "                        print(f\"CER: {cer:.4f}\")\n",
        "                        print(\"-\" * 50)\n",
        "\n",
        "                # Clean up tensors\n",
        "                del all_results\n",
        "                \n",
        "                # Periodically clear cache\n",
        "                if i % 3 == 0:  # Every 3 batches\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                        logging.info(f\"Memory cleared. Current GPU memory: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
        "            \n",
        "            except Exception as e:\n",
        "                log_print(f\"Error during greedy decoding: {str(e)}\")\n",
        "                log_print(traceback.format_exc())\n",
        "                raise\n",
        "\n",
        "            del inputs, input_lengths, labels_flat, label_lengths\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                logging.info(f\"Memory cleared. Current GPU memory: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
        "        \n",
        "        # Calculate average CER\n",
        "        n_samples = len(data_loader.dataset)\n",
        "        avg_cer = total_cer / n_samples if n_samples > 0 else float('inf')\n",
        "        \n",
        "        # Always print summary statistics to console\n",
        "        log_print(\"\\n=== Summary Statistics ===\")\n",
        "        log_print(f\"Total samples: {n_samples}\")\n",
        "        log_print(f\"Average CER: {avg_cer:.4f}\\n\")\n",
        "        \n",
        "        return avg_cer\n",
        "\n",
        "\n",
        "def evaluate_loss(data_loader):\n",
        "    \"\"\"\n",
        "    Compute average CTC+Attention loss on dev set with teacher forcing.\n",
        "    \"\"\"\n",
        "    e2e_model.eval()\n",
        "    running_loss = 0.0\n",
        "    # Save original weight\n",
        "    original_ctc_weight = e2e_model.ctc_weight\n",
        "    # Use a fixed weight for validation (e.g., 0.5)\n",
        "    e2e_model.ctc_weight = 0.5\n",
        "    with torch.no_grad():\n",
        "        for inputs, input_lengths, labels_flat, label_lengths in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            input_lengths = input_lengths.to(device)\n",
        "            labels_flat = labels_flat.to(device)\n",
        "            label_lengths = label_lengths.to(device)\n",
        "            out = e2e_model(\n",
        "                inputs, input_lengths,\n",
        "                ys=labels_flat, ys_lengths=label_lengths\n",
        "            )\n",
        "            running_loss += out['loss'].item()\n",
        "            del inputs, input_lengths, labels_flat, label_lengths, out\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "    # Restore original weight\n",
        "    e2e_model.ctc_weight = original_ctc_weight\n",
        "    return running_loss / len(data_loader) if len(data_loader) > 0 else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Training Pipeline and Checkpointing\n",
        "\n",
        "In this subsection, we define the complete training loop with curriculum learning stages, early stopping, model checkpoint saving, and best-model tracking to manage long-running experiments efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T13:59:38.644889Z",
          "iopub.status.busy": "2025-07-03T13:59:38.644427Z",
          "iopub.status.idle": "2025-07-03T13:59:38.683291Z",
          "shell.execute_reply": "2025-07-03T13:59:38.682272Z",
          "shell.execute_reply.started": "2025-07-03T13:59:38.644857Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_model(checkpoint_path=None):\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_cer = float('inf')\n",
        "    best_epoch = -1\n",
        "    start_epoch = 0\n",
        "    patience_counter = 0\n",
        "    max_patience = 10  # Increased early stopping patience\n",
        "    \n",
        "    # For mixed precision training\n",
        "    scaler = GradScaler(device.type)\n",
        "    \n",
        "    # Curriculum learning - track stage\n",
        "    curriculum_stage = 1  # Start with stage 1 (focus on CTC)\n",
        "    stage_transitions = [total_epochs // 3, 2 * total_epochs // 3]  # Transition at epochs 20 and 40\n",
        "    \n",
        "    # Load checkpoint if provided\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        log_print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "        \n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "            \n",
        "            # Load E2E model checkpoint non-strictly (ignoring mismatched keys)\n",
        "            dec_res = e2e_model.load_state_dict(checkpoint['e2e_model_state_dict'], strict=False)\n",
        "            log_print(f\"Loaded e2e_model checkpoint (non-strict): missing {dec_res.missing_keys}, unexpected {dec_res.unexpected_keys}\")\n",
        "            \n",
        "            # Load optimizer state\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "            \n",
        "            # Update training state\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "            best_val_cer = checkpoint.get('best_val_cer', float('inf'))\n",
        "            best_epoch = checkpoint.get('best_epoch', -1)\n",
        "            \n",
        "            # Determine curriculum stage based on loaded epoch\n",
        "            if start_epoch >= stage_transitions[1]:\n",
        "                curriculum_stage = 3\n",
        "            elif start_epoch >= stage_transitions[0]:\n",
        "                curriculum_stage = 2\n",
        "            else:\n",
        "                curriculum_stage = 1\n",
        "                \n",
        "            log_print(f\"Resuming at curriculum stage {curriculum_stage}\")\n",
        "            \n",
        "            # Restore RNG state if available\n",
        "            if 'rng_state' in checkpoint:\n",
        "                try:\n",
        "                    set_rng_state(checkpoint['rng_state'])\n",
        "                    # Success\n",
        "                    log_print(\"RNG state restored successfully\")\n",
        "                except Exception as e:\n",
        "                    log_print(f\"Warning: Could not restore RNG state: {e}. Continuing with current RNG state.\")\n",
        "            \n",
        "            log_print(f\"Checkpoint loaded successfully. Resuming from epoch {start_epoch + 1}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            log_print(f\"Error loading checkpoint: {str(e)}\")\n",
        "            log_print(\"Aborting training due to checkpoint loading failure.\")\n",
        "            raise\n",
        "        \n",
        "    else:\n",
        "        if checkpoint_path:\n",
        "            log_print(f\"Checkpoint file {checkpoint_path} not found. Starting training from scratch.\")\n",
        "        else:\n",
        "            log_print(\"No checkpoint specified. Starting training from scratch.\")\n",
        "    \n",
        "    train_loader, val_loader = load_dataloaders()\n",
        "    print(f\"Starting training for {total_epochs} epochs\")\n",
        "    print(f\"Logs will be saved to {log_filename}\")\n",
        "    print(f\"Checkpoints will be saved every 5 epochs\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for epoch in range(start_epoch, total_epochs):\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            logging.info(f\"GPU memory before training: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
        "        \n",
        "        # Update curriculum stage if needed\n",
        "        if epoch == stage_transitions[0]:\n",
        "            curriculum_stage = 2\n",
        "            log_print(f\"Moving to curriculum stage 2: Balanced CTC-Attention learning\")\n",
        "        elif epoch == stage_transitions[1]:\n",
        "            curriculum_stage = 3\n",
        "            log_print(f\"Moving to curriculum stage 3: Focused attention learning\")\n",
        "        \n",
        "        # Dynamically adjust CTC weight based on curriculum stage\n",
        "        if curriculum_stage == 1:\n",
        "            # Stage 1: CTC-focused learning (helps establish alignment)\n",
        "            e2e_model.ctc_weight = 0.7\n",
        "        elif curriculum_stage == 2:\n",
        "            # Stage 2: Balanced CTC-attention learning\n",
        "            e2e_model.ctc_weight = 0.5\n",
        "        else:\n",
        "            # Stage 3: Attention-focused learning\n",
        "            e2e_model.ctc_weight = max(0.2, 0.4 - 0.01 * (epoch - stage_transitions[1]))\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{total_epochs} - Training (Stage {curriculum_stage}, CTC weight: {e2e_model.ctc_weight:.3f})...\")\n",
        "        epoch_loss, equal_loss = train_one_epoch(train_loader, scaler)\n",
        "    \n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            logging.info(f\"GPU memory after training: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{total_epochs} - Evaluating...\")\n",
        "        # First compute validation loss under teacher forcing\n",
        "        val_loss = evaluate_loss(val_loader)\n",
        "        \n",
        "        # Compute CER more frequently in later stages\n",
        "        eval_cer = (epoch + 1) % 5 == 0 or curriculum_stage >= 2\n",
        "        \n",
        "        if eval_cer:\n",
        "            # Then compute decoding metrics (CER) via greedy decoding\n",
        "            val_cer = {\n",
        "                'Hybrid': evaluate_model(val_loader, epoch=epoch, print_samples=True, mode='hybrid'),\n",
        "                # 'Transformer': evaluate_model(val_loader, epoch=epoch, print_samples=True, mode='transformer'),\n",
        "                # 'CTC': evaluate_model(val_loader, epoch=epoch, print_samples=True, mode='ctc'),\n",
        "            }\n",
        "        else:\n",
        "            val_cer = None  # Skip CER evaluation this epoch\n",
        "        \n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            logging.info(f\"GPU memory after evaluation: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
        "        \n",
        "        log_print(\n",
        "            f\"Epoch {epoch + 1}/{total_epochs}\"\n",
        "            f\", Train Loss: {epoch_loss:.4f}\"\n",
        "            f\", Equal Loss: {equal_loss:.4f}\"\n",
        "            f\", Val Loss: {val_loss:.4f}\"\n",
        "            + (f\", Val Hybrid CER: {val_cer['Hybrid']:.4f}\" if isinstance(val_cer, dict) else \"\")\n",
        "        )#, Val Transformer CER: {val_cer['Transformer']:.4f}, Val CTC CER: {val_cer['CTC']:.4f}\n",
        "\n",
        "        # Early stopping check\n",
        "        improved = False\n",
        "        \n",
        "        # First priority - improve CER\n",
        "        if isinstance(val_cer, dict) and val_cer['Hybrid'] < best_val_cer:\n",
        "            best_val_cer = val_cer['Hybrid']\n",
        "            best_epoch = epoch\n",
        "            improved = True\n",
        "            log_print(f\"New best validation CER: {val_cer['Hybrid']:.4f}\")\n",
        "\n",
        "        # if isinstance(val_cer, dict) and val_cer['Transformer'] < best_val_cer:\n",
        "        #     best_val_cer = val_cer['Transformer']\n",
        "        #     if not improved:\n",
        "        #         best_epoch = epoch\n",
        "        #         improved = True\n",
        "        #         log_print(f\"New best validation CER: {val_cer['Transformer']:.4f}\")\n",
        "\n",
        "        # if isinstance(val_cer, dict) and val_cer['CTC'] < best_val_cer:\n",
        "        #     best_val_cer = val_cer['CTC']\n",
        "        #     if not improved:\n",
        "        #         best_epoch = epoch\n",
        "        #         improved = True\n",
        "        #         log_print(f\"New best validation CER: {val_cer['CTC']:.4f}\")\n",
        "        \n",
        "        # Second priority - improve loss if CER wasn't better\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            if not improved:\n",
        "                improved = True\n",
        "                log_print(f\"New best validation loss: {val_loss:.4f}\")\n",
        "            \n",
        "        # Save checkpoint every 5 epochs or when validation improves\n",
        "        if (epoch + 1) % 5 == 0 or improved:\n",
        "            # Clean up old checkpoints, keeping only the last 2\n",
        "            keep_last_n = 2\n",
        "            checkpoints = sorted([f for f in os.listdir('.') if f.startswith('checkpoint_epoch_')])\n",
        "            for old_ckpt in checkpoints[:-keep_last_n]:\n",
        "                try:\n",
        "                    os.remove(old_ckpt)\n",
        "                    log_print(f\"Removed old checkpoint: {old_ckpt}\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            checkpoint_path = f'checkpoint_epoch_{epoch+1:02d}.pth'\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'e2e_model_state_dict': e2e_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'scaler_state_dict': scaler.state_dict(),\n",
        "                'loss': val_loss,\n",
        "                'hybrid_cer': val_cer['Hybrid'] if isinstance(val_cer, dict) else None,\n",
        "                # 'transformer_cer': val_cer['Transformer'] if isinstance(val_cer, dict) else None,\n",
        "                # 'ctc_cer': val_cer['CTC'] if isinstance(val_cer, dict) else None,\n",
        "                'rng_state': get_rng_state(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'best_val_cer': best_val_cer,\n",
        "                'best_epoch': best_epoch,\n",
        "                'curriculum_stage': curriculum_stage\n",
        "            }, checkpoint_path)\n",
        "            log_print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "        \n",
        "            # Force synchronize CUDA operations and clear memory after saving\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        # Save best model if validation improves\n",
        "        if improved:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'e2e_model_state_dict': e2e_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'scaler_state_dict': scaler.state_dict(),\n",
        "                'loss': val_loss,\n",
        "                'hybrid_cer': val_cer['Hybrid'] if isinstance(val_cer, dict) else None,\n",
        "                # 'transformer_cer': val_cer['Transformer'] if isinstance(val_cer, dict) else None,\n",
        "                # 'ctc_cer': val_cer['CTC'] if isinstance(val_cer, dict) else None,\n",
        "                'rng_state': get_rng_state(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'best_val_cer': best_val_cer,\n",
        "                'best_epoch': best_epoch,\n",
        "                'curriculum_stage': curriculum_stage\n",
        "            }, 'best_model.pth')\n",
        "            log_print(f\"New best model saved at epoch {epoch+1}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            log_print(f\"No improvement for {patience_counter} epochs (patience: {max_patience})\")\n",
        "            \n",
        "            if patience_counter > 0 and patience_counter % 5 == 0:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = param_group['lr'] * 0.6\n",
        "                scheduler.base_lrs = [base_lr * 0.6 for base_lr in scheduler.base_lrs]\n",
        "                log_print(f\"Reducing learning rate to {optimizer.param_groups[0]['lr']:.7f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if patience_counter >= max_patience:\n",
        "                log_print(f\"Early stopping triggered after {patience_counter} epochs without improvement\")\n",
        "                break\n",
        "            \n",
        "        del epoch_loss, val_loss\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    log_print(\"\\nTraining completed!\")\n",
        "    log_print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "    log_print(f\"Best validation CER: {best_val_cer:.4f} (epoch {best_epoch+1})\")\n",
        "    log_print(f\"Best model saved to: best_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Launch Training\n",
        "\n",
        "This final step calls the `train_model` function to start the full training pipeline using the configured environment, datasets, and model settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "execution_failed": "2025-07-03T13:59:49.205Z",
          "iopub.execute_input": "2025-07-03T13:59:41.428164Z",
          "iopub.status.busy": "2025-07-03T13:59:41.427793Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint specified. Starting training from scratch.\n",
            "Starting training for 60 epochs\n",
            "Logs will be saved to Logs/training_20250703_134728.log\n",
            "Checkpoints will be saved every 5 epochs\n",
            "--------------------------------------------------\n",
            "Epoch 1/60 - Training (Stage 1, CTC weight: 0.700)...\n"
          ]
        }
      ],
      "source": [
        "train_model()"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31040,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
