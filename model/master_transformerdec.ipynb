{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torch, os, cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import editdistance\n",
    "from lipreading.model import Lipreading\n",
    "from lipreading.optim_utils import CosineScheduler\n",
    "# Import the Transformer decoder instead of TCN decoder\n",
    "from lipreading.transformer_decoder import ArabicTransformerDecoder\n",
    "# We need the mask utility for transformer\n",
    "from espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # 2. Initialize the seed and the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "seed = 0\n",
    "def reset_seed():\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Setting the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # 3. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 3.1. List of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ٱ': 1, 'يْ': 2, 'يّْ': 3, 'يِّ': 4, 'يُّ': 5, 'يَّ': 6, 'يٌّ': 7, 'يِ': 8, 'يُ': 9, 'يَ': 10, 'يٌ': 11, 'ي': 12, 'ى': 13, 'وْ': 14, 'وِّ': 15, 'وُّ': 16, 'وَّ': 17, 'وِ': 18, 'وُ': 19, 'وَ': 20, 'وً': 21, 'و': 22, 'هْ': 23, 'هُّ': 24, 'هِ': 25, 'هُ': 26, 'هَ': 27, 'نۢ': 28, 'نْ': 29, 'نِّ': 30, 'نُّ': 31, 'نَّ': 32, 'نِ': 33, 'نُ': 34, 'نَ': 35, 'مْ': 36, 'مّْ': 37, 'مِّ': 38, 'مُّ': 39, 'مَّ': 40, 'مِ': 41, 'مُ': 42, 'مَ': 43, 'مٍ': 44, 'مٌ': 45, 'مً': 46, 'لْ': 47, 'لّْ': 48, 'لِّ': 49, 'لُّ': 50, 'لَّ': 51, 'لِ': 52, 'لُ': 53, 'لَ': 54, 'لٍ': 55, 'لٌ': 56, 'لً': 57, 'كْ': 58, 'كِّ': 59, 'كَّ': 60, 'كِ': 61, 'كُ': 62, 'كَ': 63, 'قْ': 64, 'قَّ': 65, 'قِ': 66, 'قُ': 67, 'قَ': 68, 'قٍ': 69, 'قً': 70, 'فْ': 71, 'فِّ': 72, 'فَّ': 73, 'فِ': 74, 'فُ': 75, 'فَ': 76, 'غْ': 77, 'غِ': 78, 'غَ': 79, 'عْ': 80, 'عَّ': 81, 'عِ': 82, 'عُ': 83, 'عَ': 84, 'عٍ': 85, 'ظْ': 86, 'ظِّ': 87, 'ظَّ': 88, 'ظِ': 89, 'ظُ': 90, 'ظَ': 91, 'طْ': 92, 'طِّ': 93, 'طَّ': 94, 'طِ': 95, 'طُ': 96, 'طَ': 97, 'ضْ': 98, 'ضِّ': 99, 'ضُّ': 100, 'ضَّ': 101, 'ضِ': 102, 'ضُ': 103, 'ضَ': 104, 'ضً': 105, 'صْ': 106, 'صّْ': 107, 'صِّ': 108, 'صُّ': 109, 'صَّ': 110, 'صِ': 111, 'صُ': 112, 'صَ': 113, 'صٍ': 114, 'صً': 115, 'شْ': 116, 'شِّ': 117, 'شُّ': 118, 'شَّ': 119, 'شِ': 120, 'شُ': 121, 'شَ': 122, 'سْ': 123, 'سّْ': 124, 'سِّ': 125, 'سُّ': 126, 'سَّ': 127, 'سِ': 128, 'سُ': 129, 'سَ': 130, 'سٍ': 131, 'زْ': 132, 'زَّ': 133, 'زِ': 134, 'زُ': 135, 'زَ': 136, 'رْ': 137, 'رِّ': 138, 'رُّ': 139, 'رَّ': 140, 'رِ': 141, 'رُ': 142, 'رَ': 143, 'رٍ': 144, 'رٌ': 145, 'رً': 146, 'ذْ': 147, 'ذَّ': 148, 'ذِ': 149, 'ذُ': 150, 'ذَ': 151, 'دْ': 152, 'دِّ': 153, 'دُّ': 154, 'دَّ': 155, 'دًّ': 156, 'دِ': 157, 'دُ': 158, 'دَ': 159, 'دٍ': 160, 'دٌ': 161, 'دً': 162, 'خْ': 163, 'خِ': 164, 'خُ': 165, 'خَ': 166, 'حْ': 167, 'حَّ': 168, 'حِ': 169, 'حُ': 170, 'حَ': 171, 'جْ': 172, 'جِّ': 173, 'جُّ': 174, 'جَّ': 175, 'جِ': 176, 'جُ': 177, 'جَ': 178, 'ثْ': 179, 'ثِّ': 180, 'ثُّ': 181, 'ثَّ': 182, 'ثِ': 183, 'ثُ': 184, 'ثَ': 185, 'تْ': 186, 'تِّ': 187, 'تُّ': 188, 'تَّ': 189, 'تِ': 190, 'تُ': 191, 'تَ': 192, 'تٍ': 193, 'تٌ': 194, 'ةْ': 195, 'ةِ': 196, 'ةُ': 197, 'ةَ': 198, 'ةٍ': 199, 'ةٌ': 200, 'ةً': 201, 'بْ': 202, 'بِّ': 203, 'بَّ': 204, 'بِ': 205, 'بُ': 206, 'بَ': 207, 'بٍ': 208, 'بً': 209, 'ا': 210, 'ئْ': 211, 'ئِ': 212, 'ئَ': 213, 'ئً': 214, 'إِ': 215, 'ؤْ': 216, 'ؤُ': 217, 'ؤَ': 218, 'أْ': 219, 'أُ': 220, 'أَ': 221, 'آ': 222, 'ءْ': 223, 'ءِ': 224, 'ءَ': 225, 'ءً': 226}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def extract_label(file):\n",
    "    label = []\n",
    "    diacritics = {\n",
    "        '\\u064B',  # Fathatan\n",
    "        '\\u064C',  # Dammatan\n",
    "        '\\u064D',  # Kasratan\n",
    "        '\\u064E',  # Fatha\n",
    "        '\\u064F',  # Damma\n",
    "        '\\u0650',  # Kasra\n",
    "        '\\u0651',  # Shadda\n",
    "        '\\u0652',  # Sukun\n",
    "        '\\u06E2',  # Small High meem\n",
    "    }\n",
    "\n",
    "    sentence = pd.read_csv(file)\n",
    "    for word in sentence.word:\n",
    "        for char in word:\n",
    "            if char not in diacritics:\n",
    "                label.append(char)\n",
    "            else:\n",
    "                label[-1] += char\n",
    "\n",
    "    return label\n",
    "\n",
    "classes = set()\n",
    "for i in os.listdir('../Dataset/Csv (with Diacritics)'):\n",
    "    file = '../Dataset/Csv (with Diacritics)/' + i\n",
    "    label = extract_label(file)\n",
    "    classes.update(label)\n",
    "\n",
    "mapped_classes = {}\n",
    "for i, c in enumerate(sorted(classes, reverse=True), 1):\n",
    "    mapped_classes[c] = i\n",
    "\n",
    "print(mapped_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 3.2. Video Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Defining the video dataset class\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_paths, label_paths, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        frames = self.load_frames(video_path=video_path)\n",
    "        label = torch.tensor(list(map(lambda x: mapped_classes[x], extract_label(label_path))))\n",
    "        input_length = torch.tensor(len(frames), dtype=torch.long)\n",
    "        label_length = torch.tensor(len(label), dtype=torch.long)\n",
    "        return frames, input_length, label, label_length\n",
    "    \n",
    "    def load_frames(self, video_path):\n",
    "        frames = []\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        for i in range(total_frames):\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = video.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame_pil = Image.fromarray(frame, 'L')\n",
    "                frames.append(frame_pil)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            frames = [self.transform(frame) for frame in frames] \n",
    "        frames = torch.stack(frames).permute(1, 0, 2, 3)\n",
    "        return frames\n",
    "\n",
    "# Defining the video transform\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.421, std=0.165),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 3.2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "videos_dir = \"Dataset/Preprocessed_Video\"\n",
    "labels_dir = \"Dataset/Csv (with Diacritics)\"\n",
    "videos, labels = [], []\n",
    "file_names = [file_name[:-4] for file_name in os.listdir(videos_dir)]\n",
    "for file_name in file_names:\n",
    "    videos.append(os.path.join(videos_dir, file_name + \".mp4\"))\n",
    "    labels.append(os.path.join(labels_dir, file_name + \".csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 3.3. Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Split the dataset into training, validation, test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(videos, labels, test_size=0.1000, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 3.4. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def pad_packed_collate(batch):\n",
    "    \"\"\"Pads data and labels with different lengths in the same batch\n",
    "    \"\"\"\n",
    "    data_list, input_lengths, labels_list, label_lengths = zip(*batch)\n",
    "    c, max_len, h, w = max(data_list, key=lambda x: x.shape[1]).shape\n",
    "\n",
    "    data = torch.zeros((len(data_list), c, max_len, h, w))\n",
    "    \n",
    "    # Only copy up to the actual sequence length\n",
    "    for idx in range(len(data)):\n",
    "        data[idx, :, :input_lengths[idx], :, :] = data_list[idx][:, :input_lengths[idx], :, :]\n",
    "    \n",
    "    # Flatten labels for CTC loss\n",
    "    labels_flat = []\n",
    "    for label_seq in labels_list:\n",
    "        labels_flat.extend(label_seq)\n",
    "    labels_flat = torch.LongTensor(labels_flat)\n",
    "    \n",
    "    # Convert lengths to tensor\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    label_lengths = torch.LongTensor(label_lengths)\n",
    "    return data, input_lengths, labels_flat, label_lengths\n",
    "\n",
    "\n",
    "# Defining the video dataloaders (train, validation, test)\n",
    "train_dataset = VideoDataset(X_train, y_train, transform=transforms)\n",
    "val_dataset = VideoDataset(X_val, y_val, transform=transforms)\n",
    "test_dataset = VideoDataset(X_test, y_test, transform=transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def indices_to_text(indices, idx2char):\n",
    "    \"\"\"\n",
    "    Converts a list of indices to text using the reverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ''.join([idx2char.get(i, '') for i in indices])\n",
    "    except UnicodeEncodeError:\n",
    "        # Handle encoding issues in Windows console\n",
    "        # Return a safe representation that won't cause encoding errors\n",
    "        safe_text = []\n",
    "        for i in indices:\n",
    "            char = idx2char.get(i, '')\n",
    "            try:\n",
    "                # Test if character can be encoded\n",
    "                char.encode('cp1252')\n",
    "                safe_text.append(char)\n",
    "            except UnicodeEncodeError:\n",
    "                # Replace with a placeholder for characters that can't be displayed\n",
    "                safe_text.append(f\"[{i}]\")\n",
    "        return ''.join(safe_text)\n",
    "\n",
    "def compute_cer(reference_indices, hypothesis_indices):\n",
    "    \"\"\"\n",
    "    Computes Character Error Rate (CER) directly using token indices.\n",
    "    Takes raw token indices from our vocabulary (class_mapping.txt) rather than Unicode text.\n",
    "    \n",
    "    Returns a tuple of (CER, edit_distance)\n",
    "    \"\"\"\n",
    "    # Use the indices directly - each index is one token in our vocabulary\n",
    "    ref_tokens = reference_indices\n",
    "    hyp_tokens = hypothesis_indices\n",
    "    \n",
    "    try:\n",
    "        print(f\"Debug - Reference tokens ({len(ref_tokens)} tokens): {ref_tokens}\")\n",
    "        print(f\"Debug - Hypothesis tokens ({len(hyp_tokens)} tokens): {hyp_tokens}\")\n",
    "    except UnicodeEncodeError:\n",
    "        # Handle encoding issues in Windows console\n",
    "        print(f\"Debug - Reference tokens ({len(ref_tokens)} tokens): [Token indices omitted due to encoding issues]\")\n",
    "        print(f\"Debug - Hypothesis tokens ({len(hyp_tokens)} tokens): [Token indices omitted due to encoding issues]\")\n",
    "    \n",
    "    # Calculate edit distance using the editdistance library\n",
    "    edit_distance = editdistance.eval(ref_tokens, hyp_tokens)\n",
    "    \n",
    "    # Calculate CER\n",
    "    cer = edit_distance / max(len(ref_tokens), 1)  # Avoid division by zero\n",
    "    \n",
    "    return cer, edit_distance\n",
    "\n",
    "# Initializing the hyper-parameters\n",
    "densetcn_options = {\n",
    "    'block_config': [3, 3, 3, 3],               # Number of layers in each dense block\n",
    "    'growth_rate_set': [384, 384, 384, 384],    # Growth rate for each block (must be divisible by len(kernel_size_set))\n",
    "    'reduced_size': 512,                        # Reduced size between blocks (must be divisible by len(kernel_size_set))\n",
    "    'kernel_size_set': [3, 5, 7],               # Kernel sizes for multi-scale processing\n",
    "    'dilation_size_set': [1, 2, 5],             # Dilation rates for increasing receptive field\n",
    "    'squeeze_excitation': True,                 # Whether to use SE blocks for channel attention\n",
    "    'dropout': 0.2                              # Dropout rate\n",
    "}\n",
    "initial_lr = 3e-4\n",
    "total_epochs = 80\n",
    "scheduler = CosineScheduler(initial_lr, total_epochs)\n",
    "\n",
    "# Build reverse mapping for decoding\n",
    "idx2char = {v: k for k, v in mapped_classes.items()}\n",
    "idx2char[0] = \"\"  # Blank token for CTC\n",
    "\n",
    "# Initializing the model\n",
    "model = Lipreading(densetcn_options=densetcn_options, hidden_dim=512, num_classes=len(mapped_classes) + 1, relu_type='prelu').to(device)\n",
    "\n",
    "# Add a Transformer decoder on top of the visual encoder\n",
    "transformer_decoder = ArabicTransformerDecoder(\n",
    "    vocab_size=len(mapped_classes) + 1,  # +1 for blank token\n",
    "    attention_dim=512,  # Matching hidden_dim from the model\n",
    "    attention_heads=8,  # 8 heads for better attention to different parts of sequence\n",
    "    num_blocks=6,  # 6 transformer decoder layers\n",
    "    dropout_rate=0.1  # Dropout rate\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(transformer_decoder)\n",
    "\n",
    "# Setting up loss functions\n",
    "ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is pad token\n",
    "\n",
    "# Defining the optimizer with different learning rates for encoder and decoder\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': initial_lr},\n",
    "    {'params': transformer_decoder.parameters(), 'lr': initial_lr * 1.5}  # Higher LR for transformer\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Replace beam search with Transformer decoder inference\n",
    "def transformer_decode(log_probs, beam_size=8, maxlen=24, blank_index=0):\n",
    "    \"\"\"\n",
    "    Perform transformer-based decoding on log probabilities with additional debugging.\n",
    "    \n",
    "    Args:\n",
    "        log_probs: Log probabilities from the encoder, shape (B, T, C)\n",
    "        beam_size: Beam width for search\n",
    "        maxlen: Maximum length of the decoded sequence\n",
    "        blank_index: Index of the blank token\n",
    "        \n",
    "    Returns:\n",
    "        List of hypotheses, each with 'yseq' and 'score' keys\n",
    "    \"\"\"\n",
    "    batch_size = log_probs.size(0)\n",
    "    print(f\"Starting transformer_decode with batch size: {batch_size}\")\n",
    "    \n",
    "    # Create memory from encoder features\n",
    "    memory = log_probs\n",
    "    \n",
    "    # Create memory mask (indicates valid encoder positions)\n",
    "    memory_mask = torch.ones((batch_size, memory.size(1)), device=device)\n",
    "    \n",
    "    print(f\"Memory shape: {memory.shape}\")\n",
    "    print(f\"Memory mask shape: {memory_mask.shape}\")\n",
    "    \n",
    "    # Debug memory stats\n",
    "    print(f\"Memory stats: mean={memory.mean().item():.4f}, std={memory.std().item():.4f}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        print(f\"\\nProcessing batch item {b+1}/{batch_size} in beam search\")\n",
    "        # Get this item's memory\n",
    "        single_memory = memory[b:b+1]  # Keep batch dimension\n",
    "        single_memory_mask = memory_mask[b:b+1]  # Keep batch dimension\n",
    "        \n",
    "        try:\n",
    "            print(\"Starting beam search...\")\n",
    "            # Beam search params\n",
    "            char_list = list(idx2char.keys())  # List of valid character indices\n",
    "            \n",
    "            # Beam search initialization\n",
    "            # Initial state with start token\n",
    "            y = torch.tensor([1], dtype=torch.long, device=device).reshape(1, 1)  # Start token\n",
    "            \n",
    "            # Initialize beam with single hypothesis\n",
    "            beam = [{'score': 0.0, 'yseq': [1], 'cache': None}]\n",
    "            \n",
    "            # Set up length normalization parameters\n",
    "            length_penalty = 0.6  # Adjust this parameter for better results\n",
    "            diversity_penalty = 0.1  # Penalty for repeated tokens\n",
    "            \n",
    "            for i in range(maxlen):\n",
    "                print(f\"Beam search step {i+1}/{maxlen}\")\n",
    "                if len(beam) == 0:\n",
    "                    print(\"Empty beam, breaking\")\n",
    "                    break\n",
    "                \n",
    "                # Collect candidates from all beam hypotheses\n",
    "                new_beam = []\n",
    "                \n",
    "                for hyp in beam:\n",
    "                    # Convert yseq to tensor\n",
    "                    vy = torch.tensor(hyp['yseq'], dtype=torch.long, device=device).reshape(1, -1)\n",
    "                    \n",
    "                    # Create proper causal mask for autoregressive property\n",
    "                    vy_mask = subsequent_mask(vy.size(1)).to(device)\n",
    "                    \n",
    "                    # Forward through transformer decoder\n",
    "                    try:\n",
    "                        decoder_out = transformer_decoder(\n",
    "                            vy,                # Input token sequence\n",
    "                            vy_mask,           # Self-attention causal mask \n",
    "                            single_memory,     # Memory from encoder\n",
    "                            single_memory_mask  # Memory mask (valid positions)\n",
    "                        )\n",
    "                        \n",
    "                        # Get the last prediction (most recent token)\n",
    "                        y_logits = decoder_out[:, -1]\n",
    "                        \n",
    "                        # Convert to log probs\n",
    "                        local_scores = F.log_softmax(y_logits, dim=-1)\n",
    "                        \n",
    "                        # Add to beam for every possible next token\n",
    "                        for c in char_list:\n",
    "                            # Skip blank token\n",
    "                            if c == blank_index:\n",
    "                                continue\n",
    "                                \n",
    "                            # Apply length normalization to scores\n",
    "                            normalized_score = (hyp['score'] + local_scores[0, c].item()) / \\\n",
    "                                              ((len(hyp['yseq']) + 1) ** length_penalty)\n",
    "                            \n",
    "                            # Apply diversity penalty for repeated tokens\n",
    "                            if c in hyp['yseq']:\n",
    "                                normalized_score -= diversity_penalty\n",
    "                            \n",
    "                            # Create new hypothesis\n",
    "                            new_hyp = {\n",
    "                                'score': normalized_score,\n",
    "                                'yseq': hyp['yseq'] + [c],\n",
    "                                'cache': None\n",
    "                            }\n",
    "                            \n",
    "                            new_beam.append(new_hyp)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in decoder forward pass: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # No candidates found\n",
    "                if len(new_beam) == 0:\n",
    "                    print(\"No candidates in new beam, breaking\")\n",
    "                    break\n",
    "                \n",
    "                # Sort and keep top beam_size hypotheses\n",
    "                new_beam.sort(key=lambda x: x['score'], reverse=True)\n",
    "                beam = new_beam[:beam_size]\n",
    "                \n",
    "                # Debug beam status\n",
    "                print(f\"Top beam after step {i+1}:\")\n",
    "                for j, top_hyp in enumerate(beam[:3]):  # Just show top 3\n",
    "                    print(f\"  {j+1}: score={top_hyp['score']:.4f}, seq={top_hyp['yseq']}\")\n",
    "                \n",
    "                # Check if all beam hypotheses end with EOS\n",
    "                if all(hyp['yseq'][-1] == 2 for hyp in beam):\n",
    "                    print(\"All hypotheses end with EOS, breaking\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Beam search complete for batch item {b+1}\")\n",
    "            print(f\"Final beam size: {len(beam)}\")\n",
    "            \n",
    "            # Sort final beam\n",
    "            beam.sort(key=lambda x: x['score'], reverse=True)\n",
    "            all_results.append(beam)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during beam search for batch item {b+1}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Add an empty result for this batch\n",
    "            all_results.append([])\n",
    "    \n",
    "    # Return the best hypothesis for the first batch item (simplified)\n",
    "    if len(all_results) > 0 and len(all_results[0]) > 0:\n",
    "        return all_results[0]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def create_transformer_inputs(labels_flat, label_lengths, device):\n",
    "    \"\"\"\n",
    "    Creates input and target tensors for transformer decoder training.\n",
    "    \n",
    "    Args:\n",
    "        labels_flat: Flattened label tensor\n",
    "        label_lengths: Length of each label sequence\n",
    "        device: Device to create tensors on\n",
    "        \n",
    "    Returns:\n",
    "        decoder_input: Input tensor for transformer decoder\n",
    "        decoder_target: Target tensor for transformer decoder\n",
    "        tgt_mask: Causal attention mask for decoder\n",
    "    \"\"\"\n",
    "    # Prepare target sequences for transformer training (teacher forcing)\n",
    "    target_seqs = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for b in range(label_lengths.size(0)):\n",
    "        seq_len = label_lengths[b].item()\n",
    "        # Get this sequence\n",
    "        seq = labels_flat[start_idx:start_idx + seq_len]\n",
    "        # Add start-of-sequence token (for decoder input)\n",
    "        target_seq = torch.cat([torch.tensor([1], device=device), seq])\n",
    "        # Add end-of-sequence token\n",
    "        target_seq = torch.cat([target_seq, torch.tensor([2], device=device)])\n",
    "        # Add to list\n",
    "        target_seqs.append(target_seq)\n",
    "        # Update start index\n",
    "        start_idx += seq_len\n",
    "    \n",
    "    # Pad sequences to same length\n",
    "    max_len = max(len(seq) for seq in target_seqs)\n",
    "    padded_seqs = []\n",
    "    for seq in target_seqs:\n",
    "        padded = torch.cat([seq, torch.zeros(max_len - len(seq), device=device, dtype=torch.long)])\n",
    "        padded_seqs.append(padded)\n",
    "    \n",
    "    # Stack to tensor\n",
    "    target_tensor = torch.stack(padded_seqs)\n",
    "    \n",
    "    # Teacher forcing: decoder input is target shifted right (remove last token)\n",
    "    decoder_input = target_tensor[:, :-1]\n",
    "    # Teacher forcing: decoder target is target shifted left (remove first token)\n",
    "    decoder_target = target_tensor[:, 1:]\n",
    "    \n",
    "    # Create dynamic causal mask based on actual sequence length\n",
    "    seq_len = decoder_input.size(1)\n",
    "    batch_size = decoder_input.size(0)\n",
    "    \n",
    "    # Create causal mask that respects auto-regressive constraints\n",
    "    tgt_mask = subsequent_mask(seq_len).to(device)  # Shape [seq_len, seq_len]\n",
    "    \n",
    "    # Ensure mask is 3D for attention modules: [batch_size, seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    \n",
    "    return decoder_input, decoder_target, tgt_mask\n",
    "\n",
    "# Replace the train_one_epoch function with the transformer approach\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    transformer_decoder.train()\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    ce_criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index (0)\n",
    "    \n",
    "    for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(train_loader):\n",
    "        # Print input shape for debugging\n",
    "        print(f\"Batch {batch_idx+1} - Input shape: {inputs.shape}\")\n",
    "        \n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Fix input_lengths to reflect actual video lengths\n",
    "        # The input shape is [batch_size, channels, frames, height, width]\n",
    "        actual_input_lengths = torch.full((inputs.size(0),), inputs.size(2), dtype=torch.long, device=device)\n",
    "        print(f\"Input lengths: {input_lengths}\")\n",
    "        print(f\"Corrected input lengths: {actual_input_lengths}\")\n",
    "        \n",
    "        input_lengths = actual_input_lengths  # Use corrected lengths\n",
    "        \n",
    "        labels_flat = labels_flat.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the visual encoder\n",
    "        encoder_features = model(inputs, input_lengths)\n",
    "        \n",
    "        # Set output_lengths to match the actual encoder output length\n",
    "        output_lengths = torch.full((encoder_features.size(0),), encoder_features.size(1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Print shape to verify sequence output\n",
    "        print(f\"Batch {batch_idx+1} - Encoder features shape: {encoder_features.shape}\")\n",
    "        \n",
    "        # Apply log_softmax for CTC\n",
    "        log_probs = F.log_softmax(encoder_features, dim=2)  # (B, T, C)\n",
    "        \n",
    "        # Prepare for CTC loss - requires (T, B, C) format\n",
    "        outputs_for_ctc = log_probs.transpose(0, 1)  # from (B, T, C) to (T, B, C)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        ctc_loss_val = ctc_loss_fn(outputs_for_ctc, labels_flat, output_lengths, label_lengths)\n",
    "        \n",
    "        # Prepare target sequences for transformer training\n",
    "        # First, reconstruct the target sequences from the flattened labels\n",
    "        # Create a list of target sequences for each batch item\n",
    "        target_seqs = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            seq_len = label_lengths[b].item()\n",
    "            # Extract the sequence for this batch item\n",
    "            target_seq = labels_flat[start_idx:start_idx + seq_len]\n",
    "            # Add start-of-sequence token (1) at the beginning\n",
    "            target_seq = torch.cat([torch.tensor([1], device=device), target_seq])\n",
    "            # Add end-of-sequence token (2) at the end\n",
    "            target_seq = torch.cat([target_seq, torch.tensor([2], device=device)])\n",
    "            \n",
    "            # Add to lists\n",
    "            target_seqs.append(target_seq)\n",
    "            \n",
    "            # Update start index\n",
    "            start_idx += seq_len\n",
    "        \n",
    "        # Pad sequences to max length\n",
    "        max_len = max(len(seq) for seq in target_seqs)\n",
    "        padded_seqs = []\n",
    "        for seq in target_seqs:\n",
    "            padded = torch.cat([seq, torch.zeros(max_len - len(seq), device=device, dtype=torch.long)])\n",
    "            padded_seqs.append(padded)\n",
    "        \n",
    "        # Stack sequences\n",
    "        target_tensor = torch.stack(padded_seqs)\n",
    "        \n",
    "        # Create proper memory mask based on actual encoder output lengths\n",
    "        # This mask indicates which positions in the encoder output are valid\n",
    "        memory_mask = torch.zeros((batch_size, encoder_features.size(1)), device=device).bool()\n",
    "        for b in range(batch_size):\n",
    "            memory_mask[b, :output_lengths[b]] = True\n",
    "        \n",
    "        # For transformer training, we use the encoder features as memory\n",
    "        # and teacher-forcing with target sequences as input\n",
    "        # The input to the transformer decoder is the target sequence shifted right\n",
    "        decoder_input, decoder_target, tgt_mask = create_transformer_inputs(labels_flat, label_lengths, device)\n",
    "        print(f\"Original decoder input shape: {decoder_input.shape}\")\n",
    "        \n",
    "        # No need to truncate or pad to exactly 8 tokens anymore - use dynamic mask\n",
    "        print(f\"Final decoder input shape: {decoder_input.shape}\")\n",
    "        print(f\"Final decoder target shape: {decoder_target.shape}\")\n",
    "        print(f\"Mask shape: {tgt_mask.shape}\")\n",
    "        \n",
    "        # Create a causal mask for the target\n",
    "        print(\"Applying forward pass through transformer decoder...\")\n",
    "        \n",
    "        print(f\"Input shapes: decoder_input={decoder_input.shape}, tgt_mask={tgt_mask.shape}\")\n",
    "        print(f\"Memory shapes: encoder_features={encoder_features.shape}, memory_mask={memory_mask.shape}\")\n",
    "        \n",
    "        try:\n",
    "            # Use try-except to capture details of any error\n",
    "            # Forward pass through the transformer decoder\n",
    "            decoder_output = transformer_decoder(\n",
    "                decoder_input,  # (batch_size, seq_len)\n",
    "                tgt_mask,       # (batch_size, seq_len, seq_len)\n",
    "                encoder_features,  # (batch_size, seq_len, dim)\n",
    "                memory_mask     # (batch_size, seq_len)\n",
    "            )\n",
    "            print(f\"Decoder output shape: {decoder_output.shape}\")\n",
    "            \n",
    "            # Calculate statistics of the decoder output for debugging\n",
    "            print(f\"Decoder output stats: mean={decoder_output.float().mean():.4f}, std={decoder_output.float().std():.4f}\")\n",
    "            \n",
    "            # Calculate cross-entropy loss\n",
    "            decoder_output_flat = decoder_output.reshape(-1, decoder_output.size(-1))\n",
    "            decoder_target_flat = decoder_target.reshape(-1)\n",
    "            ce_loss = ce_criterion(decoder_output_flat, decoder_target_flat)\n",
    "            print(f\"CE Loss: {ce_loss.item():.6f}\")\n",
    "            \n",
    "            # Calculate combined loss (weighted sum of CTC and CE)\n",
    "            ctc_weight = 0.7  # Adjust this weight as needed\n",
    "            combined_loss = ctc_weight * ctc_loss_val + (1 - ctc_weight) * ce_loss\n",
    "            print(f\"Combined Loss: {combined_loss.item():.6f}\")\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            combined_loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients (important for transformers)\n",
    "            torch.nn.utils.clip_grad_norm_(list(model.parameters()) + list(transformer_decoder.parameters()), 1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += combined_loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in transformer decoder forward pass: {str(e)}\")\n",
    "            print(f\"Error type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Check specific tensor shapes in more detail\n",
    "            print(f\"decoder_input dtype: {decoder_input.dtype}, device: {decoder_input.device}\")\n",
    "            print(f\"tgt_mask dtype: {tgt_mask.dtype}, device: {tgt_mask.device}\")\n",
    "            \n",
    "            # Continue with penalty loss if error occurs\n",
    "            ce_loss = torch.tensor(5.0, device=device)  # Default penalty\n",
    "            combined_loss = ctc_weight * ctc_loss_val + (1 - ctc_weight) * ce_loss\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            combined_loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += combined_loss.item()\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(data_loader):\n",
    "    model.eval()\n",
    "    transformer_decoder.eval()\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    # Track statistics\n",
    "    total_cer = 0\n",
    "    total_edit_distance = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Process all batches in the test loader\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(data_loader):\n",
    "            # Move to device\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Fix input_lengths to reflect actual video lengths\n",
    "            actual_input_lengths = torch.full((inputs.size(0),), inputs.size(2), dtype=torch.long, device=device)\n",
    "            print(f\"Input lengths: {input_lengths}\")\n",
    "            print(f\"Corrected input lengths: {actual_input_lengths}\")\n",
    "            \n",
    "            input_lengths = actual_input_lengths  # Use corrected lengths\n",
    "            \n",
    "            labels_flat = labels_flat.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "            \n",
    "            # Forward pass through visual encoder\n",
    "            batch_size = inputs.size(0)\n",
    "            encoder_features = model(inputs, input_lengths)  # (B, T, hidden_dim)\n",
    "            \n",
    "            # Set output_lengths to match the actual encoder output length\n",
    "            output_lengths = torch.full((encoder_features.size(0),), encoder_features.size(1), dtype=torch.long, device=device)\n",
    "            \n",
    "            # Calculate CTC loss as before (for monitoring only)\n",
    "            log_probs = F.log_softmax(encoder_features, dim=2)  # (B, T, C)\n",
    "            log_probs_ctc = log_probs.transpose(0, 1)  # (T, B, C)\n",
    "            loss = ctc_loss_fn(log_probs_ctc, labels_flat, output_lengths, label_lengths)\n",
    "            \n",
    "            print(f\"\\nRunning Transformer decoding for batch {i+1}...\")\n",
    "            \n",
    "            try:\n",
    "                # Batch beam search returns a list of lists - each batch item has its own list of beams\n",
    "                print(f\"Encoder features shape: {encoder_features.shape}\")\n",
    "                print(f\"Beam size: 8, Max length: 24\")\n",
    "                \n",
    "                # Create proper memory mask based on actual encoder output lengths\n",
    "                memory_mask = torch.zeros((batch_size, encoder_features.size(1)), device=device).bool()\n",
    "                for b in range(batch_size):\n",
    "                    memory_mask[b, :output_lengths[b]] = True\n",
    "                \n",
    "                all_nbest_hyps = transformer_decoder.batch_beam_search(\n",
    "                    memory=encoder_features,\n",
    "                    memory_mask=memory_mask,\n",
    "                    beam_size=8,\n",
    "                    maxlen=24,\n",
    "                    minlen=1,  # Minimum length requirement\n",
    "                    sos=1,     # Start of sequence token\n",
    "                    eos=2      # End of sequence token\n",
    "                )\n",
    "                \n",
    "                print(f\"Transformer decoding completed for batch {i+1}\")\n",
    "                print(f\"Received {len(all_nbest_hyps)} hypotheses sets\")\n",
    "                \n",
    "                # Process each batch item\n",
    "                for b in range(batch_size):\n",
    "                    print(f\"\\nProcessing batch item {b+1}/{batch_size}\")\n",
    "                    \n",
    "                    # Get best hypothesis for this batch item\n",
    "                    if b < len(all_nbest_hyps):\n",
    "                        # With the updated batch_beam_search, results are (score, hyp) tuples\n",
    "                        score, pred_indices = all_nbest_hyps[b]\n",
    "                        print(f\"Found beam hypothesis for item {b+1} with score {score:.4f}\")\n",
    "                        \n",
    "                        # Convert to numpy array\n",
    "                        pred_indices = np.array(pred_indices)\n",
    "                        \n",
    "                        # Print warning if pred_indices is empty\n",
    "                        if len(pred_indices) == 0:\n",
    "                            print(\"WARNING: Prediction sequence is empty!\")\n",
    "                    else:\n",
    "                        # No hypotheses for this batch item - use empty prediction\n",
    "                        print(f\"No hypotheses for batch item {b+1}\")\n",
    "                        pred_indices = np.array([])\n",
    "                    \n",
    "                    # Get target indices\n",
    "                    start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                    end_idx = start_idx + label_lengths[b].item()\n",
    "                    target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                    \n",
    "                    # Convert indices to text\n",
    "                    pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                    target_text = indices_to_text(target_idx, idx2char)\n",
    "                    \n",
    "                    # Calculate CER using custom function\n",
    "                    cer, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    total_cer += cer\n",
    "                    total_edit_distance += edit_distance\n",
    "                    total_loss += loss.item() / batch_size\n",
    "                    \n",
    "                    # Print info\n",
    "                    print(\"-\" * 50)\n",
    "                    print(f\"Sample {i * batch_size + b + 1}:\")\n",
    "                    try:\n",
    "                        print(f\"Predicted text: {pred_text}\")\n",
    "                        print(f\"Target text: {target_text}\")\n",
    "                    except UnicodeEncodeError:\n",
    "                        print(\"Predicted text: [Contains characters that can't be displayed in console]\")\n",
    "                        print(\"Target text: [Contains characters that can't be displayed in console]\")\n",
    "                        print(f\"Predicted indices: {pred_indices}\")\n",
    "                        print(f\"Target indices: {target_idx}\")\n",
    "                        \n",
    "                    print(f\"Edit distance: {edit_distance}\")\n",
    "                    print(f\"CER: {cer:.4f}\")\n",
    "                    print(\"-\" * 50)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error during Transformer decoding: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                # Fall back to CTC greedy decoding\n",
    "                print(\"Falling back to CTC greedy decoding\")\n",
    "                \n",
    "                # Process batch items with greedy decoding\n",
    "                for b in range(batch_size):\n",
    "                    batch_logits = log_probs[b].cpu().numpy()\n",
    "                    pred_indices = np.argmax(batch_logits, axis=1)\n",
    "                    # Remove duplicates and blanks\n",
    "                    filtered_indices = []\n",
    "                    prev_idx = -1\n",
    "                    for idx in pred_indices:\n",
    "                        if idx != 0 and idx != prev_idx:  # Skip blanks and duplicates\n",
    "                            filtered_indices.append(idx)\n",
    "                        prev_idx = idx\n",
    "                    pred_indices = np.array(filtered_indices)\n",
    "                    \n",
    "                    # Get target indices\n",
    "                    start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                    end_idx = start_idx + label_lengths[b].item()\n",
    "                    target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                    \n",
    "                    # Convert indices to text\n",
    "                    pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                    target_text = indices_to_text(target_idx, idx2char)\n",
    "                    \n",
    "                    # Calculate CER\n",
    "                    cer, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    total_cer += cer\n",
    "                    total_edit_distance += edit_distance\n",
    "                    total_loss += loss.item() / batch_size\n",
    "                    \n",
    "                    # Print info\n",
    "                    print(\"-\" * 50)\n",
    "                    print(f\"Sample {i * batch_size + b + 1} (Greedy CTC):\")\n",
    "                    print(f\"Predicted text: {pred_text}\")\n",
    "                    print(f\"Target text: {target_text}\")\n",
    "                    print(f\"Edit distance: {edit_distance}\")\n",
    "                    print(f\"CER: {cer:.4f}\")\n",
    "                    print(\"-\" * 50)\n",
    "        \n",
    "        # Write summary statistics\n",
    "        n_samples = len(data_loader.dataset)\n",
    "        avg_cer = total_cer / n_samples\n",
    "        avg_edit_distance = total_edit_distance / n_samples\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        \n",
    "        print(\"=== Summary Statistics ===\")\n",
    "        print(f\"Total samples: {n_samples}\")\n",
    "        print(f\"Average CER: {avg_cer:.4f}\")\n",
    "        print(f\"Average Edit Distance: {avg_edit_distance:.2f}\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def test_batch(transformer_decoder, encoder_features, memory_mask, beam_size=8):\n",
    "    \"\"\"Test a batch with beam search using the transformer decoder\"\"\"\n",
    "    batch_size = encoder_features.size(0)\n",
    "    \n",
    "    print(f\"Testing batch with beam search (batch_size={batch_size})\")\n",
    "    \n",
    "    # Run batch beam search\n",
    "    try:\n",
    "        results = transformer_decoder.batch_beam_search(\n",
    "            memory=encoder_features,\n",
    "            memory_mask=memory_mask,\n",
    "            beam_size=beam_size,\n",
    "            maxlen=24,\n",
    "            minlen=1,\n",
    "            sos=1,  # Start of sequence token\n",
    "            eos=2   # End of sequence token\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        for b, (score, hyp) in enumerate(results):\n",
    "            print(f\"Batch item {b+1}:\")\n",
    "            print(f\"  Score: {score:.4f}\")\n",
    "            print(f\"  Hypothesis: {hyp}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during beam search: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "def train_model():\n",
    "    \"\"\"Train the model on the full dataset\"\"\"\n",
    "    for epoch in range(total_epochs):\n",
    "        # Train for one epoch\n",
    "        epoch_loss = train_one_epoch()\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        scheduler.adjust_lr(optimizer, epoch)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss = evaluate_model(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def quick_experiment(model, transformer_decoder, full_dataset, num_epochs=5, num_samples=50):\n",
    "    \"\"\"Run a quick experiment with a small subset of the data.\n",
    "    \n",
    "    Args:\n",
    "        model: The visual encoder model\n",
    "        transformer_decoder: The transformer decoder model\n",
    "        full_dataset: The complete training dataset\n",
    "        num_epochs: Number of epochs to train (default: 5)\n",
    "        num_samples: Number of samples to use (default: 50)\n",
    "    \"\"\"\n",
    "    print(f\"Running quick experiment with {num_samples} samples for {num_epochs} epochs\")\n",
    "    \n",
    "    try:\n",
    "        # Create small dataset for quick testing\n",
    "        indices = torch.randperm(len(full_dataset))[:num_samples]\n",
    "        small_dataset = torch.utils.data.Subset(full_dataset, indices)\n",
    "        \n",
    "        # Create dataloader with the small dataset\n",
    "        small_loader = DataLoader(small_dataset, batch_size=8, shuffle=True, \n",
    "                                pin_memory=True, collate_fn=pad_packed_collate)\n",
    "        \n",
    "        # Initialize loss functions\n",
    "        ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "        ce_loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n===== Epoch {epoch+1}/{num_epochs} Training =====\")\n",
    "            \n",
    "            # Initialize tracking variables\n",
    "            running_loss = 0.0\n",
    "            batch_count = 0\n",
    "            \n",
    "            # Set models to training mode\n",
    "            model.train()\n",
    "            transformer_decoder.train()\n",
    "            \n",
    "            # Process batches\n",
    "            for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(small_loader):\n",
    "                try:\n",
    "                    print(f\"Training batch {batch_idx+1} of {len(small_loader)}\")\n",
    "                    \n",
    "                    # Print shapes for debugging\n",
    "                    print(f\"Input shape: {inputs.shape}\")\n",
    "                    print(f\"Input lengths: {input_lengths}\")\n",
    "                    print(f\"Label flat shape: {labels_flat.shape}\")\n",
    "                    print(f\"Label lengths: {label_lengths}\")\n",
    "                    \n",
    "                    # Move data to device\n",
    "                    inputs = inputs.to(device)\n",
    "                    input_lengths = input_lengths.to(device)\n",
    "                    labels_flat = labels_flat.to(device)\n",
    "                    label_lengths = label_lengths.to(device)\n",
    "                    \n",
    "                    # Create actual input lengths tensor based on encoder output size\n",
    "                    actual_input_lengths = torch.full((inputs.size(0),), inputs.size(2), \n",
    "                                                    dtype=torch.long, device=device)\n",
    "                    print(f\"Corrected input lengths: {actual_input_lengths}\")\n",
    "                    \n",
    "                    # Zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass through visual encoder\n",
    "                    encoder_features = model(inputs, actual_input_lengths)\n",
    "                    print(f\"Encoder features shape: {encoder_features.shape}\")\n",
    "                    print(f\"Encoder features stats: mean={encoder_features.mean():.4f}, std={encoder_features.std():.4f}\")\n",
    "                    \n",
    "                    # Calculate CTC loss\n",
    "                    log_probs = F.log_softmax(encoder_features, dim=2)\n",
    "                    outputs_for_ctc = log_probs.transpose(0, 1)\n",
    "                    ctc_loss_val = ctc_loss(outputs_for_ctc, labels_flat, actual_input_lengths, label_lengths)\n",
    "                    print(f\"CTC Loss: {ctc_loss_val.item():.6f}\")\n",
    "                    \n",
    "                    # Prepare target sequences for transformer training\n",
    "                    target_seqs = []\n",
    "                    start_idx = 0\n",
    "                    batch_size = inputs.size(0)\n",
    "                    \n",
    "                    for b in range(batch_size):\n",
    "                        seq_len = label_lengths[b].item()\n",
    "                        target_seq = labels_flat[start_idx:start_idx + seq_len]\n",
    "                        # Add SOS (1) and EOS (2) tokens\n",
    "                        target_seq = torch.cat([torch.tensor([1], device=device), \n",
    "                                             target_seq, \n",
    "                                             torch.tensor([2], device=device)])\n",
    "                        target_seqs.append(target_seq)\n",
    "                        start_idx += seq_len\n",
    "                    \n",
    "                    # Pad sequences to same length\n",
    "                    max_len = max(len(seq) for seq in target_seqs)\n",
    "                    padded_seqs = []\n",
    "                    for seq in target_seqs:\n",
    "                        padded = torch.cat([seq, torch.zeros(max_len - len(seq), \n",
    "                                                           device=device, dtype=torch.long)])\n",
    "                        padded_seqs.append(padded)\n",
    "                    \n",
    "                    # Stack sequences\n",
    "                    target_tensor = torch.stack(padded_seqs)\n",
    "                    \n",
    "                    # Prepare decoder input/output\n",
    "                    decoder_input = target_tensor[:, :-1]  # Remove last token\n",
    "                    decoder_output = target_tensor[:, 1:]  # Remove first token\n",
    "                    \n",
    "                    print(f\"Original decoder input shape: {decoder_input.shape}\")\n",
    "                    \n",
    "                    # Create masks\n",
    "                    tgt_mask = subsequent_mask(decoder_input.size(1)).to(device)\n",
    "                    tgt_mask = tgt_mask.expand(batch_size, -1, -1)\n",
    "                    \n",
    "                    # Create memory mask based on actual encoder output lengths\n",
    "                    memory_mask = torch.ones((batch_size, encoder_features.size(1)), device=device)\n",
    "                    \n",
    "                    print(f\"Final decoder input shape: {decoder_input.shape}\")\n",
    "                    print(f\"Final decoder target shape: {decoder_output.shape}\")\n",
    "                    print(f\"Mask shape: {tgt_mask.shape}\")\n",
    "                    \n",
    "                    # Forward through transformer decoder\n",
    "                    print(\"Applying forward pass through transformer decoder...\")\n",
    "                    print(f\"Input shapes: decoder_input={decoder_input.shape}, tgt_mask={tgt_mask.shape}\")\n",
    "                    print(f\"Memory shapes: encoder_features={encoder_features.shape}, memory_mask={memory_mask.shape}\")\n",
    "                    \n",
    "                    decoder_out = transformer_decoder(\n",
    "                        decoder_input, tgt_mask, encoder_features, memory_mask\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"Decoder output shape: {decoder_out.shape}\")\n",
    "                    print(f\"Decoder output stats: mean={decoder_out.mean():.4f}, std={decoder_out.std():.4f}\")\n",
    "                    \n",
    "                    # Calculate transformer loss\n",
    "                    decoder_out_flat = decoder_out.reshape(-1, decoder_out.size(-1))\n",
    "                    decoder_output_flat = decoder_output.reshape(-1)\n",
    "                    transformer_loss = ce_loss(decoder_out_flat, decoder_output_flat)\n",
    "                    print(f\"CE Loss: {transformer_loss.item():.6f}\")\n",
    "                    \n",
    "                    # Combined loss\n",
    "                    alpha = 0.7  # Weight for CTC loss\n",
    "                    loss = alpha * ctc_loss_val + (1 - alpha) * transformer_loss\n",
    "                    print(f\"Combined Loss: {loss.item():.6f}\")\n",
    "                    \n",
    "                    # Backprop\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {batch_idx+1}: {str(e)}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            # Print epoch statistics\n",
    "            if batch_count > 0:\n",
    "                epoch_loss = running_loss / batch_count\n",
    "                print(f\"\\nEpoch {epoch+1}/{num_epochs} - Average Loss: {epoch_loss:.6f}\")\n",
    "            else:\n",
    "                print(f\"\\nEpoch {epoch+1}/{num_epochs} - No valid batches processed\")\n",
    "            \n",
    "            # Evaluate on a small validation set\n",
    "            print(\"\\nRunning evaluation...\")\n",
    "            model.eval()\n",
    "            transformer_decoder.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(small_loader):\n",
    "                    try:\n",
    "                        # Move to device\n",
    "                        inputs = inputs.to(device)\n",
    "                        input_lengths = input_lengths.to(device)\n",
    "                        labels_flat = labels_flat.to(device)\n",
    "                        label_lengths = label_lengths.to(device)\n",
    "                        \n",
    "                        # Forward pass through encoder\n",
    "                        encoder_features = model(inputs, input_lengths)\n",
    "                        \n",
    "                        # Beam search decoding\n",
    "                        results = transformer_decoder.batch_beam_search(\n",
    "                            encoder_features,\n",
    "                            memory_mask=None,  # Let the decoder handle mask creation\n",
    "                            beam_size=5,\n",
    "                            maxlen=50,\n",
    "                            sos=1,\n",
    "                            eos=2\n",
    "                        )\n",
    "                        \n",
    "                        # Process and print results\n",
    "                        for b, (score, hyp) in enumerate(results):\n",
    "                            # Get target indices for comparison\n",
    "                            start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                            end_idx = start_idx + label_lengths[b].item()\n",
    "                            target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                            \n",
    "                            # Convert to text\n",
    "                            pred_text = indices_to_text(hyp, idx2char)\n",
    "                            target_text = indices_to_text(target_idx, idx2char)\n",
    "                            \n",
    "                            print(f\"\\nSample {b+1}:\")\n",
    "                            print(f\"  Predicted: {pred_text}\")\n",
    "                            print(f\"  Target: {target_text}\")\n",
    "                            print(f\"  Score: {score:.4f}\")\n",
    "                        \n",
    "                        # Only process first batch during evaluation\n",
    "                        break\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in evaluation: {str(e)}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                        continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Update the function call to include the number of samples parameter\n",
    "reset_seed()\n",
    "# Uncomment one of the following lines to run the full training or quick experiment\n",
    "# train_model()\n",
    "quick_experiment(model, transformer_decoder, train_dataset, num_samples=50)  # Run the quick experiment with 50 samples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
