{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torch, os, cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import editdistance\n",
    "from lipreading.encoder_models import Lipreading\n",
    "from lipreading.optim_utils import CosineScheduler\n",
    "from lipreading.transformer_decoder import ArabicTransformerDecoder\n",
    "from espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\n",
    "from our_utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 2. Initialize the seed and the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "seed = 0\n",
    "def reset_seed():\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Setting the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 3. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 3.1. List of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def extract_label(file):\n",
    "    label = []\n",
    "    diacritics = {\n",
    "        '\\u064B',  # Fathatan\n",
    "        '\\u064C',  # Dammatan\n",
    "        '\\u064D',  # Kasratan\n",
    "        '\\u064E',  # Fatha\n",
    "        '\\u064F',  # Damma\n",
    "        '\\u0650',  # Kasra\n",
    "        '\\u0651',  # Shadda\n",
    "        '\\u0652',  # Sukun\n",
    "        '\\u06E2',  # Small High meem\n",
    "    }\n",
    "\n",
    "    sentence = pd.read_csv(file)\n",
    "    for word in sentence.word:\n",
    "        for char in word:\n",
    "            if char not in diacritics:\n",
    "                label.append(char)\n",
    "            else:\n",
    "                label[-1] += char\n",
    "\n",
    "    return label\n",
    "\n",
    "classes = set()\n",
    "for i in os.listdir('Dataset/Csv (with Diacritics)'):\n",
    "    file = 'Dataset/Csv (with Diacritics)/' + i\n",
    "    label = extract_label(file)\n",
    "    classes.update(label)\n",
    "\n",
    "mapped_classes = {}\n",
    "for i, c in enumerate(sorted(classes, reverse=True), 1):\n",
    "    mapped_classes[c] = i\n",
    "\n",
    "print(mapped_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 3.2. Video Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Defining the video dataset class\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_paths, label_paths, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        frames = self.load_frames(video_path=video_path)\n",
    "        label = torch.tensor(list(map(lambda x: mapped_classes[x], extract_label(label_path))))\n",
    "        input_length = torch.tensor(len(frames), dtype=torch.long)\n",
    "        label_length = torch.tensor(len(label), dtype=torch.long)\n",
    "        return frames, input_length, label, label_length\n",
    "    \n",
    "    def load_frames(self, video_path):\n",
    "        frames = []\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        for i in range(total_frames):\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = video.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame_pil = Image.fromarray(frame, 'L')\n",
    "                frames.append(frame_pil)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            frames = [self.transform(frame) for frame in frames] \n",
    "        frames = torch.stack(frames).permute(1, 0, 2, 3)\n",
    "        return frames\n",
    "\n",
    "# Defining the video transform\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.421, std=0.165),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 3.2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "videos_dir = \"Dataset/Preprocessed_Video\"\n",
    "labels_dir = \"Dataset/Csv (with Diacritics)\"\n",
    "videos, labels = [], []\n",
    "file_names = [file_name[:-4] for file_name in os.listdir(videos_dir)]\n",
    "for file_name in file_names:\n",
    "    videos.append(os.path.join(videos_dir, file_name + \".mp4\"))\n",
    "    labels.append(os.path.join(labels_dir, file_name + \".csv\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 3.3. Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Split the dataset into training, validation, test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(videos, labels, test_size=0.1000, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 3.4. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Defining the video dataloaders (train, validation, test)\n",
    "train_dataset = VideoDataset(X_train, y_train, transform=transforms)\n",
    "val_dataset = VideoDataset(X_val, y_val, transform=transforms)\n",
    "test_dataset = VideoDataset(X_test, y_test, transform=transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 4. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Build vocabulary setup\n",
    "base_vocab_size = len(mapped_classes) + 1  # +1 for blank token\n",
    "full_vocab_size = base_vocab_size + 2  # +2 for SOS and EOS tokens\n",
    "\n",
    "# Build reverse mapping for decoding\n",
    "idx2char = {v: k for k, v in mapped_classes.items()}\n",
    "idx2char[0] = \"\"  # Blank token for CTC\n",
    "idx2char[base_vocab_size] = \"<sos>\"  # SOS token\n",
    "idx2char[base_vocab_size + 1] = \"<eos>\"  # EOS token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 4.1 Temporal Encoder Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# DenseTCN configuration (our default backbone)\n",
    "densetcn_options = {\n",
    "    'block_config': [3, 3, 3, 3],               # Number of layers in each dense block\n",
    "    'growth_rate_set': [384, 384, 384, 384],    # Growth rate for each block (must be divisible by len(kernel_size_set))\n",
    "    'reduced_size': 512,                        # Reduced size between blocks (must be divisible by len(kernel_size_set))\n",
    "    'kernel_size_set': [3, 5, 7],               # Kernel sizes for multi-scale processing\n",
    "    'dilation_size_set': [1, 2, 5],             # Dilation rates for increasing receptive field\n",
    "    'squeeze_excitation': True,                 # Whether to use SE blocks for channel attention\n",
    "    'dropout': 0.2                              # Dropout rate\n",
    "}\n",
    "\n",
    "# MSTCN configuration\n",
    "mstcn_options = {\n",
    "    'tcn_type': 'multiscale',\n",
    "    'hidden_dim': 512,\n",
    "    'num_channels': [171, 171, 171, 171],  # 4 layers with 171 channels each (divisible by 3)\n",
    "    'kernel_size': [3, 5, 7],              # 3 kernels for multi-scale processing\n",
    "    'dropout': 0.2,\n",
    "    'stride': 1,\n",
    "    'width_mult': 1.0,\n",
    "}\n",
    "\n",
    "# Conformer configuration\n",
    "conformer_options = {\n",
    "    'attention_dim': 512,            # Same as hidden_dim for consistency\n",
    "    'attention_heads': 8,            # Number of attention heads\n",
    "    'linear_units': 2048,           # Size of position-wise feed-forward\n",
    "    'num_blocks': 6,                # Number of conformer blocks\n",
    "    'dropout_rate': 0.1,            # General dropout rate\n",
    "    'positional_dropout_rate': 0.1,  # Dropout rate for positional encoding\n",
    "    'attention_dropout_rate': 0.0,   # Dropout rate for attention\n",
    "    'cnn_module_kernel': 31         # Kernel size for convolution module\n",
    "}\n",
    "\n",
    "# Choose temporal encoder type: 'densetcn', 'mstcn', or 'conformer'\n",
    "TEMPORAL_ENCODER = 'conformer'  # Change this to select different encoder\n",
    "\n",
    "# Initialize model based on selected encoder\n",
    "if TEMPORAL_ENCODER == 'densetcn':\n",
    "    model = Lipreading(\n",
    "        densetcn_options=densetcn_options,\n",
    "        hidden_dim=512,\n",
    "        num_classes=base_vocab_size,\n",
    "        relu_type='prelu'\n",
    "    ).to(device)\n",
    "elif TEMPORAL_ENCODER == 'mstcn':\n",
    "    model = Lipreading(\n",
    "        tcn_options=mstcn_options,\n",
    "        hidden_dim=mstcn_options['hidden_dim'],\n",
    "        num_classes=base_vocab_size,\n",
    "        relu_type='prelu'\n",
    "    ).to(device)\n",
    "elif TEMPORAL_ENCODER == 'conformer':\n",
    "    model = Lipreading(\n",
    "        conformer_options=conformer_options,\n",
    "        hidden_dim=conformer_options['attention_dim'],\n",
    "        num_classes=base_vocab_size,\n",
    "        relu_type='swish'\n",
    "    ).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown temporal encoder type: {TEMPORAL_ENCODER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 4.2 Decoder and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Add transformer decoder\n",
    "transformer_decoder = ArabicTransformerDecoder(\n",
    "    vocab_size=base_vocab_size,  # Base vocab size (model will add SOS/EOS internally)\n",
    "    attention_dim=512,          # Matching hidden_dim from the model\n",
    "    attention_heads=8,          # 8 heads for better attention to different parts of sequence\n",
    "    num_blocks=6,              # 6 transformer decoder layers\n",
    "    dropout_rate=0.1           # Dropout rate\n",
    ").to(device)\n",
    "\n",
    "# Training parameters\n",
    "initial_lr = 3e-4\n",
    "total_epochs = 80\n",
    "scheduler = CosineScheduler(initial_lr, total_epochs)\n",
    "\n",
    "# Loss functions\n",
    "ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is pad token\n",
    "\n",
    "# Optimizer with different learning rates for encoder and decoder\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': initial_lr},\n",
    "    {'params': transformer_decoder.parameters(), 'lr': initial_lr * 1.5}  # Higher LR for transformer\n",
    "])\n",
    "\n",
    "print(\"Selected temporal encoder:\", TEMPORAL_ENCODER)\n",
    "print(model)\n",
    "print(transformer_decoder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def create_transformer_inputs(labels_flat, label_lengths, device):\n",
    "    \"\"\"\n",
    "    Creates input and target tensors for transformer decoder training.\n",
    "    \n",
    "    Args:\n",
    "        labels_flat: Flattened label tensor\n",
    "        label_lengths: Length of each label sequence\n",
    "        device: Device to create tensors on\n",
    "        \n",
    "    Returns:\n",
    "        decoder_input: Input tensor for transformer decoder\n",
    "        decoder_target: Target tensor for transformer decoder\n",
    "        tgt_mask: Causal attention mask for decoder\n",
    "    \"\"\"\n",
    "    # Prepare target sequences for transformer training (teacher forcing)\n",
    "    target_seqs = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for b in range(label_lengths.size(0)):\n",
    "        seq_len = label_lengths[b].item()\n",
    "        # Get this sequence\n",
    "        seq = labels_flat[start_idx:start_idx + seq_len]\n",
    "        # Add start-of-sequence token (for decoder input)\n",
    "        target_seq = torch.cat([torch.tensor([1], device=device), seq])\n",
    "        # Add end-of-sequence token\n",
    "        target_seq = torch.cat([target_seq, torch.tensor([2], device=device)])\n",
    "        # Add to list\n",
    "        target_seqs.append(target_seq)\n",
    "        # Update start index\n",
    "        start_idx += seq_len\n",
    "    \n",
    "    # Pad sequences to same length\n",
    "    max_len = max(len(seq) for seq in target_seqs)\n",
    "    padded_seqs = []\n",
    "    for seq in target_seqs:\n",
    "        padded = torch.cat([seq, torch.zeros(max_len - len(seq), device=device, dtype=torch.long)])\n",
    "        padded_seqs.append(padded)\n",
    "    \n",
    "    # Stack to tensor\n",
    "    target_tensor = torch.stack(padded_seqs)\n",
    "    \n",
    "    # Teacher forcing: decoder input is target shifted right (remove last token)\n",
    "    decoder_input = target_tensor[:, :-1]\n",
    "    # Teacher forcing: decoder target is target shifted left (remove first token)\n",
    "    decoder_target = target_tensor[:, 1:]\n",
    "    \n",
    "    # Create dynamic causal mask based on actual sequence length\n",
    "    seq_len = decoder_input.size(1)\n",
    "    batch_size = decoder_input.size(0)\n",
    "    \n",
    "    # Create causal mask that respects auto-regressive constraints\n",
    "    tgt_mask = subsequent_mask(seq_len).to(device)  # Shape [seq_len, seq_len]\n",
    "    \n",
    "    # Ensure mask is 3D for attention modules: [batch_size, seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    \n",
    "    return decoder_input, decoder_target, tgt_mask\n",
    "\n",
    "# Replace the train_one_epoch function with the transformer approach\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    transformer_decoder.train()\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    ce_criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index (0)\n",
    "    \n",
    "    for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(train_loader):\n",
    "        # Print input shape for debugging\n",
    "        print(f\"Batch {batch_idx+1} - Input shape: {inputs.shape}\")\n",
    "        \n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Fix input_lengths to reflect actual video lengths\n",
    "        # The input shape is [batch_size, channels, frames, height, width]\n",
    "        actual_input_lengths = torch.full((inputs.size(0),), inputs.size(2), dtype=torch.long, device=device)\n",
    "        print(f\"Input lengths: {input_lengths}\")\n",
    "        print(f\"Corrected input lengths: {actual_input_lengths}\")\n",
    "        \n",
    "        input_lengths = actual_input_lengths  # Use corrected lengths\n",
    "        \n",
    "        labels_flat = labels_flat.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the visual encoder\n",
    "        encoder_features = model(inputs, input_lengths)\n",
    "        \n",
    "        # Set output_lengths to match the actual encoder output length\n",
    "        output_lengths = torch.full((encoder_features.size(0),), encoder_features.size(1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Print shape to verify sequence output\n",
    "        print(f\"Batch {batch_idx+1} - Encoder features shape: {encoder_features.shape}\")\n",
    "        \n",
    "        # Apply log_softmax for CTC\n",
    "        log_probs = F.log_softmax(encoder_features, dim=2)  # (B, T, C)\n",
    "        \n",
    "        # Prepare for CTC loss - requires (T, B, C) format\n",
    "        outputs_for_ctc = log_probs.transpose(0, 1)  # from (B, T, C) to (T, B, C)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        ctc_loss_val = ctc_loss_fn(outputs_for_ctc, labels_flat, output_lengths, label_lengths)\n",
    "        \n",
    "        # Prepare target sequences for transformer training\n",
    "        # First, reconstruct the target sequences from the flattened labels\n",
    "        # Create a list of target sequences for each batch item\n",
    "        target_seqs = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            seq_len = label_lengths[b].item()\n",
    "            # Extract the sequence for this batch item\n",
    "            target_seq = labels_flat[start_idx:start_idx + seq_len]\n",
    "            # Add start-of-sequence token (1) at the beginning\n",
    "            target_seq = torch.cat([torch.tensor([1], device=device), target_seq])\n",
    "            # Add end-of-sequence token (2) at the end\n",
    "            target_seq = torch.cat([target_seq, torch.tensor([2], device=device)])\n",
    "            \n",
    "            # Add to lists\n",
    "            target_seqs.append(target_seq)\n",
    "            \n",
    "            # Update start index\n",
    "            start_idx += seq_len\n",
    "        \n",
    "        # Pad sequences to max length\n",
    "        max_len = max(len(seq) for seq in target_seqs)\n",
    "        padded_seqs = []\n",
    "        for seq in target_seqs:\n",
    "            padded = torch.cat([seq, torch.zeros(max_len - len(seq), device=device, dtype=torch.long)])\n",
    "            padded_seqs.append(padded)\n",
    "        \n",
    "        # Stack sequences\n",
    "        target_tensor = torch.stack(padded_seqs)\n",
    "        \n",
    "        # Create proper memory mask based on actual encoder output lengths\n",
    "        # This mask indicates which positions in the encoder output are valid\n",
    "        memory_mask = torch.zeros((batch_size, encoder_features.size(1)), device=device).bool()\n",
    "        for b in range(batch_size):\n",
    "            memory_mask[b, :output_lengths[b]] = True\n",
    "        \n",
    "        # For transformer training, we use the encoder features as memory\n",
    "        # and teacher-forcing with target sequences as input\n",
    "        # The input to the transformer decoder is the target sequence shifted right\n",
    "        decoder_input, decoder_target, tgt_mask = create_transformer_inputs(labels_flat, label_lengths, device)\n",
    "        print(f\"Original decoder input shape: {decoder_input.shape}\")\n",
    "        \n",
    "        # No need to truncate or pad to exactly 8 tokens anymore - use dynamic mask\n",
    "        print(f\"Final decoder input shape: {decoder_input.shape}\")\n",
    "        print(f\"Final decoder target shape: {decoder_target.shape}\")\n",
    "        print(f\"Mask shape: {tgt_mask.shape}\")\n",
    "        \n",
    "        # Create a causal mask for the target\n",
    "        print(\"Applying forward pass through transformer decoder...\")\n",
    "        \n",
    "        print(f\"Input shapes: decoder_input={decoder_input.shape}, tgt_mask={tgt_mask.shape}\")\n",
    "        print(f\"Memory shapes: encoder_features={encoder_features.shape}, memory_mask={memory_mask.shape}\")\n",
    "        \n",
    "        try:\n",
    "            # Use try-except to capture details of any error\n",
    "            # Forward pass through the transformer decoder\n",
    "            decoder_output = transformer_decoder(\n",
    "                decoder_input,  # (batch_size, seq_len)\n",
    "                tgt_mask,       # (batch_size, seq_len, seq_len)\n",
    "                encoder_features,  # (batch_size, seq_len, dim)\n",
    "                memory_mask     # (batch_size, seq_len)\n",
    "            )\n",
    "            print(f\"Decoder output shape: {decoder_output.shape}\")\n",
    "            \n",
    "            # Calculate statistics of the decoder output for debugging\n",
    "            print(f\"Decoder output stats: mean={decoder_output.float().mean():.4f}, std={decoder_output.float().std():.4f}\")\n",
    "            \n",
    "            # Calculate cross-entropy loss\n",
    "            decoder_output_flat = decoder_output.reshape(-1, decoder_output.size(-1))\n",
    "            decoder_target_flat = decoder_target.reshape(-1)\n",
    "            ce_loss = ce_criterion(decoder_output_flat, decoder_target_flat)\n",
    "            print(f\"CE Loss: {ce_loss.item():.6f}\")\n",
    "            \n",
    "            # Calculate combined loss (weighted sum of CTC and CE)\n",
    "            ctc_weight = 0.7  # Adjust this weight as needed\n",
    "            combined_loss = ctc_weight * ctc_loss_val + (1 - ctc_weight) * ce_loss\n",
    "            print(f\"Combined Loss: {combined_loss.item():.6f}\")\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            combined_loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients (important for transformers)\n",
    "            torch.nn.utils.clip_grad_norm_(list(model.parameters()) + list(transformer_decoder.parameters()), 1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += combined_loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in transformer decoder forward pass: {str(e)}\")\n",
    "            print(f\"Error type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Check specific tensor shapes in more detail\n",
    "            print(f\"decoder_input dtype: {decoder_input.dtype}, device: {decoder_input.device}\")\n",
    "            print(f\"tgt_mask dtype: {tgt_mask.dtype}, device: {tgt_mask.device}\")\n",
    "            \n",
    "            # Continue with penalty loss if error occurs\n",
    "            ce_loss = torch.tensor(5.0, device=device)  # Default penalty\n",
    "            combined_loss = ctc_weight * ctc_loss_val + (1 - ctc_weight) * ce_loss\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            combined_loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += combined_loss.item()\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(data_loader, ctc_weight=0.3):\n",
    "    model.eval()\n",
    "    transformer_decoder.eval()\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    # Track statistics\n",
    "    total_cer = 0\n",
    "    total_edit_distance = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Process all batches in the test loader\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(data_loader):\n",
    "            # Move to device\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Fix input_lengths to reflect actual video lengths\n",
    "            actual_input_lengths = torch.full((inputs.size(0),), inputs.size(2), dtype=torch.long, device=device)\n",
    "            print(f\"Input lengths: {input_lengths}\")\n",
    "            print(f\"Corrected input lengths: {actual_input_lengths}\")\n",
    "            \n",
    "            input_lengths = actual_input_lengths  # Use corrected lengths\n",
    "            \n",
    "            labels_flat = labels_flat.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "            \n",
    "            # Forward pass through visual encoder\n",
    "            batch_size = inputs.size(0)\n",
    "            encoder_features = model(inputs, input_lengths)  # (B, T, hidden_dim)\n",
    "            \n",
    "            # Set output_lengths to match the actual encoder output length\n",
    "            output_lengths = torch.full((encoder_features.size(0),), encoder_features.size(1), dtype=torch.long, device=device)\n",
    "            \n",
    "            # Calculate CTC probabilities\n",
    "            log_probs = F.log_softmax(encoder_features, dim=2)  # (B, T, C)\n",
    "            log_probs_ctc = log_probs.transpose(0, 1)  # (T, B, C)\n",
    "            ctc_loss = ctc_loss_fn(log_probs_ctc, labels_flat, output_lengths, label_lengths)\n",
    "            \n",
    "            print(f\"\\nRunning hybrid CTC/Attention decoding for batch {i+1}...\")\n",
    "            \n",
    "            try:\n",
    "                print(f\"Encoder features shape: {encoder_features.shape}\")\n",
    "                print(f\"Beam size: 8, Max length: 24\")\n",
    "                \n",
    "                # Create proper memory mask based on actual encoder output lengths\n",
    "                memory_mask = torch.zeros((batch_size, encoder_features.size(1)), device=device).bool()\n",
    "                for b in range(batch_size):\n",
    "                    memory_mask[b, :output_lengths[b]] = True\n",
    "                \n",
    "                # Run beam search with CTC weight\n",
    "                all_nbest_hyps = transformer_decoder.batch_beam_search(\n",
    "                    memory=encoder_features,\n",
    "                    memory_mask=memory_mask,\n",
    "                    beam_size=8,\n",
    "                    maxlen=24,\n",
    "                    minlen=1,\n",
    "                    sos=1,\n",
    "                    eos=2,\n",
    "                    ctc_weight=ctc_weight  # Pass CTC weight to beam search\n",
    "                )\n",
    "                \n",
    "                print(f\"Hybrid decoding completed for batch {i+1}\")\n",
    "                print(f\"Received {len(all_nbest_hyps)} hypotheses sets\")\n",
    "                \n",
    "                # Process each batch item\n",
    "                for b in range(batch_size):\n",
    "                    print(f\"\\nProcessing batch item {b+1}/{batch_size}\")\n",
    "                    \n",
    "                    if b < len(all_nbest_hyps):\n",
    "                        score, pred_indices = all_nbest_hyps[b]\n",
    "                        print(f\"Found beam hypothesis for item {b+1} with score {score:.4f}\")\n",
    "                        pred_indices = np.array(pred_indices)\n",
    "                        \n",
    "                        if len(pred_indices) == 0:\n",
    "                            print(\"WARNING: Prediction sequence is empty!\")\n",
    "                    else:\n",
    "                        print(f\"No hypotheses for batch item {b+1}\")\n",
    "                        pred_indices = np.array([])\n",
    "                    \n",
    "                    # Get target indices\n",
    "                    start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                    end_idx = start_idx + label_lengths[b].item()\n",
    "                    target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                    \n",
    "                    # Convert indices to text\n",
    "                    pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                    target_text = indices_to_text(target_idx, idx2char)\n",
    "                    \n",
    "                    # Calculate CER\n",
    "                    cer, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    total_cer += cer\n",
    "                    total_edit_distance += edit_distance\n",
    "                    total_loss += ctc_loss.item() / batch_size\n",
    "                    \n",
    "                    # Print info\n",
    "                    print(\"-\" * 50)\n",
    "                    print(f\"Sample {i * batch_size + b + 1}:\")\n",
    "                    try:\n",
    "                        print(f\"Predicted text: {pred_text}\")\n",
    "                        print(f\"Target text: {target_text}\")\n",
    "                    except UnicodeEncodeError:\n",
    "                        print(\"Predicted text: [Contains characters that can't be displayed in console]\")\n",
    "                        print(\"Target text: [Contains characters that can't be displayed in console]\")\n",
    "                        print(f\"Predicted indices: {pred_indices}\")\n",
    "                        print(f\"Target indices: {target_idx}\")\n",
    "                        \n",
    "                    print(f\"Edit distance: {edit_distance}\")\n",
    "                    print(f\"CER: {cer:.4f}\")\n",
    "                    print(\"-\" * 50)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error during hybrid decoding: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Write summary statistics\n",
    "        n_samples = len(data_loader.dataset)\n",
    "        avg_cer = total_cer / n_samples\n",
    "        avg_edit_distance = total_edit_distance / n_samples\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        \n",
    "        print(\"=== Summary Statistics ===\")\n",
    "        print(f\"Total samples: {n_samples}\")\n",
    "        print(f\"Average CER: {avg_cer:.4f}\")\n",
    "        print(f\"Average Edit Distance: {avg_edit_distance:.2f}\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"CTC Weight used: {ctc_weight}\")\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def test_batch(transformer_decoder, encoder_features, memory_mask, beam_size=8):\n",
    "    \"\"\"Test a batch with beam search using the transformer decoder\"\"\"\n",
    "    batch_size = encoder_features.size(0)\n",
    "    \n",
    "    print(f\"Testing batch with beam search (batch_size={batch_size})\")\n",
    "    \n",
    "    # Run batch beam search\n",
    "    try:\n",
    "        results = transformer_decoder.batch_beam_search(\n",
    "            memory=encoder_features,\n",
    "            memory_mask=memory_mask,\n",
    "            beam_size=beam_size,\n",
    "            maxlen=24,\n",
    "            minlen=1,\n",
    "            sos=1,  # Start of sequence token\n",
    "            eos=2   # End of sequence token\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        for b, (score, hyp) in enumerate(results):\n",
    "            print(f\"Batch item {b+1}:\")\n",
    "            print(f\"  Score: {score:.4f}\")\n",
    "            print(f\"  Hypothesis: {hyp}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during beam search: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "def train_model(ctc_weight=0.3):\n",
    "    \"\"\"Train the model on the full dataset with hybrid CTC/Attention\"\"\"\n",
    "    for epoch in range(total_epochs):\n",
    "        # Train for one epoch\n",
    "        epoch_loss = train_one_epoch()\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        scheduler.adjust_lr(optimizer, epoch)\n",
    "        \n",
    "        # Evaluate on validation set with CTC weight\n",
    "        val_loss = evaluate_model(val_loader, ctc_weight=ctc_weight)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"CTC Weight: {ctc_weight}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def quick_experiment(model, transformer_decoder, full_dataset, num_epochs=5, num_samples=50, ctc_weight=0.3):\n",
    "    \"\"\"Run a quick experiment with a small subset of the data.\n",
    "    \n",
    "    Args:\n",
    "        model: The visual encoder model\n",
    "        transformer_decoder: The transformer decoder model\n",
    "        full_dataset: The complete training dataset\n",
    "        num_epochs: Number of epochs to train (default: 5)\n",
    "        num_samples: Number of samples to use (default: 50)\n",
    "        ctc_weight: Weight for CTC scoring (default: 0.3)\n",
    "    \"\"\"\n",
    "    print(f\"Running quick experiment with {num_samples} samples for {num_epochs} epochs\")\n",
    "    print(f\"Using CTC weight: {ctc_weight}\")\n",
    "    \n",
    "    try:\n",
    "        # Create small dataset for quick testing\n",
    "        indices = torch.randperm(len(full_dataset))[:num_samples]\n",
    "        small_dataset = torch.utils.data.Subset(full_dataset, indices)\n",
    "        \n",
    "        # Create dataloader with the small dataset\n",
    "        small_loader = DataLoader(small_dataset, batch_size=8, shuffle=True, \n",
    "                                pin_memory=True, collate_fn=pad_packed_collate)\n",
    "        \n",
    "        # Initialize loss functions\n",
    "        ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "        ce_loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n===== Epoch {epoch+1}/{num_epochs} Training =====\")\n",
    "            \n",
    "            # Initialize tracking variables\n",
    "            running_loss = 0.0\n",
    "            batch_count = 0\n",
    "            \n",
    "            # Set models to training mode\n",
    "            model.train()\n",
    "            transformer_decoder.train()\n",
    "            \n",
    "            # Process batches\n",
    "            for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(small_loader):\n",
    "                try:\n",
    "                    print(f\"Training batch {batch_idx+1} of {len(small_loader)}\")\n",
    "                    \n",
    "                    # Print shapes for debugging\n",
    "                    print(f\"Input shape: {inputs.shape}\")\n",
    "                    print(f\"Input lengths: {input_lengths}\")\n",
    "                    print(f\"Label flat shape: {labels_flat.shape}\")\n",
    "                    print(f\"Label lengths: {label_lengths}\")\n",
    "                    \n",
    "                    # Move data to device\n",
    "                    inputs = inputs.to(device)\n",
    "                    input_lengths = input_lengths.to(device)\n",
    "                    labels_flat = labels_flat.to(device)\n",
    "                    label_lengths = label_lengths.to(device)\n",
    "                    \n",
    "                    # Create actual input lengths tensor based on encoder output size\n",
    "                    actual_input_lengths = torch.full((inputs.size(0),), inputs.size(2), \n",
    "                                                    dtype=torch.long, device=device)\n",
    "                    print(f\"Corrected input lengths: {actual_input_lengths}\")\n",
    "                    \n",
    "                    # Zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass through visual encoder\n",
    "                    encoder_features = model(inputs, actual_input_lengths)\n",
    "                    print(f\"Encoder features shape: {encoder_features.shape}\")\n",
    "                    print(f\"Encoder features stats: mean={encoder_features.mean():.4f}, std={encoder_features.std():.4f}\")\n",
    "                    \n",
    "                    # Calculate CTC loss\n",
    "                    log_probs = F.log_softmax(encoder_features, dim=2)\n",
    "                    outputs_for_ctc = log_probs.transpose(0, 1)\n",
    "                    ctc_loss_val = ctc_loss(outputs_for_ctc, labels_flat, actual_input_lengths, label_lengths)\n",
    "                    print(f\"CTC Loss: {ctc_loss_val.item():.6f}\")\n",
    "                    \n",
    "                    # Prepare target sequences for transformer training\n",
    "                    target_seqs = []\n",
    "                    start_idx = 0\n",
    "                    batch_size = inputs.size(0)\n",
    "                    \n",
    "                    for b in range(batch_size):\n",
    "                        seq_len = label_lengths[b].item()\n",
    "                        target_seq = labels_flat[start_idx:start_idx + seq_len]\n",
    "                        # Add SOS (1) and EOS (2) tokens\n",
    "                        target_seq = torch.cat([torch.tensor([1], device=device), \n",
    "                                             target_seq, \n",
    "                                             torch.tensor([2], device=device)])\n",
    "                        target_seqs.append(target_seq)\n",
    "                        start_idx += seq_len\n",
    "                    \n",
    "                    # Pad sequences to same length\n",
    "                    max_len = max(len(seq) for seq in target_seqs)\n",
    "                    padded_seqs = []\n",
    "                    for seq in target_seqs:\n",
    "                        padded = torch.cat([seq, torch.zeros(max_len - len(seq), \n",
    "                                                           device=device, dtype=torch.long)])\n",
    "                        padded_seqs.append(padded)\n",
    "                    \n",
    "                    # Stack sequences\n",
    "                    target_tensor = torch.stack(padded_seqs)\n",
    "                    \n",
    "                    # Prepare decoder input/output\n",
    "                    decoder_input = target_tensor[:, :-1]  # Remove last token\n",
    "                    decoder_output = target_tensor[:, 1:]  # Remove first token\n",
    "                    \n",
    "                    print(f\"Original decoder input shape: {decoder_input.shape}\")\n",
    "                    \n",
    "                    # Create masks\n",
    "                    tgt_mask = subsequent_mask(decoder_input.size(1)).to(device)\n",
    "                    tgt_mask = tgt_mask.expand(batch_size, -1, -1)\n",
    "                    \n",
    "                    # Create memory mask based on actual encoder output lengths\n",
    "                    memory_mask = torch.ones((batch_size, encoder_features.size(1)), device=device)\n",
    "                    \n",
    "                    print(f\"Final decoder input shape: {decoder_input.shape}\")\n",
    "                    print(f\"Final decoder target shape: {decoder_output.shape}\")\n",
    "                    print(f\"Mask shape: {tgt_mask.shape}\")\n",
    "                    \n",
    "                    # Forward through transformer decoder\n",
    "                    print(\"Applying forward pass through transformer decoder...\")\n",
    "                    print(f\"Input shapes: decoder_input={decoder_input.shape}, tgt_mask={tgt_mask.shape}\")\n",
    "                    print(f\"Memory shapes: encoder_features={encoder_features.shape}, memory_mask={memory_mask.shape}\")\n",
    "                    \n",
    "                    decoder_out = transformer_decoder(\n",
    "                        decoder_input, tgt_mask, encoder_features, memory_mask\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"Decoder output shape: {decoder_out.shape}\")\n",
    "                    print(f\"Decoder output stats: mean={decoder_out.mean():.4f}, std={decoder_out.std():.4f}\")\n",
    "                    \n",
    "                    # Calculate transformer loss\n",
    "                    decoder_out_flat = decoder_out.reshape(-1, decoder_out.size(-1))\n",
    "                    decoder_output_flat = decoder_output.reshape(-1)\n",
    "                    transformer_loss = ce_loss(decoder_out_flat, decoder_output_flat)\n",
    "                    print(f\"CE Loss: {transformer_loss.item():.6f}\")\n",
    "                    \n",
    "                    # Combined loss with CTC weight\n",
    "                    loss = ctc_weight * ctc_loss_val + (1 - ctc_weight) * transformer_loss\n",
    "                    print(f\"Combined Loss: {loss.item():.6f}\")\n",
    "                    \n",
    "                    # Backprop\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {batch_idx+1}: {str(e)}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            # Print epoch statistics\n",
    "            if batch_count > 0:\n",
    "                epoch_loss = running_loss / batch_count\n",
    "                print(f\"\\nEpoch {epoch+1}/{num_epochs} - Average Loss: {epoch_loss:.6f}\")\n",
    "            else:\n",
    "                print(f\"\\nEpoch {epoch+1}/{num_epochs} - No valid batches processed\")\n",
    "            \n",
    "            # Evaluate on a small validation set\n",
    "            print(\"\\nRunning evaluation...\")\n",
    "            model.eval()\n",
    "            transformer_decoder.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(small_loader):\n",
    "                    try:\n",
    "                        # Move to device\n",
    "                        inputs = inputs.to(device)\n",
    "                        input_lengths = input_lengths.to(device)\n",
    "                        labels_flat = labels_flat.to(device)\n",
    "                        label_lengths = label_lengths.to(device)\n",
    "                        \n",
    "                        # Forward pass through encoder\n",
    "                        encoder_features = model(inputs, input_lengths)\n",
    "                        \n",
    "                        # Create memory mask\n",
    "                        memory_mask = torch.ones((inputs.size(0), encoder_features.size(1)), device=device)\n",
    "                        \n",
    "                        # Beam search with CTC weight\n",
    "                        results = transformer_decoder.batch_beam_search(\n",
    "                            encoder_features,\n",
    "                            memory_mask=memory_mask,\n",
    "                            beam_size=5,\n",
    "                            maxlen=50,\n",
    "                            sos=1,\n",
    "                            eos=2,\n",
    "                            ctc_weight=ctc_weight\n",
    "                        )\n",
    "                        \n",
    "                        # Process and print results\n",
    "                        for b, (score, hyp) in enumerate(results):\n",
    "                            # Get target indices for comparison\n",
    "                            start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                            end_idx = start_idx + label_lengths[b].item()\n",
    "                            target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                            \n",
    "                            # Convert to text\n",
    "                            pred_text = indices_to_text(hyp, idx2char)\n",
    "                            target_text = indices_to_text(target_idx, idx2char)\n",
    "                            \n",
    "                            print(f\"\\nSample {b+1}:\")\n",
    "                            print(f\"  Predicted: {pred_text}\")\n",
    "                            print(f\"  Target: {target_text}\")\n",
    "                            print(f\"  Score: {score:.4f}\")\n",
    "                        \n",
    "                        # Only process first batch during evaluation\n",
    "                        break\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in evaluation: {str(e)}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                        continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Update the function call to include CTC weight\n",
    "reset_seed()\n",
    "# Uncomment one of the following lines to run the full training or quick experiment\n",
    "# train_model(ctc_weight=0.3)  # Run full training with CTC weight\n",
    "quick_experiment(model, transformer_decoder, train_dataset, num_samples=50, ctc_weight=0.3)  # Run quick experiment with CTC weight\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
