{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, cv2, gc\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import editdistance\n",
    "from lipreading.pretrained_frontend.encoder_models_pretrained import Lipreading\n",
    "from lipreading.optim_utils import CosineScheduler\n",
    "from lipreading.transformer_decoder import ArabicTransformerDecoder\n",
    "from espnet.transformer.mask import subsequent_mask\n",
    "from utils import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "os.makedirs('../Logs', exist_ok=True)\n",
    "log_filename = f'../Logs/training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2. Initialize the seed and the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed for reproducibility\n",
    "seed = 0\n",
    "def reset_seed():\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Setting the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.1. List of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(file):\n",
    "    label = []\n",
    "    diacritics = {\n",
    "        '\\u064B',  # Fathatan\n",
    "        '\\u064C',  # Dammatan\n",
    "        '\\u064D',  # Kasratan\n",
    "        '\\u064E',  # Fatha\n",
    "        '\\u064F',  # Damma\n",
    "        '\\u0650',  # Kasra\n",
    "        '\\u0651',  # Shadda\n",
    "        '\\u0652',  # Sukun\n",
    "        '\\u06E2',  # Small High meem\n",
    "    }\n",
    "\n",
    "    sentence = pd.read_csv(file)\n",
    "    for word in sentence.word:\n",
    "        for char in word:\n",
    "            if char not in diacritics:\n",
    "                label.append(char)\n",
    "            else:\n",
    "                label[-1] += char\n",
    "\n",
    "    return label\n",
    "\n",
    "classes = set()\n",
    "for i in os.listdir('../Dataset/Csv (with Diacritics)'):\n",
    "    file = '../Dataset/Csv (with Diacritics)/' + i\n",
    "    label = extract_label(file)\n",
    "    classes.update(label)\n",
    "\n",
    "mapped_classes = {}\n",
    "for i, c in enumerate(sorted(classes, reverse=True), 1):\n",
    "    mapped_classes[c] = i\n",
    "\n",
    "print(mapped_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.2. Video Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the video dataset class\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_paths, label_paths, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        frames = self.load_frames(video_path=video_path)\n",
    "        label = torch.tensor(list(map(lambda x: mapped_classes[x], extract_label(label_path))))\n",
    "        input_length = torch.tensor(frames.size(1), dtype=torch.long)\n",
    "        label_length = torch.tensor(len(label), dtype=torch.long)\n",
    "        return frames, input_length, label, label_length\n",
    "    \n",
    "    def load_frames(self, video_path):\n",
    "        frames = []\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        for i in range(total_frames):\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = video.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame_pil = Image.fromarray(frame, 'L')\n",
    "                frames.append(frame_pil)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            frames = [self.transform(frame) for frame in frames] \n",
    "        frames = torch.stack(frames).permute(1, 0, 2, 3)\n",
    "        return frames\n",
    "\n",
    "# Defining data augmentation transforms for train, validation, and test\n",
    "data_transforms = transforms.Compose([\n",
    "    # transforms.CenterCrop(88),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.419232189655303955078125, std=0.133925855159759521484375),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.3. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_dir = \"../Dataset/Preprocessed_Video\"\n",
    "labels_dir = \"../Dataset/Csv (with Diacritics)\"\n",
    "videos, labels = [], []\n",
    "file_names = [file_name[:-4] for file_name in os.listdir(videos_dir)]\n",
    "for file_name in file_names:\n",
    "    videos.append(os.path.join(videos_dir, file_name + \".mp4\"))\n",
    "    labels.append(os.path.join(labels_dir, file_name + \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.4. Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(videos, labels, test_size=0.1000, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.5. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the video dataloaders (train, validation, test)\n",
    "train_dataset = VideoDataset(X_train, y_train, transform=data_transforms)\n",
    "val_dataset = VideoDataset(X_val, y_val, transform=data_transforms)\n",
    "test_dataset = VideoDataset(X_test, y_test, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary setup\n",
    "base_vocab_size = len(mapped_classes) + 1  # +1 for blank token (0)\n",
    "sos_token_idx = base_vocab_size  # This places SOS after all normal tokens\n",
    "eos_token_idx = base_vocab_size + 1  # This places EOS after SOS\n",
    "full_vocab_size = base_vocab_size + 2  # +2 for SOS and EOS tokens\n",
    "\n",
    "# Build reverse mapping for decoding\n",
    "idx2char = {v: k for k, v in mapped_classes.items()}\n",
    "idx2char[0] = \"\"  # Blank token for CTC\n",
    "idx2char[sos_token_idx] = \"<sos>\"  # SOS token\n",
    "idx2char[eos_token_idx] = \"<eos>\"  # EOS token\n",
    "print(f\"Total vocabulary size: {full_vocab_size}\")\n",
    "print(f\"SOS token index: {sos_token_idx}\")\n",
    "print(f\"EOS token index: {eos_token_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.1 Temporal Encoder Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseTCN configuration (our default backbone)\n",
    "densetcn_options = {\n",
    "    'block_config': [3, 3, 3, 3],               # Number of layers in each dense block\n",
    "    'growth_rate_set': [384, 384, 384, 384],    # Growth rate for each block (must be divisible by len(kernel_size_set))\n",
    "    'reduced_size': 512,                        # Reduced size between blocks (must be divisible by len(kernel_size_set))\n",
    "    'kernel_size_set': [3, 5, 7],               # Kernel sizes for multi-scale processing\n",
    "    'dilation_size_set': [1, 2, 5],             # Dilation rates for increasing receptive field\n",
    "    'squeeze_excitation': True,                 # Whether to use SE blocks for channel attention\n",
    "    'dropout': 0.1                              # Dropout rate\n",
    "}\n",
    "\n",
    "# MSTCN configuration\n",
    "mstcn_options = {\n",
    "    'tcn_type': 'multiscale',\n",
    "    'hidden_dim': 512,\n",
    "    'num_channels': [171, 171, 171, 171],  # 4 layers with 171 channels each (divisible by 3)\n",
    "    'kernel_size': [3, 5, 7],              # 3 kernels for multi-scale processing\n",
    "    'dropout': 0.1,\n",
    "    'stride': 1,\n",
    "    'width_mult': 1.0,\n",
    "}\n",
    "\n",
    "# Conformer configuration\n",
    "conformer_options = {\n",
    "    'attention_dim': 512,            # Same as hidden_dim for consistency\n",
    "    'attention_heads': 8,            # Number of attention heads\n",
    "    'linear_units': 2048,           # Size of position-wise feed-forward\n",
    "    'num_blocks': 6,                # Number of conformer blocks\n",
    "    'dropout_rate': 0.1,            # General dropout rate\n",
    "    'positional_dropout_rate': 0.1,  # Dropout rate for positional encoding\n",
    "    'attention_dropout_rate': 0.0,   # Dropout rate for attention\n",
    "    'cnn_module_kernel': 31         # Kernel size for convolution module\n",
    "}\n",
    "\n",
    "# Choose temporal encoder type: 'densetcn', 'mstcn', or 'conformer'\n",
    "TEMPORAL_ENCODER = 'conformer'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.2 Model Initialization and Pretrained Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the model first\n",
    "print(f\"Initializing model with {TEMPORAL_ENCODER} temporal encoder...\")\n",
    "logging.info(f\"Initializing model with {TEMPORAL_ENCODER} temporal encoder\")\n",
    "\n",
    "if TEMPORAL_ENCODER == 'densetcn':\n",
    "    model = Lipreading(\n",
    "        densetcn_options=densetcn_options,\n",
    "        hidden_dim=512,\n",
    "        num_classes=base_vocab_size,\n",
    "        relu_type='swish'\n",
    "    ).to(device)\n",
    "elif TEMPORAL_ENCODER == 'mstcn':\n",
    "    model = Lipreading(\n",
    "        tcn_options=mstcn_options,\n",
    "        hidden_dim=mstcn_options['hidden_dim'],\n",
    "        num_classes=base_vocab_size,\n",
    "        relu_type='swish'\n",
    "    ).to(device)\n",
    "elif TEMPORAL_ENCODER == 'conformer':\n",
    "    model = Lipreading(\n",
    "        conformer_options=conformer_options,\n",
    "        hidden_dim=conformer_options['attention_dim'],\n",
    "        num_classes=base_vocab_size,\n",
    "        relu_type='swish'\n",
    "    ).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown temporal encoder type: {TEMPORAL_ENCODER}\")\n",
    "\n",
    "print(\"Model initialized successfully.\")\n",
    "\n",
    "# Step 2: Load pretrained frontend weights\n",
    "print(\"\\nStep 4.2: Loading pretrained frontend weights...\")\n",
    "logging.info(\"Loading pretrained frontend weights\")\n",
    "\n",
    "pretrained_path = 'lipreading/pretrained_frontend/frontend.pth'\n",
    "pretrained_weights = torch.load(pretrained_path, map_location=device)\n",
    "print(f\"Loaded pretrained weights from {pretrained_path}\")\n",
    "\n",
    "# Load weights into frontend\n",
    "model.visual_frontend.load_state_dict(pretrained_weights['state_dict'], strict=False)\n",
    "print(\"Successfully loaded pretrained weights\")\n",
    "\n",
    "# Freeze frontend parameters\n",
    "for param in model.visual_frontend.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Frontend frozen - parameters will not be updated during training\")\n",
    "logging.info(\"Successfully loaded and froze pretrained frontend\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.3 Decoder and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer decoder\n",
    "print(\"\\nStep 4.3: Initializing transformer decoder and training components...\")\n",
    "transformer_decoder = ArabicTransformerDecoder(\n",
    "    vocab_size=full_vocab_size,  # Use full vocab size that includes SOS/EOS\n",
    "    attention_dim=512,          # Matching hidden_dim from the model\n",
    "    attention_heads=8,          # 8 heads for better attention to different parts of sequence\n",
    "    num_blocks=6,              # 6 transformer decoder layers\n",
    "    dropout_rate=0.1           \n",
    ").to(device)\n",
    "\n",
    "# Training parameters\n",
    "initial_lr = 3e-4\n",
    "total_epochs = 80\n",
    "scheduler = CosineScheduler(initial_lr, total_epochs)\n",
    "\n",
    "# Loss functions\n",
    "ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is pad token\n",
    "\n",
    "# Optimizer with different learning rates for encoder and decoder\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': initial_lr},\n",
    "    {'params': transformer_decoder.parameters(), 'lr': initial_lr * 1.5}  # Higher LR for transformer\n",
    "])\n",
    "\n",
    "print(\"Selected temporal encoder:\", TEMPORAL_ENCODER)\n",
    "print(model)\n",
    "print(transformer_decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rng_state():\n",
    "    state = {}\n",
    "    try:\n",
    "        state['torch'] = torch.get_rng_state()\n",
    "        state['numpy'] = np.random.get_state()\n",
    "        if torch.cuda.is_available():\n",
    "            state['cuda'] = torch.cuda.get_rng_state()\n",
    "        else:\n",
    "            state['cuda'] = None\n",
    "        \n",
    "        # Validate RNG state types\n",
    "        if not isinstance(state['torch'], torch.Tensor):\n",
    "            print(\"Warning: torch RNG state is not a tensor, creating a valid state\")\n",
    "            state['torch'] = torch.random.get_rng_state()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error capturing RNG state: {str(e)}. Using default state.\")\n",
    "        logging.warning(f\"Error capturing RNG state: {str(e)}. Using default state.\")\n",
    "        # Create minimal valid state\n",
    "        state = {\n",
    "            'torch': torch.random.get_rng_state(),\n",
    "            'numpy': np.random.get_state(),\n",
    "            'cuda': torch.cuda.get_rng_state() if torch.cuda.is_available() else None\n",
    "        }\n",
    "    return state\n",
    "\n",
    "def set_rng_state(state):\n",
    "    try:\n",
    "        if 'torch' in state and isinstance(state['torch'], torch.Tensor):\n",
    "            torch.set_rng_state(state['torch'])\n",
    "        if 'numpy' in state and state['numpy'] is not None:\n",
    "            np.random.set_state(state['numpy'])\n",
    "        if torch.cuda.is_available() and 'cuda' in state and state['cuda'] is not None:\n",
    "            if isinstance(state['cuda'], torch.Tensor):\n",
    "                torch.cuda.set_rng_state(state['cuda'])\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to set RNG state: {str(e)}\")\n",
    "        logging.warning(f\"Failed to set RNG state: {str(e)}\")\n",
    "        print(\"Continuing with current RNG state\")\n",
    "        logging.info(\"Continuing with current RNG state\")\n",
    "\n",
    "def create_transformer_inputs(labels_flat, label_lengths, device):\n",
    "    target_seqs = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for b in range(label_lengths.size(0)):\n",
    "        seq_len = label_lengths[b].item()\n",
    "        seq = labels_flat[start_idx:start_idx + seq_len]\n",
    "        target_seq = torch.cat([\n",
    "            torch.tensor([sos_token_idx], device=device),\n",
    "            seq,\n",
    "            torch.tensor([eos_token_idx], device=device)\n",
    "        ])\n",
    "        target_seqs.append(target_seq)\n",
    "        start_idx += seq_len\n",
    "    \n",
    "    # Pad sequences to same length\n",
    "    max_len = max(len(seq) for seq in target_seqs)\n",
    "    padded_seqs = []\n",
    "    for seq in target_seqs:\n",
    "        padded = torch.cat([seq, torch.zeros(max_len - len(seq), device=device, dtype=torch.long)])\n",
    "        padded_seqs.append(padded)\n",
    "    \n",
    "    target_tensor = torch.stack(padded_seqs)\n",
    "    \n",
    "    # Teacher forcing with probability 0.5\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        # Teacher forcing: decoder input is target shifted right (remove last token)\n",
    "        decoder_input = target_tensor[:, :-1]\n",
    "    else:\n",
    "        # No teacher forcing: decoder input is just the start token\n",
    "        decoder_input = torch.full((target_tensor.size(0), 1), sos_token_idx, device=device)\n",
    "    \n",
    "    # Teacher forcing: decoder target is target shifted left (remove first token)\n",
    "    decoder_target = target_tensor[:, 1:]\n",
    "    \n",
    "    # Create dynamic causal mask based on actual sequence length\n",
    "    seq_len = decoder_input.size(1)\n",
    "    batch_size = decoder_input.size(0)\n",
    "    \n",
    "    # Create causal mask that respects auto-regressive constraints\n",
    "    tgt_mask = subsequent_mask(seq_len).to(device)  # Shape [seq_len, seq_len]\n",
    "    \n",
    "    # Ensure mask is 3D for attention modules: [batch_size, seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    \n",
    "    return decoder_input, decoder_target, tgt_mask\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    transformer_decoder.train()\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    ce_criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index (0)\n",
    "    \n",
    "    for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(train_loader):\n",
    "        # Print input shape for debugging\n",
    "        logging.info(f\"Batch {batch_idx+1} - Input shape: {inputs.shape}\")\n",
    "        \n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        labels_flat = labels_flat.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        encoder_features = model(inputs, input_lengths)\n",
    "        \n",
    "        # Set output_lengths to match the actual encoder output length\n",
    "        output_lengths = torch.full((encoder_features.size(0),), encoder_features.size(1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Print shape to verify sequence output\n",
    "        logging.info(f\"Batch {batch_idx+1} - Encoder features shape: {encoder_features.shape}\")\n",
    "        \n",
    "        # Apply log_softmax for CTC\n",
    "        log_probs = F.log_softmax(encoder_features, dim=2)  # (B, T, C)\n",
    "        \n",
    "        # Prepare for CTC loss - requires (T, B, C) format\n",
    "        outputs_for_ctc = log_probs.transpose(0, 1)  # from (B, T, C) to (T, B, C)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        ctc_loss_val = ctc_loss_fn(outputs_for_ctc, labels_flat, output_lengths, label_lengths)\n",
    "        \n",
    "        # Create proper memory mask based on actual encoder output lengths\n",
    "        # This mask indicates which positions in the encoder output are valid\n",
    "        batch_size = inputs.size(0)\n",
    "        memory_mask = torch.zeros((batch_size, encoder_features.size(1)), device=device).bool()\n",
    "        for b in range(batch_size):\n",
    "            memory_mask[b, :output_lengths[b]] = True\n",
    "        \n",
    "        # For transformer training, we use the encoder features as memory\n",
    "        # and teacher-forcing with probabilistic decision\n",
    "        decoder_input, decoder_target, tgt_mask = create_transformer_inputs(labels_flat, label_lengths, device)\n",
    "        logging.info(f\"decoder input shape: {decoder_input.shape}\")\n",
    "        logging.info(f\"Final decoder target shape: {decoder_target.shape}\")\n",
    "        logging.info(f\"Mask shape: {tgt_mask.shape}\")\n",
    "        logging.info(\"Applying forward pass through transformer decoder...\")\n",
    "        logging.info(f\"Input shapes: decoder_input={decoder_input.shape}, tgt_mask={tgt_mask.shape}\")\n",
    "        logging.info(f\"Memory shapes: encoder_features={encoder_features.shape}, memory_mask={memory_mask.shape}\")\n",
    "        \n",
    "        try:\n",
    "            # First token prediction\n",
    "            decoder_output = transformer_decoder(decoder_input, tgt_mask, encoder_features, memory_mask)\n",
    "            logging.info(\"\\n=== Debug: First Token Prediction ===\")\n",
    "            logging.info(f\"Initial decoder_output shape: {decoder_output.shape}\")\n",
    "            \n",
    "            first_pred = decoder_output[:, -1, :]  # Extract prediction for last position [batch_size, vocab_size]\n",
    "            logging.info(f\"first_pred shape: {first_pred.shape}\")\n",
    "            \n",
    "            all_predictions = []\n",
    "            all_targets = []\n",
    "            \n",
    "            # Add first prediction\n",
    "            all_predictions.append(first_pred)\n",
    "            \n",
    "            # Maximum prediction length (stop at EOS or this limit)\n",
    "            max_pred_len = min(24, decoder_target.size(1)-1)  # -1 because we've handled the first token already\n",
    "            logging.info(f\"Maximum prediction length: {max_pred_len}\")\n",
    "            \n",
    "            # Generate rest of sequence token by token (autoregressive)\n",
    "            logging.info(\"\\n=== Debug: Starting Autoregressive Generation ===\")\n",
    "            for t in range(max_pred_len):\n",
    "                # Add current target to targets list (shifted by 1 since we want to predict next token)\n",
    "                if t < decoder_target.size(1)-1:  # -1 to account for the shift\n",
    "                    all_targets.append(decoder_target[:, t+1])\n",
    "                \n",
    "                # Teacher forcing with 50% probability\n",
    "                use_teacher_forcing = torch.rand(1).item() < 0.5\n",
    "                \n",
    "                if use_teacher_forcing and t < decoder_target.size(1)-1:\n",
    "                    logging.info(f\"Using ground truth token as next input for position {t}\")\n",
    "                    # Use ground truth token as next input\n",
    "                    next_token = decoder_target[:, t].unsqueeze(1)\n",
    "                else:\n",
    "                    logging.info(f\"Using most recent prediction for position {t}\")\n",
    "                    # Use the most recent prediction\n",
    "                    next_token = decoder_output[:, -1].argmax(dim=-1).unsqueeze(1)\n",
    "                \n",
    "                # Concatenate with previous input\n",
    "                decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "                \n",
    "                # Update mask for longer sequence\n",
    "                tgt_mask = subsequent_mask(decoder_input.size(1)).to(device)\n",
    "                tgt_mask = tgt_mask.expand(batch_size, -1, -1)\n",
    "                \n",
    "                # Predict next token\n",
    "                decoder_output = transformer_decoder(decoder_input, tgt_mask, encoder_features, memory_mask)\n",
    "                \n",
    "                # Extract prediction for last position [batch_size, vocab_size]\n",
    "                if t < max_pred_len-1:  # Only add prediction if we're not at the last step\n",
    "                    current_pred = decoder_output[:, -1, :]\n",
    "                    all_predictions.append(current_pred)\n",
    "            \n",
    "            logging.info(\"\\n=== Debug: Final Stacking and Loss Computation ===\")\n",
    "            # Stack all predictions and targets\n",
    "            stacked_preds = torch.stack(all_predictions, dim=1)  # [batch_size, seq_len, vocab_size]\n",
    "            stacked_targets = torch.stack(all_targets, dim=1)    # [batch_size, seq_len]\n",
    "            \n",
    "            logging.info(f\"Final stacked_preds shape: {stacked_preds.shape}\")\n",
    "            logging.info(f\"Final stacked_targets shape: {stacked_targets.shape}\")\n",
    "            \n",
    "            # Compute loss on the entire sequence\n",
    "            decoder_output_flat = stacked_preds.reshape(-1, stacked_preds.size(-1))\n",
    "            target_flat = stacked_targets.reshape(-1)\n",
    "            \n",
    "            # Calculate cross-entropy loss\n",
    "            ce_loss = ce_criterion(decoder_output_flat, target_flat)\n",
    "            logging.info(f\"Cross Entropy Loss: {ce_loss.item():.6f}\")\n",
    "            \n",
    "            ctc_weight = 0.2\n",
    "            combined_loss = ctc_weight * ctc_loss_val + (1 - ctc_weight) * ce_loss\n",
    "            logging.info(f\"Final Combined Loss: {combined_loss.item():.6f}\")\n",
    "            \n",
    "            combined_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += combined_loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                logging.info(f\"Batch {batch_idx}, Loss: {combined_loss.item():.4f}\")\n",
    "\n",
    "            # Clean up large tensors to free memory\n",
    "            del decoder_output, decoder_output_flat, target_flat\n",
    "            del encoder_features, log_probs, outputs_for_ctc\n",
    "            del decoder_input, decoder_target, tgt_mask, memory_mask\n",
    "            del stacked_preds, stacked_targets, all_predictions, all_targets\n",
    "            \n",
    "            if batch_idx % 3 == 0:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                logging.info(f\"Memory cleared. Current GPU memory: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in transformer decoder forward pass: {str(e)}\")\n",
    "            logging.error(f\"Error type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            traceback_str = traceback.format_exc()\n",
    "            logging.error(traceback_str)\n",
    "            \n",
    "            # Also print to console\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            print(traceback_str)\n",
    "            \n",
    "            # Check specific tensor shapes in more detail\n",
    "            logging.error(f\"decoder_input dtype: {decoder_input.dtype}, device: {decoder_input.device}\")\n",
    "            logging.error(f\"tgt_mask dtype: {tgt_mask.dtype}, device: {tgt_mask.device}\")\n",
    "            \n",
    "            # Continue (skip this batch)\n",
    "            continue\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(data_loader, ctc_weight=0.2, epoch=None, print_samples=True):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data loader.\n",
    "    \n",
    "    Args:\n",
    "        data_loader: DataLoader for evaluation\n",
    "        ctc_weight: Weight for CTC scoring (0.0 to 1.0)\n",
    "        epoch: Current epoch number (optional)\n",
    "        print_samples: Whether to print sample predictions to console\n",
    "    \n",
    "    Returns:\n",
    "        Average loss across all batches\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    transformer_decoder.eval()\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    # Track statistics\n",
    "    total_cer = 0\n",
    "    total_edit_distance = 0\n",
    "    total_loss = 0\n",
    "    sample_count = 0\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Determine if we should print samples in this epoch\n",
    "    show_samples = (epoch is None or epoch == 0 or (epoch+1) % 5 == 0) and print_samples\n",
    "    max_samples_to_print = 20  # Limit console output to 20 samples\n",
    "\n",
    "    # Process all batches in the test loader\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(data_loader):\n",
    "            # Move to device\n",
    "            inputs = inputs.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            labels_flat = labels_flat.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "            \n",
    "            # Forward pass through visual encoder\n",
    "            batch_size = inputs.size(0)\n",
    "            encoder_features = model(inputs, input_lengths)  # (B, T, hidden_dim)\n",
    "            \n",
    "            # Set output_lengths to match the actual encoder output length\n",
    "            output_lengths = torch.full((encoder_features.size(0),), encoder_features.size(1), dtype=torch.long, device=device)\n",
    "            \n",
    "            # Calculate CTC probabilities\n",
    "            log_probs = F.log_softmax(encoder_features, dim=2)  # (B, T, C)\n",
    "            log_probs_ctc = log_probs.transpose(0, 1)  # (T, B, C)\n",
    "            ctc_loss = ctc_loss_fn(log_probs_ctc, labels_flat, output_lengths, label_lengths)\n",
    "            \n",
    "            logging.info(f\"\\nRunning hybrid CTC/Attention decoding for batch {i+1}...\")\n",
    "            if show_samples and i == 0:\n",
    "                print(f\"\\nRunning hybrid CTC/Attention decoding for validation...\")\n",
    "            \n",
    "            try:\n",
    "                logging.info(f\"Encoder features shape: {encoder_features.shape}\")\n",
    "                \n",
    "                # Create proper memory mask based on actual encoder output lengths\n",
    "                memory_mask = torch.zeros((batch_size, encoder_features.size(1)), device=device).bool()\n",
    "                for b in range(batch_size):\n",
    "                    memory_mask[b, :output_lengths[b]] = True\n",
    "                \n",
    "                # Run beam search with CTC weight\n",
    "                all_nbest_hyps = transformer_decoder.batch_beam_search(\n",
    "                    memory=encoder_features,\n",
    "                    memory_mask=memory_mask,\n",
    "                    beam_size=5,\n",
    "                    maxlen=24,\n",
    "                    minlen=1,\n",
    "                    sos=sos_token_idx,\n",
    "                    eos=eos_token_idx,\n",
    "                    ctc_weight=ctc_weight\n",
    "                )\n",
    "                \n",
    "                logging.info(f\"Hybrid decoding completed for batch {i+1}\")\n",
    "                logging.info(f\"Received {len(all_nbest_hyps)} hypotheses sets\")\n",
    "                \n",
    "                # Process each batch item\n",
    "                for b in range(batch_size):\n",
    "                    logging.info(f\"\\nProcessing batch item {b+1}/{batch_size}\")\n",
    "                    sample_count += 1\n",
    "                    \n",
    "                    if b < len(all_nbest_hyps):\n",
    "                        score, pred_indices = all_nbest_hyps[b]\n",
    "                        logging.info(f\"Found beam hypothesis for item {b+1} with score {score:.4f}\")\n",
    "                        pred_indices = np.array(pred_indices)\n",
    "                        \n",
    "                        if len(pred_indices) == 0:\n",
    "                            logging.info(\"WARNING: Prediction sequence is empty!\")\n",
    "                    else:\n",
    "                        logging.info(f\"No hypotheses for batch item {b+1}\")\n",
    "                        pred_indices = np.array([])\n",
    "                    \n",
    "                    # Get target indices\n",
    "                    start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                    end_idx = start_idx + label_lengths[b].item()\n",
    "                    target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "\n",
    "                    # Log debug information for reference and hypothesis tokens\n",
    "                    logging.info(f\"Debug - Reference tokens ({len(target_idx)} tokens): {target_idx}\")\n",
    "                    logging.info(f\"Debug - Hypothesis tokens ({len(pred_indices)} tokens): {pred_indices}\")\n",
    "                    \n",
    "                    # Convert indices to text\n",
    "                    pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                    target_text = indices_to_text(target_idx, idx2char)\n",
    "                    \n",
    "                    # Calculate CER\n",
    "                    cer, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    total_cer += cer\n",
    "                    total_edit_distance += edit_distance\n",
    "                    total_loss += ctc_loss.item() / batch_size\n",
    "                    \n",
    "                    # Store prediction details\n",
    "                    all_predictions.append({\n",
    "                        'sample_id': sample_count,\n",
    "                        'pred_text': pred_text,\n",
    "                        'target_text': target_text,\n",
    "                        'edit_distance': edit_distance,\n",
    "                        'cer': cer\n",
    "                    })\n",
    "                    \n",
    "                    # Log complete info\n",
    "                    logging.info(\"-\" * 50)\n",
    "                    logging.info(f\"Sample {sample_count}:\")\n",
    "                    try:\n",
    "                        logging.info(f\"Predicted text: {pred_text}\")\n",
    "                        logging.info(f\"Target text: {target_text}\")\n",
    "                    except UnicodeEncodeError:\n",
    "                        logging.info(\"Predicted text: [Contains characters that can't be displayed in console]\")\n",
    "                        logging.info(\"Target text: [Contains characters that can't be displayed in console]\")\n",
    "                        logging.info(f\"Predicted indices: {pred_indices}\")\n",
    "                        logging.info(f\"Target indices: {target_idx}\")\n",
    "                        \n",
    "                    logging.info(f\"Edit distance: {edit_distance}\")\n",
    "                    logging.info(f\"CER: {cer:.4f}\")\n",
    "                    logging.info(\"-\" * 50)\n",
    "                    \n",
    "                    # Print to console if this is a sample we should show\n",
    "                    if show_samples and sample_count <= max_samples_to_print:\n",
    "                        print(\"-\" * 50)\n",
    "                        print(f\"Sample {sample_count}:\")\n",
    "                        try:\n",
    "                            print(f\"Predicted text: {pred_text}\")\n",
    "                            print(f\"Target text: {target_text}\")\n",
    "                        except UnicodeEncodeError:\n",
    "                            print(\"Predicted text: [Contains characters that can't be displayed in console]\")\n",
    "                            print(\"Target text: [Contains characters that can't be displayed in console]\")\n",
    "                            \n",
    "                        print(f\"Edit distance: {edit_distance}\")\n",
    "                        print(f\"CER: {cer:.4f}\")\n",
    "                        print(\"-\" * 50)\n",
    "\n",
    "                # Clean up tensors\n",
    "                del encoder_features, log_probs, log_probs_ctc, memory_mask, all_nbest_hyps\n",
    "                \n",
    "                # Periodically clear cache\n",
    "                if i % 3 == 0:  # Every 3 batches\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    logging.info(f\"Memory cleared. Current GPU memory: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during hybrid decoding: {str(e)}\")\n",
    "                logging.error(traceback.format_exc())\n",
    "                print(f\"Error during hybrid decoding: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Write summary statistics\n",
    "        n_samples = len(data_loader.dataset)\n",
    "        avg_cer = total_cer / n_samples\n",
    "        avg_edit_distance = total_edit_distance / n_samples\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        \n",
    "        # Always print summary statistics to console\n",
    "        print(\"\\n=== Summary Statistics ===\")\n",
    "        print(f\"Total samples: {n_samples}\")\n",
    "        print(f\"Average CER: {avg_cer:.4f}\")\n",
    "        print(f\"Average Edit Distance: {avg_edit_distance:.2f}\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"CTC Weight used: {ctc_weight}\")\n",
    "        \n",
    "        # Log summary statistics as well\n",
    "        logging.info(\"\\n=== Summary Statistics ===\")\n",
    "        logging.info(f\"Total samples: {n_samples}\")\n",
    "        logging.info(f\"Average CER: {avg_cer:.4f}\")\n",
    "        logging.info(f\"Average Edit Distance: {avg_edit_distance:.2f}\")\n",
    "        logging.info(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        logging.info(f\"CTC Weight used: {ctc_weight}\")\n",
    "\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(ctc_weight=0.2, checkpoint_path=None):\n",
    "    best_val_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    rng_state = get_rng_state()\n",
    "    \n",
    "    # Load checkpoint if provided\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "        logging.info(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            \n",
    "            # Check model architecture compatibility\n",
    "            model_state_dict = model.state_dict()\n",
    "            checkpoint_model_state_dict = checkpoint['model_state_dict']\n",
    "            if set(model_state_dict.keys()) != set(checkpoint_model_state_dict.keys()):\n",
    "                missing_keys = [k for k in model_state_dict.keys() if k not in checkpoint_model_state_dict]\n",
    "                unexpected_keys = [k for k in checkpoint_model_state_dict.keys() if k not in model_state_dict]\n",
    "                error_msg = \"Model architecture mismatch detected!\\n\"\n",
    "                if missing_keys:\n",
    "                    error_msg += f\"Missing keys in checkpoint: {missing_keys}\\n\"\n",
    "                if unexpected_keys:\n",
    "                    error_msg += f\"Unexpected keys in checkpoint: {unexpected_keys}\\n\"\n",
    "                error_msg += \"Cannot proceed with training due to incompatible architecture.\"\n",
    "                print(error_msg)\n",
    "                logging.error(error_msg)\n",
    "                raise RuntimeError(\"Model architecture mismatch. Training aborted to prevent corruption.\")\n",
    "            \n",
    "            # Load the state dict\n",
    "            model.load_state_dict(checkpoint_model_state_dict)\n",
    "            \n",
    "            # Check transformer decoder architecture compatibility\n",
    "            decoder_state_dict = transformer_decoder.state_dict()\n",
    "            checkpoint_decoder_state_dict = checkpoint['transformer_decoder_state_dict']\n",
    "            \n",
    "            if set(decoder_state_dict.keys()) != set(checkpoint_decoder_state_dict.keys()):\n",
    "                missing_keys = [k for k in decoder_state_dict.keys() if k not in checkpoint_decoder_state_dict]\n",
    "                unexpected_keys = [k for k in checkpoint_decoder_state_dict.keys() if k not in decoder_state_dict]\n",
    "                error_msg = \"Transformer decoder architecture mismatch detected!\\n\"\n",
    "                if missing_keys:\n",
    "                    error_msg += f\"Missing keys in checkpoint: {missing_keys}\\n\"\n",
    "                if unexpected_keys:\n",
    "                    error_msg += f\"Unexpected keys in checkpoint: {unexpected_keys}\\n\"\n",
    "                error_msg += \"Cannot proceed with training due to incompatible architecture.\"\n",
    "                print(error_msg)\n",
    "                logging.error(error_msg)\n",
    "                raise RuntimeError(\"Transformer decoder architecture mismatch. Training aborted to prevent corruption.\")\n",
    "            \n",
    "            # Load the decoder state dict\n",
    "            transformer_decoder.load_state_dict(checkpoint_decoder_state_dict)\n",
    "            print(\"Successfully loaded checkpoint\")\n",
    "            \n",
    "            # Load optimizer state\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "            # Update training state\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "            \n",
    "            # Safely restore RNG state if available\n",
    "            if 'rng_state' in checkpoint:\n",
    "                try:\n",
    "                    set_rng_state(checkpoint['rng_state'])\n",
    "                    print(\"RNG state restored successfully\")\n",
    "                    logging.info(\"RNG state restored successfully\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not restore RNG state: {str(e)}\")\n",
    "                    logging.warning(f\"Could not restore RNG state: {str(e)}\")\n",
    "                    print(\"Continuing with current RNG state\")\n",
    "                    logging.info(\"Continuing with current RNG state\")\n",
    "                \n",
    "            print(f\"Checkpoint loaded successfully. Resuming from epoch {start_epoch}\")\n",
    "            logging.info(f\"Checkpoint loaded successfully. Resuming from epoch {start_epoch}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {str(e)}\")\n",
    "            logging.error(f\"Error loading checkpoint: {str(e)}\")\n",
    "            print(\"Aborting training due to checkpoint loading failure.\")\n",
    "            raise  # Re-raise the exception to stop execution\n",
    "        \n",
    "    else:\n",
    "        if checkpoint_path:\n",
    "            print(f\"Checkpoint file {checkpoint_path} not found. Starting training from scratch.\")\n",
    "            logging.info(f\"Checkpoint file {checkpoint_path} not found. Starting training from scratch.\")\n",
    "        else:\n",
    "            print(\"No checkpoint specified. Starting training from scratch.\")\n",
    "            logging.info(\"No checkpoint specified. Starting training from scratch.\")\n",
    "    \n",
    "    print(f\"Starting training for {total_epochs} epochs\")\n",
    "    print(f\"Logs will be saved to {log_filename}\")\n",
    "    print(f\"Checkpoints will be saved every 10 epochs\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs} - Training...\")\n",
    "        epoch_loss = train_one_epoch()\n",
    "        \n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            logging.info(f\"GPU memory after training: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
    "        \n",
    "        scheduler.adjust_lr(optimizer, epoch)\n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs} - Evaluating...\")\n",
    "        val_loss = evaluate_model(val_loader, ctc_weight=ctc_weight, epoch=epoch)\n",
    "        \n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            logging.info(f\"GPU memory after evaluation: {torch.cuda.memory_allocated()/1e6:.2f}MB\")\n",
    "        \n",
    "        # Always log to file\n",
    "        logging.info(f\"Epoch {epoch + 1}/{total_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Print summary every epoch to console\n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs} - Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Print detailed prediction samples every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"DETAILED RESULTS AFTER EPOCH {epoch + 1}\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Run evaluation on a small subset of validation data to show predictions\n",
    "            with torch.no_grad():\n",
    "                # Just evaluate first batch for demonstration\n",
    "                for inputs, input_lengths, labels_flat, label_lengths in val_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    input_lengths = input_lengths.to(device)\n",
    "                    labels_flat = labels_flat.to(device)\n",
    "                    label_lengths = label_lengths.to(device)\n",
    "                    \n",
    "                    batch_size = inputs.size(0)\n",
    "                    encoder_features = model(inputs, input_lengths)\n",
    "                    \n",
    "                    # Create memory mask\n",
    "                    memory_mask = torch.zeros((batch_size, encoder_features.size(1)), device=device).bool()\n",
    "                    for b in range(batch_size):\n",
    "                        memory_mask[b, :input_lengths[b]] = True\n",
    "                    \n",
    "                    # Run beam search\n",
    "                    all_nbest_hyps = transformer_decoder.batch_beam_search(\n",
    "                        memory=encoder_features,\n",
    "                        memory_mask=memory_mask,\n",
    "                        beam_size=5,\n",
    "                        maxlen=24,\n",
    "                        minlen=1,\n",
    "                        sos=sos_token_idx,\n",
    "                        eos=eos_token_idx,\n",
    "                        ctc_weight=ctc_weight\n",
    "                    )\n",
    "                    \n",
    "                    # Show predictions for a few samples\n",
    "                    print(f\"\\nShowing predictions for {min(3, batch_size)} samples:\")\n",
    "                    for b in range(min(3, batch_size)):\n",
    "                        # Get target indices\n",
    "                        start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                        end_idx = start_idx + label_lengths[b].item()\n",
    "                        target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                        \n",
    "                        # Get prediction\n",
    "                        _, pred_indices = all_nbest_hyps[b]\n",
    "                        \n",
    "                        # Convert to text\n",
    "                        pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                        target_text = indices_to_text(target_idx, idx2char)\n",
    "                        \n",
    "                        # Calculate CER\n",
    "                        cer, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                        \n",
    "                        print(f\"\\nSample {b+1}:\")\n",
    "                        print(f\"  Prediction: {pred_text}\")\n",
    "                        print(f\"  Target: {target_text}\")\n",
    "                        print(f\"  CER: {cer:.4f}, Edit Distance: {edit_distance}\")\n",
    "                    \n",
    "                    break  # Just show the first batch\n",
    "            \n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "            \n",
    "            # Current learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current learning rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        if True:\n",
    "            # Update the RNG state before saving\n",
    "            rng_state = get_rng_state()\n",
    "            \n",
    "            checkpoint_path = f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'transformer_decoder_state_dict': transformer_decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': val_loss,\n",
    "                'rng_state': rng_state,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "            logging.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "            # Force synchronize CUDA operations and clear memory after saving\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save best model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Save the best model with the same pattern as checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'transformer_decoder_state_dict': transformer_decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': val_loss,\n",
    "                'rng_state': rng_state,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
    "            logging.info(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Final checkpoint saved to: checkpoint_epoch_{total_epochs}.pth\")\n",
    "    print(f\"Best model saved to: best_model.pth\")\n",
    "\n",
    "    \n",
    "reset_seed()\n",
    "train_model(ctc_weight=0.2, checkpoint_path=\"\") \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
