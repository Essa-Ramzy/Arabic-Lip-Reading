{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torch, os, cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import editdistance\n",
    "from lipreading.model import Lipreading\n",
    "from lipreading.optim_utils import CosineScheduler\n",
    "# Import the TCN decoder instead of transformer decoder\n",
    "from lipreading.tcn_decoder import TCNDecoder\n",
    "# from lipreading.transformer_decoder import ArabicTransformerDecoder\n",
    "\n",
    "# We don't need the mask utility for TCN\n",
    "# from espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2. Initialize the seed and the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Setting the seed for reproducibility\n",
    "seed = 0\n",
    "def reset_seed():\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Setting the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.1. List of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def extract_label(file):\n",
    "    label = []\n",
    "    diacritics = {\n",
    "        '\\u064B',  # Fathatan\n",
    "        '\\u064C',  # Dammatan\n",
    "        '\\u064D',  # Kasratan\n",
    "        '\\u064E',  # Fatha\n",
    "        '\\u064F',  # Damma\n",
    "        '\\u0650',  # Kasra\n",
    "        '\\u0651',  # Shadda\n",
    "        '\\u0652',  # Sukun\n",
    "        '\\u06E2',  # Small High meem\n",
    "    }\n",
    "\n",
    "    sentence = pd.read_csv(file)\n",
    "    for word in sentence.word:\n",
    "        for char in word:\n",
    "            if char not in diacritics:\n",
    "                label.append(char)\n",
    "            else:\n",
    "                label[-1] += char\n",
    "\n",
    "    return label\n",
    "\n",
    "classes = set()\n",
    "for i in os.listdir('Dataset/Csv (with Diacritics)'):\n",
    "    file = 'Dataset/Csv (with Diacritics)/' + i\n",
    "    label = extract_label(file)\n",
    "    classes.update(label)\n",
    "\n",
    "mapped_classes = {}\n",
    "for i, c in enumerate(sorted(classes, reverse=True), 1):\n",
    "    mapped_classes[c] = i\n",
    "\n",
    "print(mapped_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.2. Video Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Defining the video dataset class\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_paths, label_paths, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        frames = self.load_frames(video_path=video_path)\n",
    "        label = torch.tensor(list(map(lambda x: mapped_classes[x], extract_label(label_path))))\n",
    "        input_length = torch.tensor(len(frames), dtype=torch.long)\n",
    "        label_length = torch.tensor(len(label), dtype=torch.long)\n",
    "        return frames, input_length, label, label_length\n",
    "    \n",
    "    def load_frames(self, video_path):\n",
    "        frames = []\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        for i in range(total_frames):\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = video.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame_pil = Image.fromarray(frame, 'L')\n",
    "                frames.append(frame_pil)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            frames = [self.transform(frame) for frame in frames] \n",
    "        frames = torch.stack(frames).permute(1, 0, 2, 3)\n",
    "        return frames\n",
    "\n",
    "# Defining the video transform\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.421, std=0.165),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "videos_dir = \"Dataset/Preprocessed_Video\"\n",
    "labels_dir = \"Dataset/Csv (with Diacritics)\"\n",
    "videos, labels = [], []\n",
    "file_names = [file_name[:-4] for file_name in os.listdir(videos_dir)]\n",
    "for file_name in file_names:\n",
    "    videos.append(os.path.join(videos_dir, file_name + \".mp4\"))\n",
    "    labels.append(os.path.join(labels_dir, file_name + \".csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.3. Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Split the dataset into training, validation, test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(videos, labels, test_size=0.1000, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.4. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def pad_packed_collate(batch):\n",
    "    \"\"\"Pads data and labels with different lengths in the same batch\n",
    "    \"\"\"\n",
    "    data_list, input_lengths, labels_list, label_lengths = zip(*batch)\n",
    "    c, max_len, h, w = max(data_list, key=lambda x: x.shape[1]).shape\n",
    "\n",
    "    data = torch.zeros((len(data_list), c, max_len, h, w))\n",
    "    \n",
    "    # Only copy up to the actual sequence length\n",
    "    for idx in range(len(data)):\n",
    "        data[idx, :, :input_lengths[idx], :, :] = data_list[idx][:, :input_lengths[idx], :, :]\n",
    "    \n",
    "    # Flatten labels for CTC loss\n",
    "    labels_flat = []\n",
    "    for label_seq in labels_list:\n",
    "        labels_flat.extend(label_seq)\n",
    "    labels_flat = torch.LongTensor(labels_flat)\n",
    "    \n",
    "    # Convert lengths to tensor\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    label_lengths = torch.LongTensor(label_lengths)\n",
    "    return data, input_lengths, labels_flat, label_lengths\n",
    "\n",
    "\n",
    "# Defining the video dataloaders (train, validation, test)\n",
    "train_dataset = VideoDataset(X_train, y_train, transform=transforms)\n",
    "val_dataset = VideoDataset(X_val, y_val, transform=transforms)\n",
    "test_dataset = VideoDataset(X_test, y_test, transform=transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True, collate_fn=pad_packed_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def indices_to_text(indices, idx2char):\n",
    "    \"\"\"\n",
    "    Converts a list of indices to text using the reverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ''.join([idx2char.get(i, '') for i in indices])\n",
    "    except UnicodeEncodeError:\n",
    "        # Handle encoding issues in Windows console\n",
    "        # Return a safe representation that won't cause encoding errors\n",
    "        safe_text = []\n",
    "        for i in indices:\n",
    "            char = idx2char.get(i, '')\n",
    "            try:\n",
    "                # Test if character can be encoded\n",
    "                char.encode('cp1252')\n",
    "                safe_text.append(char)\n",
    "            except UnicodeEncodeError:\n",
    "                # Replace with a placeholder for characters that can't be displayed\n",
    "                safe_text.append(f\"[{i}]\")\n",
    "        return ''.join(safe_text)\n",
    "\n",
    "def compute_cer(reference_indices, hypothesis_indices):\n",
    "    \"\"\"\n",
    "    Computes Character Error Rate (CER) directly using token indices.\n",
    "    Takes raw token indices from our vocabulary (class_mapping.txt) rather than Unicode text.\n",
    "    \n",
    "    Returns a tuple of (CER, edit_distance)\n",
    "    \"\"\"\n",
    "    # Use the indices directly - each index is one token in our vocabulary\n",
    "    ref_tokens = reference_indices\n",
    "    hyp_tokens = hypothesis_indices\n",
    "    \n",
    "    try:\n",
    "        print(f\"Debug - Reference tokens ({len(ref_tokens)} tokens): {ref_tokens}\")\n",
    "        print(f\"Debug - Hypothesis tokens ({len(hyp_tokens)} tokens): {hyp_tokens}\")\n",
    "    except UnicodeEncodeError:\n",
    "        # Handle encoding issues in Windows console\n",
    "        print(f\"Debug - Reference tokens ({len(ref_tokens)} tokens): [Token indices omitted due to encoding issues]\")\n",
    "        print(f\"Debug - Hypothesis tokens ({len(hyp_tokens)} tokens): [Token indices omitted due to encoding issues]\")\n",
    "    \n",
    "    # Calculate edit distance using the editdistance library\n",
    "    edit_distance = editdistance.eval(ref_tokens, hyp_tokens)\n",
    "    \n",
    "    # Calculate CER\n",
    "    cer = edit_distance / max(len(ref_tokens), 1)  # Avoid division by zero\n",
    "    \n",
    "    return cer, edit_distance\n",
    "\n",
    "# Initializing the hyper-parameters\n",
    "densetcn_options = {\n",
    "    'block_config': [3, 3, 3, 3],               # Number of layers in each dense block\n",
    "    'growth_rate_set': [384, 384, 384, 384],    # Growth rate for each block (must be divisible by len(kernel_size_set))\n",
    "    'reduced_size': 512,                        # Reduced size between blocks (must be divisible by len(kernel_size_set))\n",
    "    'kernel_size_set': [3, 5, 7],               # Kernel sizes for multi-scale processing\n",
    "    'dilation_size_set': [1, 2, 5],             # Dilation rates for increasing receptive field\n",
    "    'squeeze_excitation': True,                 # Whether to use SE blocks for channel attention\n",
    "    'dropout': 0.2                              # Dropout rate\n",
    "}\n",
    "initial_lr = 3e-4\n",
    "total_epochs = 80\n",
    "scheduler = CosineScheduler(initial_lr, total_epochs)\n",
    "\n",
    "# Build reverse mapping for decoding\n",
    "idx2char = {v: k for k, v in mapped_classes.items()}\n",
    "idx2char[0] = \"\"  # Blank token for CTC\n",
    "\n",
    "# Initializing the model\n",
    "model = Lipreading(densetcn_options=densetcn_options, hidden_dim=512, num_classes=len(mapped_classes) + 1, relu_type='prelu').to(device)\n",
    "\n",
    "# Add a TCN decoder on top of the visual encoder\n",
    "tcn_decoder = TCNDecoder(\n",
    "    vocab_size=len(mapped_classes) + 1,  # +1 for blank token\n",
    "    hidden_dim=512,  # Matching hidden_dim from the model\n",
    "    num_channels=[384, 384, 384, 384],  # Channels must be divisible by 3 (num_kernels in multibranch mode)\n",
    "    kernel_size=3,  # Kernel size for TCN convolutions\n",
    "    dropout=0.2,  # Dropout rate\n",
    "    emb_dropout=0.2,  # Embedding dropout rate\n",
    "    mode='multibranch'  # Use multi-branch TCN for better feature extraction\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Defining the loss function and optimizer\n",
    "optimizer = optim.Adam(list(model.parameters()) + list(tcn_decoder.parameters()), lr=initial_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Replace beam search with TCN decoder inference\n",
    "def tcn_decode(log_probs, blank_index=0):\n",
    "    \"\"\"\n",
    "    Perform TCN-based decoding on CTC log probabilities.\n",
    "    \n",
    "    Args:\n",
    "        log_probs: Log probabilities of shape (B, T, C)\n",
    "        blank_index: Index of the blank token\n",
    "        \n",
    "    Returns:\n",
    "        List of hypotheses, each with 'yseq' and 'score' keys\n",
    "    \"\"\"\n",
    "    batch_size = log_probs.size(0)\n",
    "    max_length = log_probs.size(1)\n",
    "    \n",
    "    # Create memory from encoder features (log_probs)\n",
    "    memory = log_probs\n",
    "    \n",
    "    # Use TCN decoder for beam search decoding\n",
    "    results = tcn_decoder.batch_beam_search(memory, beam_size=5, maxlen=24)  # max_label_length=24 from dataset\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Training the model\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    tcn_decoder.train()\n",
    "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    ce_loss = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index (0)\n",
    "    \n",
    "    for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(train_loader):\n",
    "        # Print input shape for debugging\n",
    "        print(f\"Batch {batch_idx+1} - Input shape: {inputs.shape}\")\n",
    "        \n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        labels_flat = labels_flat.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the visual encoder\n",
    "        encoder_features = model(inputs, input_lengths)\n",
    "        output_lengths = torch.full((encoder_features.size(0),), encoder_features.size(1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Print shape to verify sequence output\n",
    "        print(f\"Batch {batch_idx+1} - Encoder features shape: {encoder_features.shape}\")\n",
    "        \n",
    "        # Apply log_softmax for CTC\n",
    "        log_probs = F.log_softmax(encoder_features, dim=2)  # (B, T, C)\n",
    "        \n",
    "        # Prepare for CTC loss - requires (T, B, C) format\n",
    "        outputs_for_ctc = log_probs.transpose(0, 1)  # from (B, T, C) to (T, B, C)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        ctc_loss_val = ctc_loss(outputs_for_ctc, labels_flat, output_lengths, label_lengths)\n",
    "        \n",
    "        # Prepare target sequences for TCN training\n",
    "        # First, reconstruct the target sequences from the flattened labels\n",
    "        # Create a list of target sequences for each batch item\n",
    "        target_seqs = []\n",
    "        target_masks = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            seq_len = label_lengths[b].item()\n",
    "            # Extract the sequence for this batch item\n",
    "            target_seq = labels_flat[start_idx:start_idx + seq_len]\n",
    "            # Add start-of-sequence token (1) at the beginning\n",
    "            target_seq = torch.cat([torch.tensor([1], device=device), target_seq])\n",
    "            # Add end-of-sequence token (2) at the end\n",
    "            target_seq = torch.cat([target_seq, torch.tensor([2], device=device)])\n",
    "            \n",
    "            # Prepare masks for TCN\n",
    "            target_mask = torch.ones((seq_len + 2,), device=device)\n",
    "            \n",
    "            # Add to lists\n",
    "            target_seqs.append(target_seq)\n",
    "            target_masks.append(target_mask)\n",
    "            \n",
    "            # Update start index\n",
    "            start_idx += seq_len\n",
    "        \n",
    "        # Pad sequences to max length\n",
    "        max_len = max(len(seq) for seq in target_seqs)\n",
    "        padded_seqs = []\n",
    "        for seq in target_seqs:\n",
    "            padded = torch.cat([seq, torch.zeros(max_len - len(seq), device=device, dtype=torch.long)])\n",
    "            padded_seqs.append(padded)\n",
    "        \n",
    "        # Stack sequences and masks\n",
    "        target_tensor = torch.stack(padded_seqs)\n",
    "        \n",
    "        # Create memory mask (indicates valid encoder positions)\n",
    "        memory_mask = torch.ones((batch_size, encoder_features.size(1)), device=device)\n",
    "        \n",
    "        # For TCN training, we use the encoder features as memory\n",
    "        # and teacher-forcing with target sequences as input\n",
    "        # The input to the TCN decoder is the target sequence shifted right\n",
    "        decoder_input = target_tensor[:, :-1]  # Exclude the last token (EOS)\n",
    "        decoder_output = target_tensor[:, 1:]  # Exclude the first token (SOS)\n",
    "        \n",
    "        # Create mask for the target - to enforce causal attention\n",
    "        tgt_mask = torch.ones((batch_size, decoder_input.size(1)), device=device)\n",
    "        \n",
    "        # Forward through TCN decoder\n",
    "        tcn_out = tcn_decoder(\n",
    "            encoder_features\n",
    "        )\n",
    "        \n",
    "        # Calculate TCN loss - need to handle shape mismatch\n",
    "        # Get sequence lengths for proper comparison\n",
    "        tcn_seq_len = tcn_out.size(1)\n",
    "        decoder_seq_len = decoder_output.size(1)\n",
    "        \n",
    "        # Print shapes for debugging\n",
    "        print(f\"TCN output shape: {tcn_out.shape}, Decoder output shape: {decoder_output.shape}\")\n",
    "        \n",
    "        # Adjust decoder_output to match tcn_out length using interpolation if needed\n",
    "        if tcn_seq_len != decoder_seq_len:\n",
    "            print(f\"Sequence length mismatch: TCN={tcn_seq_len}, Decoder={decoder_seq_len}\")\n",
    "            # Use only the common prefix of both sequences\n",
    "            min_seq_len = min(tcn_seq_len, decoder_seq_len)\n",
    "            tcn_out = tcn_out[:, :min_seq_len, :]\n",
    "            decoder_output = decoder_output[:, :min_seq_len]\n",
    "            print(f\"Using common prefix with length {min_seq_len}\")\n",
    "            print(f\"New shapes - TCN: {tcn_out.shape}, Decoder: {decoder_output.shape}\")\n",
    "        \n",
    "        # Flatten for cross entropy loss\n",
    "        tcn_out_flat = tcn_out.reshape(-1, tcn_out.size(-1))\n",
    "        decoder_output_flat = decoder_output.reshape(-1)\n",
    "        \n",
    "        # Verify shapes are compatible\n",
    "        print(f\"Flattened shapes - TCN: {tcn_out_flat.shape}, Decoder: {decoder_output_flat.shape}\")\n",
    "        \n",
    "        # Calculate loss\n",
    "        tcn_loss = ce_loss(tcn_out_flat, decoder_output_flat)\n",
    "        \n",
    "        # Combined loss (weighted sum of CTC and TCN losses)\n",
    "        alpha = 0.7  # Weight for CTC loss\n",
    "        loss = alpha * ctc_loss_val + (1 - alpha) * tcn_loss\n",
    "        \n",
    "        print(f\"Batch {batch_idx+1} - CTC Loss: {ctc_loss_val.item():.4f}, TCN Loss: {tcn_loss.item():.4f}, Combined Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(data_loader):\n",
    "    model.eval()\n",
    "    tcn_decoder.eval()\n",
    "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    # Track statistics\n",
    "    total_cer = 0\n",
    "    total_edit_distance = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Process all batches in the test loader\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(data_loader):\n",
    "            # Move to device\n",
    "            inputs = inputs.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            labels_flat = labels_flat.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "            \n",
    "            # Forward pass through visual encoder\n",
    "            batch_size = inputs.size(0)\n",
    "            encoder_features = model(inputs, input_lengths)  # (B, T, hidden_dim)\n",
    "            \n",
    "            # Calculate CTC loss as before (for monitoring only)\n",
    "            output_lengths = torch.full((encoder_features.size(0),), encoder_features.size(1), dtype=torch.long, device=device)\n",
    "            log_probs = F.log_softmax(encoder_features, dim=2)  # (B, T, C)\n",
    "            log_probs_ctc = log_probs.transpose(0, 1)  # (T, B, C)\n",
    "            loss = ctc_loss(log_probs_ctc, labels_flat, output_lengths, label_lengths)\n",
    "            \n",
    "            print(f\"\\nRunning TCN decoding for batch {i+1}...\")\n",
    "            \n",
    "            try:\n",
    "                # Batch beam search returns a list of lists - each batch item has its own list of beams\n",
    "                print(f\"Encoder features shape: {encoder_features.shape}\")\n",
    "                print(f\"Beam size: 5, Max length: 24\")\n",
    "                \n",
    "                all_nbest_hyps = tcn_decoder.batch_beam_search(\n",
    "                    encoder_features, beam_size=5, maxlen=24\n",
    "                )\n",
    "                \n",
    "                print(f\"TCN decoding completed for batch {i+1}\")\n",
    "                print(f\"Received {len(all_nbest_hyps)} hypotheses sets\")\n",
    "                \n",
    "                # Process each batch item\n",
    "                for b in range(batch_size):\n",
    "                    print(f\"\\nProcessing batch item {b+1}/{batch_size}\")\n",
    "                    \n",
    "                    # Get best hypothesis for this batch item\n",
    "                    if b < len(all_nbest_hyps) and len(all_nbest_hyps[b]) > 0:\n",
    "                        nbest_hyps = all_nbest_hyps[b]  # List of beams for this batch item\n",
    "                        print(f\"Found {len(nbest_hyps)} beam hypotheses for item {b+1}\")\n",
    "                        \n",
    "                        best_hyp = nbest_hyps[0]  # Best beam (highest score)\n",
    "                        print(f\"Best hypothesis raw sequence: {best_hyp['yseq']}\")\n",
    "                        \n",
    "                        # Get predicted sequence (remove SOS token if present)\n",
    "                        pred_indices = best_hyp[\"yseq\"][1:] if best_hyp[\"yseq\"][0] == 1 else best_hyp[\"yseq\"]\n",
    "                        print(f\"After SOS removal: {pred_indices}\")\n",
    "                        \n",
    "                        # Clean up sequence (remove padding, EOS tokens)\n",
    "                        # Assuming 2 is EOS token\n",
    "                        if 2 in pred_indices:\n",
    "                            eos_idx = pred_indices.index(2)\n",
    "                            pred_indices = pred_indices[:eos_idx]\n",
    "                            print(f\"After EOS removal: {pred_indices}\")\n",
    "                        \n",
    "                        # Print warning if pred_indices is empty\n",
    "                        if len(pred_indices) == 0:\n",
    "                            print(\"WARNING: Prediction sequence is empty after token filtering!\")\n",
    "                        \n",
    "                        # Convert list to numpy array\n",
    "                        pred_indices = np.array(pred_indices)\n",
    "                        \n",
    "                        # Print top beam search results\n",
    "                        print(\"\\nTop beam search results:\")\n",
    "                        for j, hyp in enumerate(nbest_hyps[:3]):  # Show top 3 results\n",
    "                            # Clean up the sequence - remove SOS/EOS tokens\n",
    "                            hyp_indices = hyp[\"yseq\"][1:] if hyp[\"yseq\"][0] == 1 else hyp[\"yseq\"]\n",
    "                            if 2 in hyp_indices:\n",
    "                                eos_idx = hyp_indices.index(2)\n",
    "                                hyp_indices = hyp_indices[:eos_idx]\n",
    "                            \n",
    "                            hyp_text = indices_to_text(hyp_indices, idx2char)\n",
    "                            try:\n",
    "                                print(f\"  Hyp {j+1}: {hyp_text} (Score: {hyp['score']:.4f})\")\n",
    "                            except UnicodeEncodeError:\n",
    "                                print(f\"  Hyp {j+1}: [Text contains non-displayable characters] (Score: {hyp['score']:.4f})\")\n",
    "                                print(f\"  Token indices: {hyp_indices}\")\n",
    "                    else:\n",
    "                        # No hypotheses for this batch item - use empty prediction\n",
    "                        print(f\"No hypotheses for batch item {b+1}\")\n",
    "                        pred_indices = np.array([])\n",
    "                        \n",
    "                        # Add more debug info to understand why no hypotheses were returned\n",
    "                        if b >= len(all_nbest_hyps):\n",
    "                            print(f\"  Issue: Batch index {b} is out of range for all_nbest_hyps (len={len(all_nbest_hyps)})\")\n",
    "                        elif len(all_nbest_hyps[b]) == 0:\n",
    "                            print(f\"  Issue: Empty hypothesis list for batch item {b+1}\")\n",
    "                    \n",
    "                    # Get target indices\n",
    "                    start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                    end_idx = start_idx + label_lengths[b].item()\n",
    "                    target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                    \n",
    "                    # Convert indices to text\n",
    "                    pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                    target_text = indices_to_text(target_idx, idx2char)\n",
    "                    \n",
    "                    # Calculate CER using custom function\n",
    "                    cer, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    total_cer += cer\n",
    "                    total_edit_distance += edit_distance\n",
    "                    total_loss += loss.item() / batch_size\n",
    "                    \n",
    "                    # Print info\n",
    "                    print(\"-\" * 50)\n",
    "                    print(f\"Sample {i * batch_size + b + 1}:\")\n",
    "                    try:\n",
    "                        print(f\"Predicted text: {pred_text}\")\n",
    "                        print(f\"Target text: {target_text}\")\n",
    "                    except UnicodeEncodeError:\n",
    "                        print(\"Predicted text: [Contains characters that can't be displayed in console]\")\n",
    "                        print(\"Target text: [Contains characters that can't be displayed in console]\")\n",
    "                        print(f\"Predicted indices: {pred_indices}\")\n",
    "                        print(f\"Target indices: {target_idx}\")\n",
    "                        \n",
    "                    print(f\"Edit distance: {edit_distance}\")\n",
    "                    print(f\"CER: {cer:.4f}\")\n",
    "                    print(\"-\" * 50)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error during TCN decoding: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                # Fall back to CTC greedy decoding\n",
    "                print(\"Falling back to CTC greedy decoding\")\n",
    "                \n",
    "                # Process batch items with greedy decoding\n",
    "                for b in range(batch_size):\n",
    "                    batch_logits = log_probs[b].cpu().numpy()\n",
    "                    pred_indices = np.argmax(batch_logits, axis=1)\n",
    "                    # Remove duplicates and blanks\n",
    "                    filtered_indices = []\n",
    "                    prev_idx = -1\n",
    "                    for idx in pred_indices:\n",
    "                        if idx != 0 and idx != prev_idx:  # Skip blanks and duplicates\n",
    "                            filtered_indices.append(idx)\n",
    "                        prev_idx = idx\n",
    "                    pred_indices = np.array(filtered_indices)\n",
    "                    \n",
    "                    # Get target indices\n",
    "                    start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                    end_idx = start_idx + label_lengths[b].item()\n",
    "                    target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                    \n",
    "                    # Convert indices to text\n",
    "                    pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                    target_text = indices_to_text(target_idx, idx2char)\n",
    "                    \n",
    "                    # Calculate CER\n",
    "                    cer, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    total_cer += cer\n",
    "                    total_edit_distance += edit_distance\n",
    "                    total_loss += loss.item() / batch_size\n",
    "                    \n",
    "                    # Print info\n",
    "                    print(\"-\" * 50)\n",
    "                    print(f\"Sample {i * batch_size + b + 1} (Greedy CTC):\")\n",
    "                    print(f\"Predicted text: {pred_text}\")\n",
    "                    print(f\"Target text: {target_text}\")\n",
    "                    print(f\"Edit distance: {edit_distance}\")\n",
    "                    print(f\"CER: {cer:.4f}\")\n",
    "                    print(\"-\" * 50)\n",
    "        \n",
    "        # Write summary statistics\n",
    "        n_samples = len(data_loader.dataset)\n",
    "        avg_cer = total_cer / n_samples\n",
    "        avg_edit_distance = total_edit_distance / n_samples\n",
    "        avg_loss = total_loss / n_samples\n",
    "        \n",
    "        print(\"=== Summary Statistics ===\")\n",
    "        print(f\"Total samples: {n_samples}\")\n",
    "        print(f\"Average CER: {avg_cer:.4f}\")\n",
    "        print(f\"Average Edit Distance: {avg_edit_distance:.2f}\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_model():\n",
    "    # Train and validate\n",
    "    for epoch in range(total_epochs):\n",
    "        train_one_epoch()\n",
    "        scheduler.adjust_lr(optimizer, epoch)\n",
    "        val_loss = evaluate_model(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def quick_experiment():\n",
    "    \"\"\"\n",
    "    Run a quick experiment with a small subset of the data to test the TCN decoder.\n",
    "    Uses 30 training samples and 5 test samples.\n",
    "    \"\"\"\n",
    "    print(\"Running quick experiment with a small dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Open a file to save results (avoids console encoding issues)\n",
    "        with open('tcn_results.txt', 'w', encoding='utf-8') as results_file:\n",
    "            # Write function to ensure output is flushed to disk\n",
    "            def write_line(line):\n",
    "                results_file.write(line + \"\\n\")\n",
    "                results_file.flush()  # Flush after each write to ensure data is saved\n",
    "            \n",
    "            write_line(\"TCN Decoder Experiment Results\")\n",
    "            write_line(\"============================\")\n",
    "            write_line(\"\")\n",
    "            \n",
    "            try:\n",
    "                # Create small datasets for quick testing\n",
    "                small_train_dataset = torch.utils.data.Subset(train_dataset, list(range(30)))\n",
    "                small_val_dataset = torch.utils.data.Subset(val_dataset, list(range(5)))\n",
    "                \n",
    "                # Create dataloaders with the small datasets\n",
    "                small_train_loader = DataLoader(small_train_dataset, batch_size=5, shuffle=True, \n",
    "                                            pin_memory=True, collate_fn=pad_packed_collate)\n",
    "                small_val_loader = DataLoader(small_val_dataset, batch_size=2, shuffle=False, \n",
    "                                         pin_memory=True, collate_fn=pad_packed_collate)\n",
    "                \n",
    "                # Train for only 5 epochs\n",
    "                write_line(\"Training on 30 samples...\")\n",
    "                for epoch in range(5):\n",
    "                    try:\n",
    "                        # Training\n",
    "                        model.train()\n",
    "                        tcn_decoder.train()\n",
    "                        running_loss = 0.0\n",
    "                        ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "                        ce_loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "                        \n",
    "                        for batch_idx, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(small_train_loader):\n",
    "                            try:\n",
    "                                # Move data to device\n",
    "                                inputs = inputs.to(device)\n",
    "                                input_lengths = input_lengths.to(device)\n",
    "                                labels_flat = labels_flat.to(device)\n",
    "                                label_lengths = label_lengths.to(device)\n",
    "                                \n",
    "                                # Zero the gradients\n",
    "                                optimizer.zero_grad()\n",
    "                                \n",
    "                                # Forward pass through encoder\n",
    "                                encoder_features = model(inputs, input_lengths)\n",
    "                                output_lengths = torch.full((encoder_features.size(0),), encoder_features.size(1), \n",
    "                                                       dtype=torch.long, device=device)\n",
    "                                \n",
    "                                # Apply log_softmax for CTC\n",
    "                                log_probs = F.log_softmax(encoder_features, dim=2)\n",
    "                                outputs_for_ctc = log_probs.transpose(0, 1)\n",
    "                                \n",
    "                                # Compute CTC loss\n",
    "                                ctc_loss_val = ctc_loss(outputs_for_ctc, labels_flat, output_lengths, label_lengths)\n",
    "                                \n",
    "                                # Prepare target sequences for TCN\n",
    "                                target_seqs = []\n",
    "                                start_idx = 0\n",
    "                                batch_size = inputs.size(0)\n",
    "                                \n",
    "                                for b in range(batch_size):\n",
    "                                    seq_len = label_lengths[b].item()\n",
    "                                    target_seq = labels_flat[start_idx:start_idx + seq_len]\n",
    "                                    # Add SOS and EOS tokens\n",
    "                                    target_seq = torch.cat([torch.tensor([1], device=device), \n",
    "                                                       target_seq, \n",
    "                                                       torch.tensor([2], device=device)])\n",
    "                                    target_seqs.append(target_seq)\n",
    "                                    start_idx += seq_len\n",
    "                                \n",
    "                                # Pad sequences to same length\n",
    "                                max_len = max(len(seq) for seq in target_seqs)\n",
    "                                padded_seqs = []\n",
    "                                for seq in target_seqs:\n",
    "                                    padded = torch.cat([seq, torch.zeros(max_len - len(seq), \n",
    "                                                                     device=device, dtype=torch.long)])\n",
    "                                    padded_seqs.append(padded)\n",
    "                                \n",
    "                                # Stack sequences\n",
    "                                target_tensor = torch.stack(padded_seqs)\n",
    "                                \n",
    "                                # Create masks\n",
    "                                memory_mask = torch.ones((batch_size, encoder_features.size(1)), device=device)\n",
    "                                \n",
    "                                # Prepare decoder input/output\n",
    "                                decoder_input = target_tensor[:, :-1]  # Remove last token (shift right)\n",
    "                                decoder_output = target_tensor[:, 1:]  # Remove first token\n",
    "                                \n",
    "                                # Create target mask for TCN\n",
    "                                tgt_mask = torch.ones((batch_size, decoder_input.size(1)), device=device)\n",
    "                                tgt_mask = tgt_mask.expand(batch_size, -1)\n",
    "                                \n",
    "                                # Forward through TCN\n",
    "                                tcn_out = tcn_decoder(encoder_features)\n",
    "                                \n",
    "                                # Calculate TCN loss - need to handle shape mismatch\n",
    "                                # Get sequence lengths for proper comparison\n",
    "                                tcn_seq_len = tcn_out.size(1)\n",
    "                                decoder_seq_len = decoder_output.size(1)\n",
    "                                \n",
    "                                # Log shapes to file\n",
    "                                write_line(f\"Batch {batch_idx+1} - TCN output shape: {tcn_out.shape}, \" + \n",
    "                                       f\"Decoder output shape: {decoder_output.shape}\")\n",
    "                                \n",
    "                                # Adjust decoder_output to match tcn_out length using interpolation if needed\n",
    "                                if tcn_seq_len != decoder_seq_len:\n",
    "                                    write_line(f\"Sequence length mismatch: TCN={tcn_seq_len}, Decoder={decoder_seq_len}\")\n",
    "                                    # Use only the common prefix of both sequences\n",
    "                                    min_seq_len = min(tcn_seq_len, decoder_seq_len)\n",
    "                                    tcn_out = tcn_out[:, :min_seq_len, :]\n",
    "                                    decoder_output = decoder_output[:, :min_seq_len]\n",
    "                                    write_line(f\"Using common prefix with length {min_seq_len}\")\n",
    "                                    write_line(f\"New shapes - TCN: {tcn_out.shape}, Decoder: {decoder_output.shape}\")\n",
    "                                \n",
    "                                # Flatten for cross entropy loss\n",
    "                                tcn_out_flat = tcn_out.reshape(-1, tcn_out.size(-1))\n",
    "                                decoder_output_flat = decoder_output.reshape(-1)\n",
    "                                \n",
    "                                # Calculate loss\n",
    "                                tcn_loss = ce_loss(tcn_out_flat, decoder_output_flat)\n",
    "                                \n",
    "                                # Combined loss\n",
    "                                alpha = 0.7  # Weight for CTC loss\n",
    "                                loss = alpha * ctc_loss_val + (1 - alpha) * tcn_loss\n",
    "                                \n",
    "                                # Backward pass and optimize\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "                                \n",
    "                                # Print and log progress\n",
    "                                running_loss += loss.item()\n",
    "                                log_msg = f\"Batch {batch_idx+1} - CTC: {ctc_loss_val.item():.4f}, \" + \\\n",
    "                                      f\"TCN: {tcn_loss.item():.4f}, Loss: {loss.item():.4f}\"\n",
    "                                print(log_msg)\n",
    "                                write_line(log_msg)\n",
    "                            except Exception as e:\n",
    "                                write_line(f\"Error in batch {batch_idx+1}: {str(e)}\")\n",
    "                                import traceback\n",
    "                                write_line(traceback.format_exc())\n",
    "                        \n",
    "                        # Evaluate\n",
    "                        epoch_msg = f\"\\nEpoch {epoch+1}/5 - Loss: {running_loss/len(small_train_loader):.4f}\"\n",
    "                        print(epoch_msg)\n",
    "                        write_line(epoch_msg)\n",
    "                        \n",
    "                        # Test with beam search to see actual outputs\n",
    "                        write_line(\"\\nTesting model with beam search decoding...\")\n",
    "                        test_batch(small_val_loader, results_file, write_line)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        write_line(f\"Error in epoch {epoch+1}: {str(e)}\")\n",
    "                        import traceback\n",
    "                        write_line(traceback.format_exc())\n",
    "            \n",
    "            except Exception as e:\n",
    "                write_line(f\"Error setting up experiment: {str(e)}\")\n",
    "                import traceback\n",
    "                write_line(traceback.format_exc())\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Critical error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nExperiment complete! Check tcn_results.txt for details\")\n",
    "\n",
    "def test_batch(data_loader, results_file, write_fn):\n",
    "    \"\"\"\n",
    "    Test the model with the TCN decoder using beam search.\n",
    "    Args:\n",
    "        data_loader: DataLoader with test data\n",
    "        results_file: File handle for saving results\n",
    "        write_fn: Function for writing lines to the file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tcn_decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process validation data\n",
    "        for i, (inputs, input_lengths, labels_flat, label_lengths) in enumerate(data_loader):\n",
    "            try:\n",
    "                # Move to device\n",
    "                inputs = inputs.to(device)\n",
    "                input_lengths = input_lengths.to(device)\n",
    "                labels_flat = labels_flat.to(device)\n",
    "                label_lengths = label_lengths.to(device)\n",
    "                \n",
    "                # Forward pass through visual encoder\n",
    "                batch_size = inputs.size(0)\n",
    "                encoder_features = model(inputs, input_lengths)  # (B, T, hidden_dim)\n",
    "                \n",
    "                write_fn(f\"\\nRunning TCN decoding for batch {i+1}...\")\n",
    "                write_fn(f\"Encoder features shape: {encoder_features.shape}\")\n",
    "                write_fn(f\"Using beam size: 5\")\n",
    "                \n",
    "                # Batch beam search\n",
    "                all_nbest_hyps = tcn_decoder.batch_beam_search(\n",
    "                    encoder_features, beam_size=5, maxlen=24\n",
    "                )\n",
    "                \n",
    "                write_fn(f\"TCN decoding completed for batch {i+1}\")\n",
    "                write_fn(f\"Received {len(all_nbest_hyps)} hypotheses sets\")\n",
    "                \n",
    "                # Process each batch item\n",
    "                for b in range(batch_size):\n",
    "                    try:\n",
    "                        write_fn(f\"\\nProcessing batch item {b+1}/{batch_size}\")\n",
    "                        \n",
    "                        # Get target indices\n",
    "                        start_idx = sum(label_lengths[:b].cpu().tolist()) if b > 0 else 0\n",
    "                        end_idx = start_idx + label_lengths[b].item()\n",
    "                        target_idx = labels_flat[start_idx:end_idx].cpu().numpy()\n",
    "                        \n",
    "                        # Get target text representation\n",
    "                        target_text = indices_to_text(target_idx, idx2char)\n",
    "                        write_fn(f\"Target text: {target_text}\")\n",
    "                        write_fn(f\"Target indices: {target_idx.tolist()}\")\n",
    "                        \n",
    "                        # Get best hypothesis for this batch item\n",
    "                        if b < len(all_nbest_hyps) and len(all_nbest_hyps[b]) > 0:\n",
    "                            nbest_hyps = all_nbest_hyps[b]  # List of beams for this batch item\n",
    "                            write_fn(f\"Found {len(nbest_hyps)} beam hypotheses for item {b+1}\")\n",
    "                            \n",
    "                            best_hyp = nbest_hyps[0]  # Best beam (highest score)\n",
    "                            write_fn(f\"Best hypothesis raw sequence: {best_hyp['yseq']}\")\n",
    "                            \n",
    "                            # Get predicted sequence (remove SOS token if present)\n",
    "                            pred_indices = best_hyp[\"yseq\"][1:] if best_hyp[\"yseq\"][0] == 1 else best_hyp[\"yseq\"]\n",
    "                            write_fn(f\"After SOS removal: {pred_indices}\")\n",
    "                            \n",
    "                            # Clean up sequence (remove padding, EOS tokens)\n",
    "                            # Assuming 2 is EOS token\n",
    "                            if 2 in pred_indices:\n",
    "                                eos_idx = pred_indices.index(2)\n",
    "                                pred_indices = pred_indices[:eos_idx]\n",
    "                                write_fn(f\"After EOS removal: {pred_indices}\")\n",
    "                            \n",
    "                            # Print warning if pred_indices is empty\n",
    "                            if len(pred_indices) == 0:\n",
    "                                write_fn(\"WARNING: Prediction sequence is empty after token filtering!\")\n",
    "                            \n",
    "                            # Convert list to numpy array\n",
    "                            pred_indices = np.array(pred_indices)\n",
    "                            \n",
    "                            # Print top beam search results\n",
    "                            write_fn(\"\\nTop beam search results:\")\n",
    "                            for j, hyp in enumerate(nbest_hyps[:3]):  # Show top 3 results\n",
    "                                # Clean up the sequence - remove SOS/EOS tokens\n",
    "                                hyp_indices = hyp[\"yseq\"][1:] if hyp[\"yseq\"][0] == 1 else hyp[\"yseq\"]\n",
    "                                if 2 in hyp_indices:\n",
    "                                    eos_idx = hyp_indices.index(2)\n",
    "                                    hyp_indices = hyp_indices[:eos_idx]\n",
    "                                \n",
    "                                hyp_text = indices_to_text(hyp_indices, idx2char)\n",
    "                                write_fn(f\"  Hyp {j+1}: {hyp_text} (Score: {hyp['score']:.4f})\")\n",
    "                                write_fn(f\"  Token indices: {hyp_indices}\")\n",
    "                        else:\n",
    "                            # No hypotheses for this batch item - use empty prediction\n",
    "                            write_fn(f\"No hypotheses for batch item {b+1}\")\n",
    "                            \n",
    "                            # Add more debug info to understand why no hypotheses were returned\n",
    "                            if b >= len(all_nbest_hyps):\n",
    "                                write_fn(f\"  Issue: Batch index {b} is out of range for all_nbest_hyps (len={len(all_nbest_hyps)})\")\n",
    "                            elif len(all_nbest_hyps[b]) == 0:\n",
    "                                write_fn(f\"  Issue: Empty hypothesis list for batch item {b+1}\")\n",
    "                            \n",
    "                            pred_indices = np.array([])\n",
    "                        \n",
    "                        # Convert indices to text\n",
    "                        pred_text = indices_to_text(pred_indices, idx2char)\n",
    "                        \n",
    "                        # Calculate CER using custom function\n",
    "                        cer, edit_distance = compute_cer(target_idx, pred_indices)\n",
    "                        \n",
    "                        # Print info to file\n",
    "                        write_fn(\"-\" * 50)\n",
    "                        write_fn(f\"Sample {i * batch_size + b + 1}:\")\n",
    "                        write_fn(f\"Predicted text: {pred_text}\")\n",
    "                        write_fn(f\"Predicted indices: {pred_indices.tolist()}\")\n",
    "                        write_fn(f\"Target text: {target_text}\")\n",
    "                        write_fn(f\"Target indices: {target_idx.tolist()}\")\n",
    "                        write_fn(f\"Edit distance: {edit_distance}\")\n",
    "                        write_fn(f\"CER: {cer:.4f}\")\n",
    "                        write_fn(\"-\" * 50)\n",
    "                    except Exception as e:\n",
    "                        write_fn(f\"Error processing batch item {b+1}: {str(e)}\")\n",
    "                        import traceback\n",
    "                        write_fn(traceback.format_exc())\n",
    "            except Exception as e:\n",
    "                write_fn(f\"Error processing batch {i+1}: {str(e)}\")\n",
    "                import traceback\n",
    "                write_fn(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "reset_seed()\n",
    "# Uncomment one of the following lines to run the full training or quick experiment\n",
    "# train_model()\n",
    "quick_experiment()  # Run the quick experiment with TCN decoder instead of beam search\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
